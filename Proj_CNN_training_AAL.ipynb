{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bsS1Wdw3nv_0",
        "outputId": "aa28911b-6e2c-41f1-9415-e987c57dee62"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "p3u1WEF_pzxM",
        "outputId": "395314a0-6b5b-4657-d952-dd8a6e7d325c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import glob\n",
        "\n",
        "# Get a list of all the file paths that ends with .txt from in specified directory\n",
        "# #fmri_list = glob.glob(\"/content/drive/MyDrive/fmri_data1/*\")\n",
        "# fmri_list = glob.glob(\"/content/drive/MyDrive/fmri_all_sub01/sub-001/func/*T1w_desc-preproc_bold.nii.gz\")\\\n",
        "fmri_list = glob.glob(\"/content/drive/MyDrive/fmri_all_sub01/sub-001/func/*2_desc-preproc_bold.nii.gz\")\n",
        "# sort and Print the file list\n",
        "fmri_list.sort()\n",
        "# fmri_list.reverse()\n",
        "file_list = glob.glob(\"/content/drive/MyDrive/fmri_data1/sub-001/func/*.tsv\")\n",
        "\n",
        "# sort and Print the file list\n",
        "file_list.sort()\n",
        "list_genre = [\"'metal'\",\"'classical'\",\"'rock'\",\"'blues'\",\"'country'\",\"'disco'\",\"'hiphop'\",\"'reggae'\",\"'pop'\",\"'jazz'\"]\n",
        "arr_mapping  = [7,8,9,10,11,12,13,14,15,16,17,18,1,2,3,4,5,6]"
      ],
      "metadata": {
        "id": "pjvxgtbwn8wK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "fmri_list"
      ],
      "metadata": {
        "id": "y_zjXhB0G3kp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "70a060a8-55bb-4c5e-fabe-45d376f09f15"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Test_run-01_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Test_run-02_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Test_run-03_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Test_run-04_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Test_run-05_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Test_run-06_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-01_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-02_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-03_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-04_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-05_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-06_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-07_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-08_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-09_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-10_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-11_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz',\n",
              " '/content/drive/MyDrive/fmri_all_sub01/sub-001/func/sub-001_task-Training_run-12_space-MNI152NLin2009cAsym_res-2_desc-preproc_bold.nii.gz']"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "file_list"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cwGTsu3pKbkY",
        "outputId": "6246a472-9b58-455d-c29f-28771b6ddded"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Test_run-01_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Test_run-02_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Test_run-03_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Test_run-04_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Test_run-05_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Test_run-06_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-01_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-02_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-03_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-04_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-05_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-06_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-07_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-08_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-09_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-10_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-11_events.tsv',\n",
              " '/content/drive/MyDrive/fmri_data1/sub-001/func/sub-001_task-Training_run-12_events.tsv']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "count = 0\n",
        "for k in range (1):\n",
        "    df = pd.read_csv(file_list[k], sep=\"\\t\")\n",
        "    # load fmri\n",
        "    fmri =nib.load(fmri_list[k])\n",
        "    brain_run_data = fmri.get_fdata()\n",
        "    # print(brain_run_data[40,40,40,4])"
      ],
      "metadata": {
        "id": "UtFjdODqMjn0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "# X = np.zeros((738,96,96,68,10))\n",
        "Y = np.zeros(738)"
      ],
      "metadata": {
        "id": "02ixLKuaoCdE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_run_data.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vuCJwUfpOxE9",
        "outputId": "76bd3ea1-0298-454e-82c3-3f6f4e71d4f9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(97, 115, 97, 410)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_tensor(tensor):\n",
        "    mean = np.mean(tensor)\n",
        "    std_dev = np.std(tensor)\n",
        "    return (tensor - mean) / std_dev\n",
        "# Save the training files on disk\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "count = 0\n",
        "for k in range (18):\n",
        "    df = pd.read_csv(file_list[k], sep=\"\\t\")\n",
        "\n",
        "    # load fmri\n",
        "    #fmri =nib.load(fmri_list[arr_mapping[k]])\n",
        "    fmri =nib.load(fmri_list[k])\n",
        "    brain_run_data = fmri.get_fdata()\n",
        "    # load fmri\n",
        "    for j in range(len(df)):\n",
        "        # add Y\n",
        "        row_info = df.iloc[j]\n",
        "        Y[count] = list_genre.index(row_info[\"genre\"])\n",
        "        file_name = \"Brain_\"+str(count)+\".npy\"\n",
        "        # save fmri\n",
        "        # np.save(file_name,standardize_tensor(brain_run_data[:,:,:,j*10:10*j+10]))\n",
        "        np.save(file_name,standardize_tensor(brain_run_data[18:39,67:81,22:39,j*10:10*j+10]))\n",
        "        file_name = \"Brain_\"+str(count)+\"y.npy\"\n",
        "        # save fmri\n",
        "        np.save(file_name,Y[count])\n",
        "        count+=1\n"
      ],
      "metadata": {
        "id": "k1heU9x7oFAs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index(n_sample):\n",
        "  # Change to 0.7\n",
        "  train_idx = np.random.choice(n_sample, size=int(n_sample*0.7), replace=False)\n",
        "  val_and_test = [i for i in range(n_sample) if i not in train_idx]\n",
        "  vt_size = len(val_and_test)\n",
        "  val_index = np.random.choice(vt_size, size=int(vt_size*0.5), replace=False)\n",
        "  val_idx = [val_and_test[i] for i in val_index]\n",
        "  test_idx = [i for i in range(n_sample) if i not in train_idx and i not in val_idx]\n",
        "  return train_idx, val_idx, test_idx"
      ],
      "metadata": {
        "id": "GIcOHj0woUtn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zifMk-XFEQwp",
        "outputId": "f33e6a2c-3249-4e2d-bd69-44cb7341dd1a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8., 2., 4., 3., 5., 0., 7., 1., 6., 9., 8., 2., 4., 3., 5., 0., 7.,\n",
              "       1., 6., 9., 8., 2., 4., 3., 5., 0., 7., 1., 6., 9., 8., 2., 4., 3.,\n",
              "       5., 0., 7., 1., 6., 9., 8., 0., 7., 2., 6., 8., 4., 9., 3., 1., 5.,\n",
              "       0., 7., 2., 6., 8., 4., 9., 3., 1., 5., 0., 7., 2., 6., 8., 4., 9.,\n",
              "       3., 1., 5., 0., 7., 2., 6., 8., 4., 9., 3., 1., 5., 0., 9., 6., 3.,\n",
              "       2., 4., 8., 5., 1., 0., 7., 9., 6., 3., 2., 4., 8., 5., 1., 0., 7.,\n",
              "       9., 6., 3., 2., 4., 8., 5., 1., 0., 7., 9., 6., 3., 2., 4., 8., 5.,\n",
              "       1., 0., 7., 9., 9., 0., 4., 2., 7., 3., 6., 5., 1., 8., 9., 0., 4.,\n",
              "       2., 7., 3., 6., 5., 1., 8., 9., 0., 4., 2., 7., 3., 6., 5., 1., 8.,\n",
              "       9., 0., 4., 2., 7., 3., 6., 5., 1., 8., 9., 7., 8., 4., 1., 0., 5.,\n",
              "       6., 2., 3., 9., 7., 8., 4., 1., 0., 5., 6., 2., 3., 9., 7., 8., 4.,\n",
              "       1., 0., 5., 6., 2., 3., 9., 7., 8., 4., 1., 0., 5., 6., 2., 3., 9.,\n",
              "       7., 7., 3., 8., 1., 5., 0., 9., 6., 4., 2., 7., 3., 8., 1., 5., 0.,\n",
              "       9., 6., 4., 2., 7., 3., 8., 1., 5., 0., 9., 6., 4., 2., 7., 3., 8.,\n",
              "       1., 5., 0., 9., 6., 4., 2., 7., 5., 3., 6., 8., 9., 1., 5., 4., 0.,\n",
              "       7., 2., 3., 9., 2., 1., 7., 4., 6., 8., 0., 5., 3., 2., 1., 5., 6.,\n",
              "       7., 8., 0., 9., 4., 9., 7., 5., 8., 2., 3., 1., 4., 6., 0., 0., 2.,\n",
              "       0., 5., 9., 4., 7., 6., 8., 1., 3., 2., 6., 5., 9., 0., 8., 7., 3.,\n",
              "       4., 1., 2., 6., 0., 4., 8., 7., 9., 1., 5., 3., 9., 7., 3., 5., 6.,\n",
              "       2., 0., 4., 1., 8., 8., 0., 8., 9., 2., 4., 5., 7., 1., 3., 6., 8.,\n",
              "       1., 0., 7., 4., 2., 5., 3., 9., 6., 0., 3., 2., 6., 8., 1., 7., 4.,\n",
              "       5., 9., 6., 9., 1., 8., 3., 2., 0., 5., 4., 7., 7., 5., 6., 2., 9.,\n",
              "       4., 7., 0., 8., 1., 3., 9., 6., 7., 4., 0., 2., 1., 5., 3., 8., 3.,\n",
              "       4., 9., 1., 7., 6., 8., 0., 2., 5., 8., 3., 6., 7., 4., 2., 5., 1.,\n",
              "       9., 0., 0., 6., 1., 7., 4., 9., 0., 5., 3., 2., 8., 9., 8., 6., 0.,\n",
              "       3., 7., 1., 4., 2., 5., 6., 0., 5., 1., 2., 4., 9., 8., 7., 3., 4.,\n",
              "       6., 1., 0., 2., 8., 9., 5., 3., 7., 7., 4., 1., 2., 6., 7., 5., 9.,\n",
              "       0., 3., 8., 2., 9., 1., 6., 0., 4., 5., 7., 3., 8., 1., 7., 9., 0.,\n",
              "       3., 2., 6., 8., 5., 4., 1., 4., 8., 0., 3., 2., 7., 6., 5., 9., 9.,\n",
              "       8., 9., 4., 1., 6., 5., 7., 0., 2., 3., 0., 6., 2., 4., 8., 5., 1.,\n",
              "       7., 3., 9., 8., 7., 1., 9., 2., 6., 3., 5., 0., 4., 7., 9., 5., 2.,\n",
              "       0., 3., 1., 6., 8., 4., 4., 7., 0., 4., 8., 2., 6., 5., 3., 9., 1.,\n",
              "       5., 1., 4., 3., 2., 9., 8., 6., 7., 0., 9., 3., 7., 6., 0., 5., 8.,\n",
              "       4., 1., 2., 9., 5., 2., 4., 1., 0., 8., 7., 6., 3., 3., 9., 3., 4.,\n",
              "       7., 6., 5., 2., 1., 0., 8., 3., 2., 5., 0., 8., 9., 7., 6., 1., 4.,\n",
              "       6., 5., 9., 7., 3., 2., 8., 4., 1., 0., 3., 9., 2., 4., 7., 5., 0.,\n",
              "       8., 6., 1., 1., 0., 2., 4., 9., 1., 7., 6., 5., 3., 8., 4., 5., 6.,\n",
              "       3., 2., 9., 1., 7., 8., 0., 8., 2., 7., 4., 3., 0., 1., 5., 9., 6.,\n",
              "       7., 4., 5., 3., 2., 8., 0., 9., 6., 1., 1., 3., 1., 2., 5., 7., 4.,\n",
              "       9., 6., 8., 0., 1., 8., 4., 5., 6., 0., 7., 3., 2., 9., 3., 4., 0.,\n",
              "       9., 2., 6., 8., 5., 7., 1., 4., 1., 0., 9., 3., 6., 2., 8., 5., 7.,\n",
              "       7., 1., 7., 0., 2., 6., 8., 5., 9., 4., 3., 4., 1., 8., 5., 6., 3.,\n",
              "       2., 7., 0., 9., 8., 2., 3., 0., 7., 4., 9., 5., 6., 1., 2., 8., 7.,\n",
              "       1., 9., 3., 4., 6., 0., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Load your dataset here\n",
        "# For this example, let's just create dummy datasets\n",
        "num_samples = 738  # Number of samples in dummy dataset\n",
        "num_classes = 10  # The number of classes you want to classify into\n",
        "\n",
        "train_idx, val_idx, test_idx = get_index(num_samples)\n",
        "# X_train = np.zeros((len(train_idx), 10, 96, 96, 68))\n",
        "# y_train = np.zeros(( len(train_idx),))\n",
        "# X_val = np.zeros((len(val_idx), 10, 96, 96, 68))\n",
        "# y_val = np.zeros(( len(val_idx),))\n",
        "# X_test = np.zeros((len(test_idx), 10, 96, 96, 68))\n",
        "# y_test = np.zeros(( len(test_idx),))\n",
        "# count = 0\n",
        "# for i in train_idx:\n",
        "#   X_train[count,:,:,:,:] = np.load(\"Brain_\"+str(i)+\".npy\").transpose(3,0,1,2)\n",
        "#   y_train[count] = Y[i]\n",
        "#   count+=1\n",
        "# count=0\n",
        "# for i in val_idx:\n",
        "#   X_val[count,:,:,:,:] = np.load(\"Brain_\"+str(i)+\".npy\").transpose(3,0,1,2)\n",
        "#   y_val[count] = Y[i]\n",
        "#   count+=1\n",
        "# count=0\n",
        "# for i in test_idx:\n",
        "#   X_test[count,:,:,:,:] = np.load(\"Brain_\"+str(i)+\".npy\").transpose(3,0,1,2)\n",
        "#   y_test[count] = Y[i]\n",
        "#   count+=1\n",
        "np.save(\"/content/drive/MyDrive/aime/CNN/train_idx.txt\",train_idx)\n",
        "np.save(\"/content/drive/MyDrive/aime/CNN/val_idx.txt\",val_idx)\n",
        "np.save(\"/content/drive/MyDrive/aime/CNN/test_idx.txt\",test_idx)"
      ],
      "metadata": {
        "id": "xacrqWtYob8f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-QHVTz--L7Go"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "batch_size =  4\n",
        "# Function to load and preprocess your data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # Load your data from file (e.g., images, CSV, etc.)\n",
        "    #print(file_path)\n",
        "    file_path = bytes.decode(file_path.numpy())\n",
        "    data =  np.load(file_path)\n",
        "    # Preprocess the data (normalization, resizing, etc.)\n",
        "    data = data.transpose(3,0,1,2)\n",
        "    # print(data.shape)\n",
        "    #slice data\n",
        "    #data = data[4:,:,:,:]\n",
        "    data = tf.convert_to_tensor(data)\n",
        "    data = tf.expand_dims(data, axis=-1)\n",
        "    file_arr = file_path.split(\".\")\n",
        "    file_path_y = file_arr[0]+\"y.\"+ file_arr[1]\n",
        "\n",
        "    y_res = int(np.load(str(file_path_y)))\n",
        "    one_hot = np.zeros(10)\n",
        "    one_hot[y_res] = 1\n",
        "    return data, tf.convert_to_tensor(one_hot)\n",
        "\n",
        "# Create a dataset from the list of file paths\n",
        "file_paths = [\"Brain_\"+str(i)+\".npy\" for i in train_idx]\n",
        "# train_dataset = tf.data.Dataset.list_files('/content/bird.jpg')\n",
        "dataset = tf.data.Dataset.list_files(file_paths)\n",
        "\n",
        "# Map the preprocessing function across dataset elements\n",
        "dataset = dataset.map(lambda x: tf.py_function(load_and_preprocess_data, [x], [tf.float64,tf.float64]),\n",
        "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle, repeat, and batch the dataset\n",
        "dataset = dataset.shuffle(buffer_size=16).repeat().batch(batch_size)\n",
        "\n",
        "# Prefetch to improve performance\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "IO9TKF1IpDb2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, ReLU, TimeDistributed, LSTM\n",
        "\n",
        "def create_4d_cnn_with_spacetime_kernel(input_shape, num_classes):\n",
        "    # Input shape should be (batch_size, time_steps, height, width, channels)\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Example spacetime convolutional layer (extend kernel over two frames)\n",
        "    # Kernel size here is (3, 3, 3), which extends over temporal and spatial dimensions\n",
        "    conv_3d_lay1 = Conv3D(filters=32, kernel_size=(5, 5, 5), strides=(1, 1, 1), padding='same')\n",
        "    x = TimeDistributed(conv_3d_lay1)(inputs)\n",
        "    batch1 = BatchNormalization()\n",
        "    x =  TimeDistributed(batch1)(x)\n",
        "    x = ReLU()(x)\n",
        "    max_pool_lay1 = MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1))\n",
        "    x = TimeDistributed(max_pool_lay1)(x)\n",
        "\n",
        "    conv_3d_lay2 = Conv3D(filters=64, kernel_size=(4, 4, 4), strides=(1, 1, 1), padding='same')\n",
        "    x = TimeDistributed(conv_3d_lay2)(x)\n",
        "    batch2 = BatchNormalization()\n",
        "    x =  TimeDistributed(batch2)(x)\n",
        "    x = ReLU()(x)\n",
        "    max_pool_lay2 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))\n",
        "    x = TimeDistributed(max_pool_lay2)(x)\n",
        "\n",
        "    conv_3d_lay3 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(2,2 ,2), padding='same')\n",
        "    x = TimeDistributed(conv_3d_lay3)(x)\n",
        "    batch3 = BatchNormalization()\n",
        "    x =  TimeDistributed(batch3)(x)\n",
        "    x = ReLU()(x)\n",
        "    max_pool_lay3 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))\n",
        "    x = TimeDistributed(max_pool_lay3)(x)\n",
        "\n",
        "    # conv_3d_lay4 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same')\n",
        "    # x = TimeDistributed(conv_3d_lay4)(x)\n",
        "    # batch4 = BatchNormalization()\n",
        "    # x =  TimeDistributed(batch4)(x)\n",
        "    # x = ReLU()(x)\n",
        "    # max_pool_lay4 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))\n",
        "    # x = TimeDistributed(max_pool_lay4)(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = TimeDistributed(Dense(32))(x)\n",
        "    x = LSTM(64,return_sequences=False)(x)\n",
        "\n",
        "\n",
        "    # Flatten and add Dense layers at the end\n",
        "    # # Add more layers according to your specific problem, for example:\n",
        "    # x = Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 2, 2), padding='same')(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    # x = ReLU()(x)\n",
        "    # x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)\n",
        "\n",
        "    # # Flatten and add Dense layers at the end\n",
        "    # x = Flatten()(x)\n",
        "    # x = Dense(512)(x)\n",
        "    # x = ReLU()(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # # Create model\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example input shape: 10 frames (time_steps), each frame with size 64x64, and 3 channels\n",
        "input_shape = (10,21,14,17,1)\n",
        "num_classes = 10  # Example number of classes for classification\n",
        "\n",
        "model = create_4d_cnn_with_spacetime_kernel(input_shape, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "id": "TFJwEWhxfaFL",
        "outputId": "6250c484-4bf1-4d67-c586-0a0f4e1923a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_11                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │           \u001b[38;5;34m4,032\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_12                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_3 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m21\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_13                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_14                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │         \u001b[38;5;34m131,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_15                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_4 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m20\u001b[0m, \u001b[38;5;34m13\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_16                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_17                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m221,312\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_18                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_5 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_19                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_20                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m512\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_21                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │          \u001b[38;5;34m16,416\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m24,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_11                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,032</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_12                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">21</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_13                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_14                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_15                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">13</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_16                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_17                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_18                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_19                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_20                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_21                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">16,416</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m399,274\u001b[0m (1.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">399,274</span> (1.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m398,826\u001b[0m (1.52 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">398,826</span> (1.52 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "# Example input shape: 10 frames (time_steps), each frame with size 64x64, and 3 channels\n",
        "# input_shape = (10,96,96,68)\n",
        "\n",
        "num_classes = 10  # Example number of classes for classification\n",
        "\n",
        "model = create_4d_cnn_with_spacetime_kernel(input_shape, num_classes)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
        "loss_object = tf.losses.CategoricalCrossentropy(from_logits=False)\n",
        "val_acc_metric = CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "7nE_KcRppI1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        predictions = model(inputs, training=True)\n",
        "        #print(predictions)\n",
        "        #print(targets)\n",
        "        loss = loss_object(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "8ozaKLaxpKL_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def perform_validation(model, x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    return val_logits"
      ],
      "metadata": {
        "id": "IHpHw7elpLaI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 4\n",
        "steps_per_epoch = len(train_idx) // batch_size\n",
        "val_best = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    loss_tot = 0\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (inputs, targets) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        loss = train_step(inputs, targets)\n",
        "        loss_tot += loss.numpy()\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss_tot/(step+1)}\")\n",
        "\n",
        "    # End of epoch actions...\n",
        "    # Perform validation\n",
        "    for idx in val_idx:\n",
        "        x_batch_val = np.load(\"Brain_\"+str(idx)+\".npy\")\n",
        "        x_batch_val = x_batch_val.transpose(3,0,1,2)\n",
        "        x_batch_val = x_batch_val.reshape(1,*x_batch_val.shape)\n",
        "        # x_batch_val = x_batch_val.reshape(1,96,96,68,10)\n",
        "        #x_batch_val = x_batch_val[4:,:,:,:].reshape(1,6,96,96,68)\n",
        "        x_batch_val = tf.convert_to_tensor(x_batch_val)\n",
        "        x_batch_val = tf.expand_dims(x_batch_val, axis=-1)\n",
        "        y_batch_val = np.load(\"Brain_\"+str(idx)+\"y.npy\")\n",
        "        y_batch_val = int(y_batch_val)\n",
        "        one_hot = np.zeros(10)\n",
        "        one_hot[y_batch_val] = 1\n",
        "        y_batch_val = tf.convert_to_tensor(one_hot)\n",
        "        val_logits = perform_validation(model, x_batch_val, y_batch_val)\n",
        "        # Update validation metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    if(val_acc > val_best):\n",
        "        val_best = val_acc\n",
        "        model.save(\"model_alt.h5\")\n",
        "    val_acc_metric.reset_state()\n",
        "    print(\"Training Loss: %.4f\" % (float(loss_tot)/len(train_idx),))\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "NsdP8-KUgutR",
        "outputId": "1c749153-b593-43ab-b4d5-2015c10c0325"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Step 0, Loss: 2.442028522491455\n",
            "Step 100, Loss: 2.3537122436089093\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5881\n",
            "Validation acc: 0.1171\n",
            "Epoch 2/100\n",
            "Step 0, Loss: 2.322967052459717\n",
            "Step 100, Loss: 2.323551168536196\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5799\n",
            "Validation acc: 0.1351\n",
            "Epoch 3/100\n",
            "Step 0, Loss: 2.3591854572296143\n",
            "Step 100, Loss: 2.328776543683345\n",
            "Training Loss: 0.5816\n",
            "Validation acc: 0.1171\n",
            "Epoch 4/100\n",
            "Step 0, Loss: 2.2767462730407715\n",
            "Step 100, Loss: 2.3165320996010657\n",
            "Training Loss: 0.5785\n",
            "Validation acc: 0.0631\n",
            "Epoch 5/100\n",
            "Step 0, Loss: 2.381899118423462\n",
            "Step 100, Loss: 2.3147618841416766\n",
            "Training Loss: 0.5794\n",
            "Validation acc: 0.0811\n",
            "Epoch 6/100\n",
            "Step 0, Loss: 2.444847822189331\n",
            "Step 100, Loss: 2.3108694506163645\n",
            "Training Loss: 0.5782\n",
            "Validation acc: 0.0901\n",
            "Epoch 7/100\n",
            "Step 0, Loss: 2.3461756706237793\n",
            "Step 100, Loss: 2.3096656398017807\n",
            "Training Loss: 0.5772\n",
            "Validation acc: 0.0901\n",
            "Epoch 8/100\n",
            "Step 0, Loss: 2.306924343109131\n",
            "Step 100, Loss: 2.307523715614092\n",
            "Training Loss: 0.5773\n",
            "Validation acc: 0.0901\n",
            "Epoch 9/100\n",
            "Step 0, Loss: 2.253807544708252\n",
            "Step 100, Loss: 2.3079511788812015\n",
            "Training Loss: 0.5763\n",
            "Validation acc: 0.0721\n",
            "Epoch 10/100\n",
            "Step 0, Loss: 2.2553303241729736\n",
            "Step 100, Loss: 2.3022945399331576\n",
            "Training Loss: 0.5755\n",
            "Validation acc: 0.0901\n",
            "Epoch 11/100\n",
            "Step 0, Loss: 2.3884453773498535\n",
            "Step 100, Loss: 2.3101078684967344\n",
            "Training Loss: 0.5770\n",
            "Validation acc: 0.0901\n",
            "Epoch 12/100\n",
            "Step 0, Loss: 2.3423409461975098\n",
            "Step 100, Loss: 2.3068113940777164\n",
            "Training Loss: 0.5771\n",
            "Validation acc: 0.0811\n",
            "Epoch 13/100\n",
            "Step 0, Loss: 2.3326616287231445\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-5f323f663788>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss_tot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "21*14*17"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWOGwgsaKHX2",
        "outputId": "39b2346a-77bb-41d3-921f-39e40b17c3dd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4998"
            ]
          },
          "metadata": {},
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read in L_Heschl_gyrus_AAL_83.npy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the file is in the current working directory\n",
        "brain_data_LH = np.load(\"L_Heschl_gyrus_AAL_83.npy\")\n",
        "\n",
        "# Now you can work with the loaded data, e.g., print its shape\n",
        "print(brain_data_LH.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RVJn1V8YGT-v",
        "outputId": "8524f938-38bd-4e71-ff21-af52886ae8b9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 594)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: for each Brain_*.npy file zero out the voxels that does not belong in brain_data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming brain_data is a boolean array indicating brain voxels\n",
        "brain_mask = np.load(\"L_Heschl_gyrus_AAL_83.npy\")\n",
        "# brain_mask = np.load(\"/content/drive/MyDrive/aime/CNN/L_Heschl_gyrus_AAL_83.npy\")\n",
        "\n",
        "\n",
        "for j in range(num_samples):\n",
        "  file_path = \"Brain_\"+str(j)+\".npy\"\n",
        "  brain_data = np.load(file_path)\n",
        "  new_brain_data = np.zeros(brain_data.shape)\n",
        "  for i in range(brain_data_LH.shape[1]):\n",
        "    new_brain_data[brain_data_LH[0,i]-18,brain_data_LH[1,i]-67,brain_data_LH[2,i]-22,:] = brain_data[brain_data_LH[0,i]-18,brain_data_LH[1,i]-67,brain_data_LH[2,i]-22,:]\n",
        "  # Save the modified data back to the file\n",
        "  np.save(file_path, new_brain_data)\n"
      ],
      "metadata": {
        "id": "9LH1EnSEGhre"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file_path = \"Brain_1.npy\"\n",
        "brain_data = np.load(file_path)"
      ],
      "metadata": {
        "id": "fh2QhMUWG5f4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "brain_data[0,0,0,:]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BLM8nsCbG8kN",
        "outputId": "e28512f5-bea2-438e-c09e-1e00ef57dbed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-2.3394529 , -2.34392719, -2.39638441, -2.31877858, -2.34871005,\n",
              "       -2.37416723, -2.35580721, -2.37432152, -2.39607584, -2.33976147])"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "new_brain_data = np.zeros(brain_data.shape)"
      ],
      "metadata": {
        "id": "0ndbWQ1rG_7N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(brain_data_LH.shape[1]):\n",
        "  new_brain_data[brain_data_LH[0,i]-18,brain_data_LH[1,i]-67,brain_data_LH[2,i]-22,:] = brain_data[brain_data_LH[0,i]-18,brain_data_LH[1,i]-67,brain_data_LH[2,i]-22,:]"
      ],
      "metadata": {
        "id": "aCSpHd9hHTsH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_brain_data[:,:,10,1]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OjRf9oHmH9pk",
        "outputId": "8373a4a5-7b9d-4542-9361-7b1e0596612f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.94930613,  0.96735759,\n",
              "         0.98849476,  0.73716297,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.19731648,  0.38261389,\n",
              "         0.40961393,  0.25100798,  0.12094494, -0.12699256, -1.15114261,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.13328781,  0.12341351,\n",
              "         0.1849736 ,  0.28603089,  0.17772216,  0.46777972,  0.9068775 ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        , -0.24332416, -0.08950108,\n",
              "         0.18589932,  0.38307674,  0.40020248,  0.57176844,  0.90518035,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.42504252,  0.24190511,\n",
              "         0.41809965,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.62206566,  0.77172302,\n",
              "         0.48691118,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ],\n",
              "       [ 0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ,  0.        ,\n",
              "         0.        ,  0.        ,  0.        ,  0.        ]])"
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_samples"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qPNMrzenIPjf",
        "outputId": "daee0c46-8ceb-4856-897d-de7b3ee1235a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "738"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 4\n",
        "steps_per_epoch = len(train_idx) // batch_size\n",
        "val_best = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    loss_tot = 0\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (inputs, targets) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        loss = train_step(inputs, targets)\n",
        "        loss_tot += loss.numpy()\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss_tot/(step+1)}\")\n",
        "\n",
        "    # End of epoch actions...\n",
        "    # Perform validation\n",
        "    for idx in val_idx:\n",
        "        x_batch_val = np.load(\"Brain_\"+str(idx)+\".npy\")\n",
        "        x_batch_val = x_batch_val.transpose(3,0,1,2)\n",
        "        x_batch_val = x_batch_val.reshape(1,*x_batch_val.shape)\n",
        "        # x_batch_val = x_batch_val.reshape(1,96,96,68,10)\n",
        "        #x_batch_val = x_batch_val[4:,:,:,:].reshape(1,6,96,96,68)\n",
        "        x_batch_val = tf.convert_to_tensor(x_batch_val)\n",
        "        x_batch_val = tf.expand_dims(x_batch_val, axis=-1)\n",
        "        y_batch_val = np.load(\"Brain_\"+str(idx)+\"y.npy\")\n",
        "        y_batch_val = int(y_batch_val)\n",
        "        one_hot = np.zeros(10)\n",
        "        one_hot[y_batch_val] = 1\n",
        "        y_batch_val = tf.convert_to_tensor(one_hot)\n",
        "        val_logits = perform_validation(model, x_batch_val, y_batch_val)\n",
        "        # Update validation metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    if(val_acc > val_best):\n",
        "        val_best = val_acc\n",
        "        model.save(\"model_alt.h5\")\n",
        "    val_acc_metric.reset_state()\n",
        "    print(\"Training Loss: %.4f\" % (float(loss_tot)/len(train_idx),))\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "BIEo9EXLI2xd",
        "outputId": "e49ffd4b-c9d1-4ed4-9dc3-2d0717dfda52"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Step 0, Loss: 2.2542576789855957\n",
            "Step 100, Loss: 2.327776037820495\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5810\n",
            "Validation acc: 0.0631\n",
            "Epoch 2/100\n",
            "Step 0, Loss: 2.3220129013061523\n",
            "Step 100, Loss: 2.3129209150182137\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5780\n",
            "Validation acc: 0.0901\n",
            "Epoch 3/100\n",
            "Step 0, Loss: 2.2994589805603027\n",
            "Step 100, Loss: 2.305446702654999\n",
            "Training Loss: 0.5769\n",
            "Validation acc: 0.0811\n",
            "Epoch 4/100\n",
            "Step 0, Loss: 2.2374677658081055\n",
            "Step 100, Loss: 2.3056893820809847\n",
            "Training Loss: 0.5766\n",
            "Validation acc: 0.0450\n",
            "Epoch 5/100\n",
            "Step 0, Loss: 2.2740252017974854\n",
            "Step 100, Loss: 2.3060994478735592\n",
            "Training Loss: 0.5764\n",
            "Validation acc: 0.0721\n",
            "Epoch 6/100\n",
            "Step 0, Loss: 2.3098349571228027\n",
            "Step 100, Loss: 2.302339688386067\n",
            "Training Loss: 0.5760\n",
            "Validation acc: 0.0811\n",
            "Epoch 7/100\n",
            "Step 0, Loss: 2.3171310424804688\n",
            "Step 100, Loss: 2.3043783773290047\n",
            "Training Loss: 0.5760\n",
            "Validation acc: 0.0721\n",
            "Epoch 8/100\n",
            "Step 0, Loss: 2.2666139602661133\n",
            "Step 100, Loss: 2.3060050317556553\n",
            "Training Loss: 0.5761\n",
            "Validation acc: 0.0811\n",
            "Epoch 9/100\n",
            "Step 0, Loss: 2.325131416320801\n",
            "Step 100, Loss: 2.303071673553769\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0360\n",
            "Epoch 10/100\n",
            "Step 0, Loss: 2.313225030899048\n",
            "Step 100, Loss: 2.3013240323208346\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0721\n",
            "Epoch 11/100\n",
            "Step 0, Loss: 2.3312203884124756\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-32-5f323f663788>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss_tot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Build a simple LSTM"
      ],
      "metadata": {
        "id": "YW2FeO7hI5YB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a simple LSTM model that takes in (594,10) input and output a 10 class classification for 10 temporal steps\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Build the LSTM model\n",
        "model_l = Sequential()\n",
        "model_l.add(LSTM(units=64, return_sequences=True, input_shape=(10, 594)))  # Adjust units as needed\n",
        "model_l.add(LSTM(units=32, return_sequences=False))\n",
        "model_l.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_l.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_l.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "id": "pd-gfrQPMY-A",
        "outputId": "51ee1b26-7c8f-4def-a96c-aac6839bc000"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_5 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │         \u001b[38;5;34m168,704\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">168,704</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m181,450\u001b[0m (708.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">181,450</span> (708.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m181,450\u001b[0m (708.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">181,450</span> (708.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_LSTM = np.zeros((738,10,594))\n",
        "Y_LSTM = np.zeros(738)\n",
        "for i in range(738):\n",
        "  meta_data = np.load(\"Brain_\"+str(i)+\".npy\")\n",
        "  meta_data_y = np.load(\"Brain_\"+str(i)+\"y.npy\")\n",
        "  Y_LSTM[i] = meta_data_y\n",
        "  for j in range(brain_data_LH.shape[1]):\n",
        "    X_LSTM[i,:,j] = meta_data[brain_data_LH[0,j]-18,brain_data_LH[1,j]-67,brain_data_LH[2,j]-22,:]\n",
        "\n"
      ],
      "metadata": {
        "id": "8b7PiJ9pNCTt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_LSTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y4OOHcZSOZOL",
        "outputId": "081c62f0-50dd-4cfb-948e-6d549a72f708"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8., 2., 4., 3., 5., 0., 7., 1., 6., 9., 8., 2., 4., 3., 5., 0., 7.,\n",
              "       1., 6., 9., 8., 2., 4., 3., 5., 0., 7., 1., 6., 9., 8., 2., 4., 3.,\n",
              "       5., 0., 7., 1., 6., 9., 8., 0., 7., 2., 6., 8., 4., 9., 3., 1., 5.,\n",
              "       0., 7., 2., 6., 8., 4., 9., 3., 1., 5., 0., 7., 2., 6., 8., 4., 9.,\n",
              "       3., 1., 5., 0., 7., 2., 6., 8., 4., 9., 3., 1., 5., 0., 9., 6., 3.,\n",
              "       2., 4., 8., 5., 1., 0., 7., 9., 6., 3., 2., 4., 8., 5., 1., 0., 7.,\n",
              "       9., 6., 3., 2., 4., 8., 5., 1., 0., 7., 9., 6., 3., 2., 4., 8., 5.,\n",
              "       1., 0., 7., 9., 9., 0., 4., 2., 7., 3., 6., 5., 1., 8., 9., 0., 4.,\n",
              "       2., 7., 3., 6., 5., 1., 8., 9., 0., 4., 2., 7., 3., 6., 5., 1., 8.,\n",
              "       9., 0., 4., 2., 7., 3., 6., 5., 1., 8., 9., 7., 8., 4., 1., 0., 5.,\n",
              "       6., 2., 3., 9., 7., 8., 4., 1., 0., 5., 6., 2., 3., 9., 7., 8., 4.,\n",
              "       1., 0., 5., 6., 2., 3., 9., 7., 8., 4., 1., 0., 5., 6., 2., 3., 9.,\n",
              "       7., 7., 3., 8., 1., 5., 0., 9., 6., 4., 2., 7., 3., 8., 1., 5., 0.,\n",
              "       9., 6., 4., 2., 7., 3., 8., 1., 5., 0., 9., 6., 4., 2., 7., 3., 8.,\n",
              "       1., 5., 0., 9., 6., 4., 2., 7., 5., 3., 6., 8., 9., 1., 5., 4., 0.,\n",
              "       7., 2., 3., 9., 2., 1., 7., 4., 6., 8., 0., 5., 3., 2., 1., 5., 6.,\n",
              "       7., 8., 0., 9., 4., 9., 7., 5., 8., 2., 3., 1., 4., 6., 0., 0., 2.,\n",
              "       0., 5., 9., 4., 7., 6., 8., 1., 3., 2., 6., 5., 9., 0., 8., 7., 3.,\n",
              "       4., 1., 2., 6., 0., 4., 8., 7., 9., 1., 5., 3., 9., 7., 3., 5., 6.,\n",
              "       2., 0., 4., 1., 8., 8., 0., 8., 9., 2., 4., 5., 7., 1., 3., 6., 8.,\n",
              "       1., 0., 7., 4., 2., 5., 3., 9., 6., 0., 3., 2., 6., 8., 1., 7., 4.,\n",
              "       5., 9., 6., 9., 1., 8., 3., 2., 0., 5., 4., 7., 7., 5., 6., 2., 9.,\n",
              "       4., 7., 0., 8., 1., 3., 9., 6., 7., 4., 0., 2., 1., 5., 3., 8., 3.,\n",
              "       4., 9., 1., 7., 6., 8., 0., 2., 5., 8., 3., 6., 7., 4., 2., 5., 1.,\n",
              "       9., 0., 0., 6., 1., 7., 4., 9., 0., 5., 3., 2., 8., 9., 8., 6., 0.,\n",
              "       3., 7., 1., 4., 2., 5., 6., 0., 5., 1., 2., 4., 9., 8., 7., 3., 4.,\n",
              "       6., 1., 0., 2., 8., 9., 5., 3., 7., 7., 4., 1., 2., 6., 7., 5., 9.,\n",
              "       0., 3., 8., 2., 9., 1., 6., 0., 4., 5., 7., 3., 8., 1., 7., 9., 0.,\n",
              "       3., 2., 6., 8., 5., 4., 1., 4., 8., 0., 3., 2., 7., 6., 5., 9., 9.,\n",
              "       8., 9., 4., 1., 6., 5., 7., 0., 2., 3., 0., 6., 2., 4., 8., 5., 1.,\n",
              "       7., 3., 9., 8., 7., 1., 9., 2., 6., 3., 5., 0., 4., 7., 9., 5., 2.,\n",
              "       0., 3., 1., 6., 8., 4., 4., 7., 0., 4., 8., 2., 6., 5., 3., 9., 1.,\n",
              "       5., 1., 4., 3., 2., 9., 8., 6., 7., 0., 9., 3., 7., 6., 0., 5., 8.,\n",
              "       4., 1., 2., 9., 5., 2., 4., 1., 0., 8., 7., 6., 3., 3., 9., 3., 4.,\n",
              "       7., 6., 5., 2., 1., 0., 8., 3., 2., 5., 0., 8., 9., 7., 6., 1., 4.,\n",
              "       6., 5., 9., 7., 3., 2., 8., 4., 1., 0., 3., 9., 2., 4., 7., 5., 0.,\n",
              "       8., 6., 1., 1., 0., 2., 4., 9., 1., 7., 6., 5., 3., 8., 4., 5., 6.,\n",
              "       3., 2., 9., 1., 7., 8., 0., 8., 2., 7., 4., 3., 0., 1., 5., 9., 6.,\n",
              "       7., 4., 5., 3., 2., 8., 0., 9., 6., 1., 1., 3., 1., 2., 5., 7., 4.,\n",
              "       9., 6., 8., 0., 1., 8., 4., 5., 6., 0., 7., 3., 2., 9., 3., 4., 0.,\n",
              "       9., 2., 6., 8., 5., 7., 1., 4., 1., 0., 9., 3., 6., 2., 8., 5., 7.,\n",
              "       7., 1., 7., 0., 2., 6., 8., 5., 9., 4., 3., 4., 1., 8., 5., 6., 3.,\n",
              "       2., 7., 0., 9., 8., 2., 3., 0., 7., 4., 9., 5., 6., 1., 2., 8., 7.,\n",
              "       1., 9., 3., 4., 6., 0., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate the training loop for X_LSTM and Y_LSTM for model_l\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert Y_LSTM to one-hot encoding\n",
        "Y_LSTM_cat = to_categorical(Y_LSTM, num_classes=10)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "X_train_l = X_LSTM[train_idx]\n",
        "y_train_l = Y_LSTM_cat[train_idx]\n",
        "X_val_l = X_LSTM[val_idx]\n",
        "y_val_l = Y_LSTM_cat[val_idx]\n",
        "X_test_l = X_LSTM[test_idx]\n",
        "y_test_l = Y_LSTM_cat[test_idx]\n",
        "\n",
        "# Train the LSTM model\n",
        "epochs = 100\n",
        "batch_size = 4\n",
        "history = model_l.fit(\n",
        "    X_train_l, y_train_l,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val_l, y_val_l)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0wVu5805OKBW",
        "outputId": "5c24f6e6-77fb-4a58-b2b1-c157506788c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 7ms/step - accuracy: 0.1089 - loss: 2.3778 - val_accuracy: 0.0901 - val_loss: 2.3187\n",
            "Epoch 2/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1255 - loss: 2.3083 - val_accuracy: 0.0901 - val_loss: 2.3079\n",
            "Epoch 3/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1274 - loss: 2.3062 - val_accuracy: 0.1171 - val_loss: 2.3089\n",
            "Epoch 4/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0678 - loss: 2.3113 - val_accuracy: 0.0450 - val_loss: 2.3181\n",
            "Epoch 5/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0807 - loss: 2.3144 - val_accuracy: 0.0450 - val_loss: 2.3147\n",
            "Epoch 6/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1095 - loss: 2.3027 - val_accuracy: 0.0901 - val_loss: 2.3226\n",
            "Epoch 7/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0883 - loss: 2.3092 - val_accuracy: 0.0631 - val_loss: 2.3173\n",
            "Epoch 8/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0819 - loss: 2.3049 - val_accuracy: 0.0631 - val_loss: 2.3117\n",
            "Epoch 9/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0990 - loss: 2.3061 - val_accuracy: 0.0631 - val_loss: 2.3141\n",
            "Epoch 10/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0657 - loss: 2.3063 - val_accuracy: 0.0631 - val_loss: 2.3079\n",
            "Epoch 11/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0589 - loss: 2.3083 - val_accuracy: 0.0631 - val_loss: 2.3099\n",
            "Epoch 12/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1104 - loss: 2.3020 - val_accuracy: 0.0631 - val_loss: 2.3110\n",
            "Epoch 13/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1156 - loss: 2.3035 - val_accuracy: 0.0631 - val_loss: 2.3151\n",
            "Epoch 14/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0767 - loss: 2.3059 - val_accuracy: 0.0631 - val_loss: 2.3121\n",
            "Epoch 15/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0771 - loss: 2.2984 - val_accuracy: 0.0631 - val_loss: 2.3169\n",
            "Epoch 16/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0939 - loss: 2.3033 - val_accuracy: 0.0901 - val_loss: 2.3127\n",
            "Epoch 17/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0914 - loss: 2.3050 - val_accuracy: 0.0541 - val_loss: 2.3156\n",
            "Epoch 18/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0962 - loss: 2.2996 - val_accuracy: 0.1171 - val_loss: 2.3145\n",
            "Epoch 19/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0920 - loss: 2.3084 - val_accuracy: 0.1171 - val_loss: 2.3113\n",
            "Epoch 20/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0971 - loss: 2.3066 - val_accuracy: 0.0450 - val_loss: 2.3166\n",
            "Epoch 21/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0962 - loss: 2.2992 - val_accuracy: 0.0721 - val_loss: 2.3169\n",
            "Epoch 22/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1021 - loss: 2.3033 - val_accuracy: 0.1171 - val_loss: 2.3137\n",
            "Epoch 23/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0969 - loss: 2.2986 - val_accuracy: 0.0631 - val_loss: 2.3179\n",
            "Epoch 24/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1355 - loss: 2.2964 - val_accuracy: 0.0901 - val_loss: 2.3153\n",
            "Epoch 25/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0953 - loss: 2.3036 - val_accuracy: 0.0721 - val_loss: 2.3147\n",
            "Epoch 26/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1005 - loss: 2.2991 - val_accuracy: 0.0631 - val_loss: 2.3146\n",
            "Epoch 27/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0977 - loss: 2.3003 - val_accuracy: 0.0541 - val_loss: 2.3172\n",
            "Epoch 28/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1086 - loss: 2.2985 - val_accuracy: 0.0721 - val_loss: 2.3180\n",
            "Epoch 29/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1025 - loss: 2.2982 - val_accuracy: 0.0360 - val_loss: 2.3192\n",
            "Epoch 30/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1053 - loss: 2.3033 - val_accuracy: 0.0811 - val_loss: 2.3231\n",
            "Epoch 31/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1236 - loss: 2.2985 - val_accuracy: 0.0631 - val_loss: 2.3199\n",
            "Epoch 32/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1172 - loss: 2.2982 - val_accuracy: 0.1081 - val_loss: 2.3211\n",
            "Epoch 33/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1134 - loss: 2.2993 - val_accuracy: 0.0721 - val_loss: 2.3188\n",
            "Epoch 34/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0844 - loss: 2.3022 - val_accuracy: 0.0721 - val_loss: 2.3231\n",
            "Epoch 35/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1216 - loss: 2.3034 - val_accuracy: 0.1081 - val_loss: 2.3262\n",
            "Epoch 36/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1021 - loss: 2.2985 - val_accuracy: 0.0631 - val_loss: 2.3173\n",
            "Epoch 37/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1248 - loss: 2.2978 - val_accuracy: 0.1081 - val_loss: 2.3235\n",
            "Epoch 38/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1146 - loss: 2.2993 - val_accuracy: 0.0631 - val_loss: 2.3224\n",
            "Epoch 39/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1113 - loss: 2.2970 - val_accuracy: 0.0631 - val_loss: 2.3211\n",
            "Epoch 40/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0982 - loss: 2.3001 - val_accuracy: 0.0541 - val_loss: 2.3208\n",
            "Epoch 41/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1237 - loss: 2.3000 - val_accuracy: 0.0991 - val_loss: 2.3233\n",
            "Epoch 42/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1167 - loss: 2.3011 - val_accuracy: 0.0450 - val_loss: 2.3297\n",
            "Epoch 43/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1208 - loss: 2.2959 - val_accuracy: 0.0450 - val_loss: 2.3231\n",
            "Epoch 44/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1329 - loss: 2.2955 - val_accuracy: 0.0811 - val_loss: 2.3338\n",
            "Epoch 45/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0912 - loss: 2.2994 - val_accuracy: 0.0450 - val_loss: 2.3366\n",
            "Epoch 46/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1007 - loss: 2.2897 - val_accuracy: 0.0450 - val_loss: 2.3382\n",
            "Epoch 47/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1053 - loss: 2.2922 - val_accuracy: 0.1261 - val_loss: 2.3284\n",
            "Epoch 48/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1219 - loss: 2.3003 - val_accuracy: 0.0541 - val_loss: 2.3333\n",
            "Epoch 49/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1274 - loss: 2.2935 - val_accuracy: 0.0360 - val_loss: 2.3397\n",
            "Epoch 50/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1256 - loss: 2.2917 - val_accuracy: 0.0631 - val_loss: 2.3457\n",
            "Epoch 51/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1385 - loss: 2.2868 - val_accuracy: 0.0811 - val_loss: 2.3531\n",
            "Epoch 52/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1277 - loss: 2.2864 - val_accuracy: 0.0450 - val_loss: 2.3430\n",
            "Epoch 53/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1074 - loss: 2.2917 - val_accuracy: 0.0450 - val_loss: 2.3567\n",
            "Epoch 54/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1064 - loss: 2.2886 - val_accuracy: 0.0541 - val_loss: 2.3473\n",
            "Epoch 55/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1157 - loss: 2.2862 - val_accuracy: 0.0541 - val_loss: 2.3491\n",
            "Epoch 56/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1294 - loss: 2.2882 - val_accuracy: 0.0541 - val_loss: 2.3632\n",
            "Epoch 57/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1154 - loss: 2.2812 - val_accuracy: 0.0450 - val_loss: 2.3477\n",
            "Epoch 58/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1343 - loss: 2.2975 - val_accuracy: 0.0360 - val_loss: 2.3519\n",
            "Epoch 59/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1331 - loss: 2.2952 - val_accuracy: 0.0631 - val_loss: 2.3617\n",
            "Epoch 60/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1754 - loss: 2.2790 - val_accuracy: 0.0450 - val_loss: 2.3705\n",
            "Epoch 61/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1291 - loss: 2.2869 - val_accuracy: 0.0631 - val_loss: 2.3966\n",
            "Epoch 62/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0983 - loss: 2.2816 - val_accuracy: 0.0450 - val_loss: 2.3474\n",
            "Epoch 63/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1368 - loss: 2.2868 - val_accuracy: 0.0450 - val_loss: 2.3922\n",
            "Epoch 64/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1551 - loss: 2.2805 - val_accuracy: 0.0631 - val_loss: 2.3717\n",
            "Epoch 65/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1464 - loss: 2.2738 - val_accuracy: 0.0450 - val_loss: 2.3807\n",
            "Epoch 66/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1250 - loss: 2.2762 - val_accuracy: 0.0360 - val_loss: 2.4339\n",
            "Epoch 67/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1430 - loss: 2.2685 - val_accuracy: 0.0541 - val_loss: 2.4451\n",
            "Epoch 68/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1153 - loss: 2.2867 - val_accuracy: 0.0450 - val_loss: 2.4304\n",
            "Epoch 69/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1629 - loss: 2.2389 - val_accuracy: 0.0541 - val_loss: 2.3817\n",
            "Epoch 70/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1699 - loss: 2.2349 - val_accuracy: 0.0541 - val_loss: 2.3637\n",
            "Epoch 71/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1155 - loss: 2.2787 - val_accuracy: 0.0541 - val_loss: 2.4116\n",
            "Epoch 72/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1723 - loss: 2.2536 - val_accuracy: 0.0721 - val_loss: 2.4077\n",
            "Epoch 73/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1314 - loss: 2.2471 - val_accuracy: 0.0631 - val_loss: 2.4159\n",
            "Epoch 74/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1298 - loss: 2.2285 - val_accuracy: 0.0721 - val_loss: 2.3606\n",
            "Epoch 75/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1784 - loss: 2.2361 - val_accuracy: 0.0901 - val_loss: 2.4301\n",
            "Epoch 76/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1314 - loss: 2.2450 - val_accuracy: 0.0541 - val_loss: 2.4507\n",
            "Epoch 77/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1533 - loss: 2.2389 - val_accuracy: 0.0631 - val_loss: 2.4168\n",
            "Epoch 78/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1976 - loss: 2.1985 - val_accuracy: 0.0721 - val_loss: 2.4199\n",
            "Epoch 79/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1744 - loss: 2.2107 - val_accuracy: 0.0721 - val_loss: 2.4221\n",
            "Epoch 80/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2182 - loss: 2.1880 - val_accuracy: 0.0631 - val_loss: 2.4976\n",
            "Epoch 81/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1803 - loss: 2.1913 - val_accuracy: 0.0721 - val_loss: 2.4153\n",
            "Epoch 82/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1758 - loss: 2.2176 - val_accuracy: 0.0541 - val_loss: 2.4624\n",
            "Epoch 83/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1915 - loss: 2.1950 - val_accuracy: 0.0450 - val_loss: 2.5302\n",
            "Epoch 84/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2016 - loss: 2.1641 - val_accuracy: 0.1081 - val_loss: 2.4505\n",
            "Epoch 85/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1999 - loss: 2.1733 - val_accuracy: 0.0901 - val_loss: 2.4760\n",
            "Epoch 86/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2178 - loss: 2.1387 - val_accuracy: 0.0811 - val_loss: 2.5347\n",
            "Epoch 87/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1922 - loss: 2.1652 - val_accuracy: 0.0811 - val_loss: 2.5370\n",
            "Epoch 88/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2057 - loss: 2.0976 - val_accuracy: 0.0811 - val_loss: 2.4901\n",
            "Epoch 89/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2077 - loss: 2.1173 - val_accuracy: 0.0721 - val_loss: 2.5018\n",
            "Epoch 90/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2374 - loss: 2.0959 - val_accuracy: 0.0991 - val_loss: 2.5329\n",
            "Epoch 91/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2640 - loss: 2.0197 - val_accuracy: 0.0811 - val_loss: 2.5431\n",
            "Epoch 92/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2549 - loss: 2.0585 - val_accuracy: 0.1171 - val_loss: 2.5783\n",
            "Epoch 93/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3083 - loss: 2.0120 - val_accuracy: 0.0991 - val_loss: 2.5995\n",
            "Epoch 94/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3180 - loss: 1.9848 - val_accuracy: 0.1261 - val_loss: 2.5589\n",
            "Epoch 95/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2515 - loss: 2.0041 - val_accuracy: 0.1081 - val_loss: 2.6069\n",
            "Epoch 96/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2861 - loss: 1.9815 - val_accuracy: 0.0631 - val_loss: 2.6961\n",
            "Epoch 97/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3234 - loss: 1.9064 - val_accuracy: 0.0811 - val_loss: 2.6461\n",
            "Epoch 98/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3718 - loss: 1.8496 - val_accuracy: 0.0811 - val_loss: 2.6859\n",
            "Epoch 99/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3447 - loss: 1.8330 - val_accuracy: 0.0721 - val_loss: 2.6687\n",
            "Epoch 100/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3439 - loss: 1.8515 - val_accuracy: 0.1081 - val_loss: 2.7274\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "j-oZODCbOgvk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### R_Hesch_gyrus"
      ],
      "metadata": {
        "id": "ETiEccoTWj_X"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeVv2CCLWwIN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def standardize_tensor(tensor):\n",
        "    mean = np.mean(tensor)\n",
        "    std_dev = np.std(tensor)\n",
        "    return (tensor - mean) / std_dev\n",
        "# Save the training files on disk\n",
        "import nibabel as nib\n",
        "import pandas as pd\n",
        "count = 0\n",
        "for k in range (18):\n",
        "    df = pd.read_csv(file_list[k], sep=\"\\t\")\n",
        "\n",
        "    # load fmri\n",
        "    #fmri =nib.load(fmri_list[arr_mapping[k]])\n",
        "    fmri =nib.load(fmri_list[k])\n",
        "    brain_run_data = fmri.get_fdata()\n",
        "    # load fmri\n",
        "    for j in range(len(df)):\n",
        "        # add Y\n",
        "        row_info = df.iloc[j]\n",
        "        Y[count] = list_genre.index(row_info[\"genre\"])\n",
        "        file_name = \"Brain_\"+str(count)+\".npy\"\n",
        "        # save fmri\n",
        "        # np.save(file_name,standardize_tensor(brain_run_data[:,:,:,j*10:10*j+10]))\n",
        "        np.save(file_name,standardize_tensor(brain_run_data[60:83,67:79,23:40,j*10:10*j+10]))\n",
        "        file_name = \"Brain_\"+str(count)+\"y.npy\"\n",
        "        # save fmri\n",
        "        np.save(file_name,Y[count])\n",
        "        count+=1\n"
      ],
      "metadata": {
        "id": "GxKFDX89W93L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_index(n_sample):\n",
        "  # Change to 0.7\n",
        "  train_idx = np.random.choice(n_sample, size=int(n_sample*0.7), replace=False)\n",
        "  val_and_test = [i for i in range(n_sample) if i not in train_idx]\n",
        "  vt_size = len(val_and_test)\n",
        "  val_index = np.random.choice(vt_size, size=int(vt_size*0.5), replace=False)\n",
        "  val_idx = [val_and_test[i] for i in val_index]\n",
        "  test_idx = [i for i in range(n_sample) if i not in train_idx and i not in val_idx]\n",
        "  return train_idx, val_idx, test_idx"
      ],
      "metadata": {
        "id": "MF2bWhanW93M"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from re import X\n",
        "\n",
        "# Example usage\n",
        "\n",
        "# Load your dataset here\n",
        "# For this example, let's just create dummy datasets\n",
        "num_samples = 738  # Number of samples in dummy dataset\n",
        "num_classes = 10  # The number of classes you want to classify into\n",
        "\n",
        "train_idx, val_idx, test_idx = get_index(num_samples)\n",
        "# X_train = np.zeros((len(train_idx), 10, 96, 96, 68))\n",
        "# y_train = np.zeros(( len(train_idx),))\n",
        "# X_val = np.zeros((len(val_idx), 10, 96, 96, 68))\n",
        "# y_val = np.zeros(( len(val_idx),))\n",
        "# X_test = np.zeros((len(test_idx), 10, 96, 96, 68))\n",
        "# y_test = np.zeros(( len(test_idx),))\n",
        "# count = 0\n",
        "# for i in train_idx:\n",
        "#   X_train[count,:,:,:,:] = np.load(\"Brain_\"+str(i)+\".npy\").transpose(3,0,1,2)\n",
        "#   y_train[count] = Y[i]\n",
        "#   count+=1\n",
        "# count=0\n",
        "# for i in val_idx:\n",
        "#   X_val[count,:,:,:,:] = np.load(\"Brain_\"+str(i)+\".npy\").transpose(3,0,1,2)\n",
        "#   y_val[count] = Y[i]\n",
        "#   count+=1\n",
        "# count=0\n",
        "# for i in test_idx:\n",
        "#   X_test[count,:,:,:,:] = np.load(\"Brain_\"+str(i)+\".npy\").transpose(3,0,1,2)\n",
        "#   y_test[count] = Y[i]\n",
        "#   count+=1\n",
        "np.save(\"/content/drive/MyDrive/aime/CNN/train_idx.txt\",train_idx)\n",
        "np.save(\"/content/drive/MyDrive/aime/CNN/val_idx.txt\",val_idx)\n",
        "np.save(\"/content/drive/MyDrive/aime/CNN/test_idx.txt\",test_idx)"
      ],
      "metadata": {
        "id": "nnXHwQ5rW93N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "from pathlib import Path\n",
        "batch_size =  4\n",
        "# Function to load and preprocess your data\n",
        "def load_and_preprocess_data(file_path):\n",
        "    # Load your data from file (e.g., images, CSV, etc.)\n",
        "    #print(file_path)\n",
        "    file_path = bytes.decode(file_path.numpy())\n",
        "    data =  np.load(file_path)\n",
        "    # Preprocess the data (normalization, resizing, etc.)\n",
        "    data = data.transpose(3,0,1,2)\n",
        "    # print(data.shape)\n",
        "    #slice data\n",
        "    #data = data[4:,:,:,:]\n",
        "    data = tf.convert_to_tensor(data)\n",
        "    data = tf.expand_dims(data, axis=-1)\n",
        "    file_arr = file_path.split(\".\")\n",
        "    file_path_y = file_arr[0]+\"y.\"+ file_arr[1]\n",
        "\n",
        "    y_res = int(np.load(str(file_path_y)))\n",
        "    one_hot = np.zeros(10)\n",
        "    one_hot[y_res] = 1\n",
        "    return data, tf.convert_to_tensor(one_hot)\n",
        "\n",
        "# Create a dataset from the list of file paths\n",
        "file_paths = [\"Brain_\"+str(i)+\".npy\" for i in train_idx]\n",
        "# train_dataset = tf.data.Dataset.list_files('/content/bird.jpg')\n",
        "dataset = tf.data.Dataset.list_files(file_paths)\n",
        "\n",
        "# Map the preprocessing function across dataset elements\n",
        "dataset = dataset.map(lambda x: tf.py_function(load_and_preprocess_data, [x], [tf.float64,tf.float64]),\n",
        "                      num_parallel_calls=tf.data.AUTOTUNE)\n",
        "\n",
        "# Shuffle, repeat, and batch the dataset\n",
        "dataset = dataset.shuffle(buffer_size=16).repeat().batch(batch_size)\n",
        "\n",
        "# Prefetch to improve performance\n",
        "dataset = dataset.prefetch(tf.data.AUTOTUNE)"
      ],
      "metadata": {
        "id": "3cYx9g8DW93N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Conv3D, MaxPooling3D, Flatten, Dense, Input, BatchNormalization, ReLU, TimeDistributed, LSTM\n",
        "\n",
        "def create_4d_cnn_with_spacetime_kernel(input_shape, num_classes):\n",
        "    # Input shape should be (batch_size, time_steps, height, width, channels)\n",
        "    inputs = Input(shape=input_shape)\n",
        "\n",
        "    # Example spacetime convolutional layer (extend kernel over two frames)\n",
        "    # Kernel size here is (3, 3, 3), which extends over temporal and spatial dimensions\n",
        "    conv_3d_lay1 = Conv3D(filters=32, kernel_size=(5, 5, 5), strides=(1, 1, 1), padding='same')\n",
        "    x = TimeDistributed(conv_3d_lay1)(inputs)\n",
        "    batch1 = BatchNormalization()\n",
        "    x =  TimeDistributed(batch1)(x)\n",
        "    x = ReLU()(x)\n",
        "    max_pool_lay1 = MaxPooling3D(pool_size=(2, 2, 2), strides=(1, 1, 1))\n",
        "    x = TimeDistributed(max_pool_lay1)(x)\n",
        "\n",
        "    conv_3d_lay2 = Conv3D(filters=64, kernel_size=(4, 4, 4), strides=(1, 1, 1), padding='same')\n",
        "    x = TimeDistributed(conv_3d_lay2)(x)\n",
        "    batch2 = BatchNormalization()\n",
        "    x =  TimeDistributed(batch2)(x)\n",
        "    x = ReLU()(x)\n",
        "    max_pool_lay2 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))\n",
        "    x = TimeDistributed(max_pool_lay2)(x)\n",
        "\n",
        "    conv_3d_lay3 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(2,2 ,2), padding='same')\n",
        "    x = TimeDistributed(conv_3d_lay3)(x)\n",
        "    batch3 = BatchNormalization()\n",
        "    x =  TimeDistributed(batch3)(x)\n",
        "    x = ReLU()(x)\n",
        "    max_pool_lay3 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))\n",
        "    x = TimeDistributed(max_pool_lay3)(x)\n",
        "\n",
        "    # conv_3d_lay4 = Conv3D(filters=128, kernel_size=(3, 3, 3), strides=(1, 1, 1), padding='same')\n",
        "    # x = TimeDistributed(conv_3d_lay4)(x)\n",
        "    # batch4 = BatchNormalization()\n",
        "    # x =  TimeDistributed(batch4)(x)\n",
        "    # x = ReLU()(x)\n",
        "    # max_pool_lay4 = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))\n",
        "    # x = TimeDistributed(max_pool_lay4)(x)\n",
        "    x = TimeDistributed(Flatten())(x)\n",
        "    x = TimeDistributed(Dense(32))(x)\n",
        "    x = LSTM(64,return_sequences=False)(x)\n",
        "\n",
        "\n",
        "    # Flatten and add Dense layers at the end\n",
        "    # # Add more layers according to your specific problem, for example:\n",
        "    # x = Conv3D(filters=64, kernel_size=(3, 3, 3), strides=(1, 2, 2), padding='same')(x)\n",
        "    # x = BatchNormalization()(x)\n",
        "    # x = ReLU()(x)\n",
        "    # x = MaxPooling3D(pool_size=(2, 2, 2), strides=(2, 2, 2))(x)\n",
        "\n",
        "    # # Flatten and add Dense layers at the end\n",
        "    # x = Flatten()(x)\n",
        "    # x = Dense(512)(x)\n",
        "    # x = ReLU()(x)\n",
        "    x = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    # # Create model\n",
        "    model = Model(inputs=inputs, outputs=x)\n",
        "\n",
        "    return model\n",
        "\n",
        "# Example input shape: 10 frames (time_steps), each frame with size 64x64, and 3 channels\n",
        "input_shape = (10,23,12,17,1)\n",
        "num_classes = 10  # Example number of classes for classification\n",
        "\n",
        "model = create_4d_cnn_with_spacetime_kernel(input_shape, num_classes)\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 901
        },
        "outputId": "486438b6-80ea-4c07-f3ae-a8804422789f",
        "id": "b04tI2NJW93N"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_9\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_9\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m1\u001b[0m)   │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_33                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │           \u001b[38;5;34m4,032\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_34                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │             \u001b[38;5;34m128\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_9 (\u001b[38;5;33mReLU\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m23\u001b[0m, \u001b[38;5;34m12\u001b[0m, \u001b[38;5;34m17\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_35                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m32\u001b[0m)  │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_36                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │         \u001b[38;5;34m131,136\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_37                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │             \u001b[38;5;34m256\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_10 (\u001b[38;5;33mReLU\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m22\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m16\u001b[0m, \u001b[38;5;34m64\u001b[0m)  │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_38                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m11\u001b[0m, \u001b[38;5;34m5\u001b[0m, \u001b[38;5;34m8\u001b[0m, \u001b[38;5;34m64\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_39                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │         \u001b[38;5;34m221,312\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_40                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │             \u001b[38;5;34m512\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_11 (\u001b[38;5;33mReLU\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m6\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m4\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_41                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1\u001b[0m, \u001b[38;5;34m2\u001b[0m, \u001b[38;5;34m128\u001b[0m)    │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_42                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m768\u001b[0m)             │               \u001b[38;5;34m0\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_43                  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m32\u001b[0m)              │          \u001b[38;5;34m24,608\u001b[0m │\n",
              "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)                  │          \u001b[38;5;34m24,832\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m650\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)   │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_33                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │           <span style=\"color: #00af00; text-decoration-color: #00af00\">4,032</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_34                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">23</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">17</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_35                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_36                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │         <span style=\"color: #00af00; text-decoration-color: #00af00\">131,136</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_37                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">22</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">16</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)  │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_38                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">8</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_39                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │         <span style=\"color: #00af00; text-decoration-color: #00af00\">221,312</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_40                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │             <span style=\"color: #00af00; text-decoration-color: #00af00\">512</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ re_lu_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">ReLU</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">6</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">4</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_41                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)    │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_42                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">768</span>)             │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ time_distributed_43                  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,608</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)                    │                             │                 │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">24,832</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">650</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m407,466\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,466</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m407,018\u001b[0m (1.55 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">407,018</span> (1.55 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m448\u001b[0m (1.75 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">448</span> (1.75 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.metrics import CategoricalAccuracy\n",
        "# Example input shape: 10 frames (time_steps), each frame with size 64x64, and 3 channels\n",
        "# input_shape = (10,96,96,68)\n",
        "\n",
        "num_classes = 10  # Example number of classes for classification\n",
        "\n",
        "model = create_4d_cnn_with_spacetime_kernel(input_shape, num_classes)\n",
        "optimizer = tf.optimizers.Adam(learning_rate=1e-4)\n",
        "loss_object = tf.losses.CategoricalCrossentropy(from_logits=False)\n",
        "val_acc_metric = CategoricalAccuracy()"
      ],
      "metadata": {
        "id": "aSGp_D81W93O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def train_step(inputs, targets):\n",
        "    with tf.GradientTape() as tape:\n",
        "\n",
        "        predictions = model(inputs, training=True)\n",
        "        #print(predictions)\n",
        "        #print(targets)\n",
        "        loss = loss_object(targets, predictions)\n",
        "    gradients = tape.gradient(loss, model.trainable_variables)\n",
        "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
        "    return loss"
      ],
      "metadata": {
        "id": "NpTLeU9WW93O"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@tf.function\n",
        "def perform_validation(model, x, y):\n",
        "    val_logits = model(x, training=False)\n",
        "    return val_logits"
      ],
      "metadata": {
        "id": "TABG3bwMW93P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 4\n",
        "steps_per_epoch = len(train_idx) // batch_size\n",
        "val_best = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    loss_tot = 0\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (inputs, targets) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        loss = train_step(inputs, targets)\n",
        "        loss_tot += loss.numpy()\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss_tot/(step+1)}\")\n",
        "\n",
        "    # End of epoch actions...\n",
        "    # Perform validation\n",
        "    for idx in val_idx:\n",
        "        x_batch_val = np.load(\"Brain_\"+str(idx)+\".npy\")\n",
        "        x_batch_val = x_batch_val.transpose(3,0,1,2)\n",
        "        x_batch_val = x_batch_val.reshape(1,*x_batch_val.shape)\n",
        "        # x_batch_val = x_batch_val.reshape(1,96,96,68,10)\n",
        "        #x_batch_val = x_batch_val[4:,:,:,:].reshape(1,6,96,96,68)\n",
        "        x_batch_val = tf.convert_to_tensor(x_batch_val)\n",
        "        x_batch_val = tf.expand_dims(x_batch_val, axis=-1)\n",
        "        y_batch_val = np.load(\"Brain_\"+str(idx)+\"y.npy\")\n",
        "        y_batch_val = int(y_batch_val)\n",
        "        one_hot = np.zeros(10)\n",
        "        one_hot[y_batch_val] = 1\n",
        "        y_batch_val = tf.convert_to_tensor(one_hot)\n",
        "        val_logits = perform_validation(model, x_batch_val, y_batch_val)\n",
        "        # Update validation metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    if(val_acc > val_best):\n",
        "        val_best = val_acc\n",
        "        model.save(\"model_alt.h5\")\n",
        "    val_acc_metric.reset_state()\n",
        "    print(\"Training Loss: %.4f\" % (float(loss_tot)/len(train_idx),))\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "796ef986-9ef9-4c04-e58c-6e001b4d9eac",
        "id": "8WZl4KyaW93P"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Step 0, Loss: 1.7971081733703613\n",
            "Step 100, Loss: 2.359218780357059\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5891\n",
            "Validation acc: 0.0721\n",
            "Epoch 2/100\n",
            "Step 0, Loss: 2.3856453895568848\n",
            "Step 100, Loss: 2.3152021724398772\n",
            "Training Loss: 0.5804\n",
            "Validation acc: 0.0631\n",
            "Epoch 3/100\n",
            "Step 0, Loss: 2.3253533840179443\n",
            "Step 100, Loss: 2.3344125936527065\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5817\n",
            "Validation acc: 0.0811\n",
            "Epoch 4/100\n",
            "Step 0, Loss: 2.3624629974365234\n",
            "Step 100, Loss: 2.3171272419466833\n",
            "Training Loss: 0.5794\n",
            "Validation acc: 0.0811\n",
            "Epoch 5/100\n",
            "Step 0, Loss: 2.5155889987945557\n",
            "Step 100, Loss: 2.3200543729385528\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5801\n",
            "Validation acc: 0.0991\n",
            "Epoch 6/100\n",
            "Step 0, Loss: 2.4121034145355225\n",
            "Step 100, Loss: 2.3163319106149203\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5805\n",
            "Validation acc: 0.1081\n",
            "Epoch 7/100\n",
            "Step 0, Loss: 2.4181270599365234\n",
            "Step 100, Loss: 2.3138176072942147\n",
            "Training Loss: 0.5779\n",
            "Validation acc: 0.0631\n",
            "Epoch 8/100\n",
            "Step 0, Loss: 2.4144182205200195\n",
            "Step 100, Loss: 2.3073850504242546\n",
            "Training Loss: 0.5766\n",
            "Validation acc: 0.0721\n",
            "Epoch 9/100\n",
            "Step 0, Loss: 2.2260913848876953\n",
            "Step 100, Loss: 2.3107853384301213\n",
            "Training Loss: 0.5774\n",
            "Validation acc: 0.0811\n",
            "Epoch 10/100\n",
            "Step 0, Loss: 2.2878470420837402\n",
            "Step 100, Loss: 2.3062708330626536\n",
            "Training Loss: 0.5768\n",
            "Validation acc: 0.1081\n",
            "Epoch 11/100\n",
            "Step 0, Loss: 2.3249197006225586\n",
            "Step 100, Loss: 2.3080386525333516\n",
            "Training Loss: 0.5769\n",
            "Validation acc: 0.0631\n",
            "Epoch 12/100\n",
            "Step 0, Loss: 2.2791013717651367\n",
            "Step 100, Loss: 2.3050289555351333\n",
            "Training Loss: 0.5763\n",
            "Validation acc: 0.0811\n",
            "Epoch 13/100\n",
            "Step 0, Loss: 2.3258285522460938\n",
            "Step 100, Loss: 2.306808497646067\n",
            "Training Loss: 0.5767\n",
            "Validation acc: 0.0901\n",
            "Epoch 14/100\n",
            "Step 0, Loss: 2.2753593921661377\n",
            "Step 100, Loss: 2.308612637000509\n",
            "Training Loss: 0.5764\n",
            "Validation acc: 0.0541\n",
            "Epoch 15/100\n",
            "Step 0, Loss: 2.2088778018951416\n",
            "Step 100, Loss: 2.3054351051255026\n",
            "Training Loss: 0.5762\n",
            "Validation acc: 0.0541\n",
            "Epoch 16/100\n",
            "Step 0, Loss: 2.24029278755188\n",
            "Step 100, Loss: 2.3003769747101432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5754\n",
            "Validation acc: 0.1171\n",
            "Epoch 17/100\n",
            "Step 0, Loss: 2.373727560043335\n",
            "Step 100, Loss: 2.308694811150579\n",
            "Training Loss: 0.5766\n",
            "Validation acc: 0.0721\n",
            "Epoch 18/100\n",
            "Step 0, Loss: 2.3004884719848633\n",
            "Step 100, Loss: 2.3051170807073613\n",
            "Training Loss: 0.5763\n",
            "Validation acc: 0.0721\n",
            "Epoch 19/100\n",
            "Step 0, Loss: 2.360264778137207\n",
            "Step 100, Loss: 2.2997540270928107\n",
            "Training Loss: 0.5761\n",
            "Validation acc: 0.1081\n",
            "Epoch 20/100\n",
            "Step 0, Loss: 2.2573812007904053\n",
            "Step 100, Loss: 2.302854960507686\n",
            "Training Loss: 0.5760\n",
            "Validation acc: 0.0721\n",
            "Epoch 21/100\n",
            "Step 0, Loss: 2.3428986072540283\n",
            "Step 100, Loss: 2.302546201366009\n",
            "Training Loss: 0.5752\n",
            "Validation acc: 0.1081\n",
            "Epoch 22/100\n",
            "Step 0, Loss: 2.2233633995056152\n",
            "Step 100, Loss: 2.306974758016001\n",
            "Training Loss: 0.5769\n",
            "Validation acc: 0.0721\n",
            "Epoch 23/100\n",
            "Step 0, Loss: 2.2860097885131836\n",
            "Step 100, Loss: 2.3019448601373353\n",
            "Training Loss: 0.5759\n",
            "Validation acc: 0.0991\n",
            "Epoch 24/100\n",
            "Step 0, Loss: 2.396965742111206\n",
            "Step 100, Loss: 2.306045976015601\n",
            "Training Loss: 0.5760\n",
            "Validation acc: 0.0991\n",
            "Epoch 25/100\n",
            "Step 0, Loss: 2.3375132083892822\n",
            "Step 100, Loss: 2.304836912910537\n",
            "Training Loss: 0.5750\n",
            "Validation acc: 0.0721\n",
            "Epoch 26/100\n",
            "Step 0, Loss: 2.190364360809326\n",
            "Step 100, Loss: 2.333901901056271\n",
            "Training Loss: 0.5824\n",
            "Validation acc: 0.0991\n",
            "Epoch 27/100\n",
            "Step 0, Loss: 2.276573419570923\n",
            "Step 100, Loss: 2.315541925996837\n",
            "Training Loss: 0.5787\n",
            "Validation acc: 0.1081\n",
            "Epoch 28/100\n",
            "Step 0, Loss: 2.3960914611816406\n",
            "Step 100, Loss: 2.306801734584393\n",
            "Training Loss: 0.5780\n",
            "Validation acc: 0.0631\n",
            "Epoch 29/100\n",
            "Step 0, Loss: 2.244586706161499\n",
            "Step 100, Loss: 2.3153822587268187\n",
            "Training Loss: 0.5773\n",
            "Validation acc: 0.1081\n",
            "Epoch 30/100\n",
            "Step 0, Loss: 2.272977113723755\n",
            "Step 100, Loss: 2.30304214977982\n",
            "Training Loss: 0.5767\n",
            "Validation acc: 0.0901\n",
            "Epoch 31/100\n",
            "Step 0, Loss: 2.2971596717834473\n",
            "Step 100, Loss: 2.306309409660868\n",
            "Training Loss: 0.5771\n",
            "Validation acc: 0.1081\n",
            "Epoch 32/100\n",
            "Step 0, Loss: 2.331859588623047\n",
            "Step 100, Loss: 2.3038159455403244\n",
            "Training Loss: 0.5764\n",
            "Validation acc: 0.0631\n",
            "Epoch 33/100\n",
            "Step 0, Loss: 2.34090518951416\n",
            "Step 100, Loss: 2.301499118899355\n",
            "Training Loss: 0.5761\n",
            "Validation acc: 0.0991\n",
            "Epoch 34/100\n",
            "Step 0, Loss: 2.3266379833221436\n",
            "Step 100, Loss: 2.307064169704324\n",
            "Training Loss: 0.5764\n",
            "Validation acc: 0.0991\n",
            "Epoch 35/100\n",
            "Step 0, Loss: 2.255671501159668\n",
            "Step 100, Loss: 2.2985478155683765\n",
            "Training Loss: 0.5754\n",
            "Validation acc: 0.0721\n",
            "Epoch 36/100\n",
            "Step 0, Loss: 2.3497447967529297\n",
            "Step 100, Loss: 2.2998136293770064\n",
            "Training Loss: 0.5761\n",
            "Validation acc: 0.0721\n",
            "Epoch 37/100\n",
            "Step 0, Loss: 2.304543972015381\n",
            "Step 100, Loss: 2.304555777275916\n",
            "Training Loss: 0.5765\n",
            "Validation acc: 0.0811\n",
            "Epoch 38/100\n",
            "Step 0, Loss: 2.307046890258789\n",
            "Step 100, Loss: 2.300197974289998\n",
            "Training Loss: 0.5755\n",
            "Validation acc: 0.0901\n",
            "Epoch 39/100\n",
            "Step 0, Loss: 2.361882448196411\n",
            "Step 100, Loss: 2.306023139764767\n",
            "Training Loss: 0.5763\n",
            "Validation acc: 0.0811\n",
            "Epoch 40/100\n",
            "Step 0, Loss: 2.3856019973754883\n",
            "Step 100, Loss: 2.3090131920162995\n",
            "Training Loss: 0.5768\n",
            "Validation acc: 0.1081\n",
            "Epoch 41/100\n",
            "Step 0, Loss: 2.229567289352417\n",
            "Step 100, Loss: 2.3018019860333734\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0541\n",
            "Epoch 42/100\n",
            "Step 0, Loss: 2.2679035663604736\n",
            "Step 100, Loss: 2.3001030506473956\n",
            "Training Loss: 0.5755\n",
            "Validation acc: 0.0991\n",
            "Epoch 43/100\n",
            "Step 0, Loss: 2.2610299587249756\n",
            "Step 100, Loss: 2.2980342973576913\n",
            "Training Loss: 0.5751\n",
            "Validation acc: 0.1081\n",
            "Epoch 44/100\n",
            "Step 0, Loss: 2.298579692840576\n",
            "Step 100, Loss: 2.300887206993481\n",
            "Training Loss: 0.5751\n",
            "Validation acc: 0.0811\n",
            "Epoch 45/100\n",
            "Step 0, Loss: 2.291822910308838\n",
            "Step 100, Loss: 2.299345033003552\n",
            "Training Loss: 0.5747\n",
            "Validation acc: 0.0991\n",
            "Epoch 46/100\n",
            "Step 0, Loss: 2.288527011871338\n",
            "Step 100, Loss: 2.2939154818506524\n",
            "Training Loss: 0.5744\n",
            "Validation acc: 0.0811\n",
            "Epoch 47/100\n",
            "Step 0, Loss: 2.2014310359954834\n",
            "Step 100, Loss: 2.2960661968382277\n",
            "Training Loss: 0.5736\n",
            "Validation acc: 0.1081\n",
            "Epoch 48/100\n",
            "Step 0, Loss: 2.2724409103393555\n",
            "Step 100, Loss: 2.3417095028527894\n",
            "Training Loss: 0.5839\n",
            "Validation acc: 0.0631\n",
            "Epoch 49/100\n",
            "Step 0, Loss: 2.3387279510498047\n",
            "Step 100, Loss: 2.318386939492556\n",
            "Training Loss: 0.5793\n",
            "Validation acc: 0.0991\n",
            "Epoch 50/100\n",
            "Step 0, Loss: 2.2620790004730225\n",
            "Step 100, Loss: 2.306629686072321\n",
            "Training Loss: 0.5767\n",
            "Validation acc: 0.0811\n",
            "Epoch 51/100\n",
            "Step 0, Loss: 2.2172234058380127\n",
            "Step 100, Loss: 2.304155134918666\n",
            "Training Loss: 0.5763\n",
            "Validation acc: 0.1081\n",
            "Epoch 52/100\n",
            "Step 0, Loss: 2.36822509765625\n",
            "Step 100, Loss: 2.3065317361661704\n",
            "Training Loss: 0.5764\n",
            "Validation acc: 0.0631\n",
            "Epoch 53/100\n",
            "Step 0, Loss: 2.2613534927368164\n",
            "Step 100, Loss: 2.3039875738691578\n",
            "Training Loss: 0.5752\n",
            "Validation acc: 0.0811\n",
            "Epoch 54/100\n",
            "Step 0, Loss: 2.2687458992004395\n",
            "Step 100, Loss: 2.2989213395826886\n",
            "Training Loss: 0.5744\n",
            "Validation acc: 0.0991\n",
            "Epoch 55/100\n",
            "Step 0, Loss: 2.548734664916992\n",
            "Step 100, Loss: 2.3009926729863235\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5f323f663788>\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;31m# Iterate over the batches of the dataset.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m         \u001b[0mloss_tot\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m100\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    831\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    832\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 833\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    834\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    835\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/polymorphic_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    867\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    868\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 869\u001b[0;31m       return tracing_compilation.call_function(\n\u001b[0m\u001b[1;32m    870\u001b[0m           \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_no_variable_creation_config\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    871\u001b[0m       )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/tracing_compilation.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[1;32m    137\u001b[0m   \u001b[0mbound_args\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbind\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m   \u001b[0mflat_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munpack_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbound_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 139\u001b[0;31m   return function._call_flat(  # pylint: disable=protected-access\n\u001b[0m\u001b[1;32m    140\u001b[0m       \u001b[0mflat_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   )\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/concrete_function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[1;32m   1320\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1321\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1322\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inference_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1323\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1324\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_preflattened\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    214\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mcall_preflattened\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mSequence\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;34m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m     \u001b[0mflat_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpack_output\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mflat_outputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/polymorphic_function/atomic_function.py\u001b[0m in \u001b[0;36mcall_flat\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m    249\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mrecord\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstop_recording\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    250\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_bound_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 251\u001b[0;31m             outputs = self._bound_context.call_function(\n\u001b[0m\u001b[1;32m    252\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/context.py\u001b[0m in \u001b[0;36mcall_function\u001b[0;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[1;32m   1550\u001b[0m     \u001b[0mcancellation_context\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcancellation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcancellation_context\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1552\u001b[0;31m       outputs = execute.execute(\n\u001b[0m\u001b[1;32m   1553\u001b[0m           \u001b[0mname\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"utf-8\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1554\u001b[0m           \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnum_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     51\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     54\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     55\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: read in L_Heschl_gyrus_AAL_83.npy\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming the file is in the current working directory\n",
        "brain_data_RH = np.load(\"R_Heschl_gyrus_AAL_84.npy\")\n",
        "\n",
        "# Now you can work with the loaded data, e.g., print its shape\n",
        "print(brain_data_RH.shape)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uoqUHKQNYszx",
        "outputId": "7d52f611-fa2d-442d-d5be-f4f223818efa"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 611)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: for each Brain_*.npy file zero out the voxels that does not belong in brain_data\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "# Assuming brain_data is a boolean array indicating brain voxels\n",
        "brain_mask = np.load(\"L_Heschl_gyrus_AAL_83.npy\")\n",
        "# brain_mask = np.load(\"/content/drive/MyDrive/aime/CNN/L_Heschl_gyrus_AAL_83.npy\")\n",
        "\n",
        "\n",
        "for j in range(num_samples):\n",
        "  file_path = \"Brain_\"+str(j)+\".npy\"\n",
        "  brain_data = np.load(file_path)\n",
        "  new_brain_data = np.zeros(brain_data.shape)\n",
        "  for i in range(brain_data_RH.shape[1]):\n",
        "    new_brain_data[brain_data_RH[0,i]-60,brain_data_RH[1,i]-67,brain_data_RH[2,i]-23,:] = brain_data[brain_data_RH[0,i]-60,brain_data_RH[1,i]-67,brain_data_RH[2,i]-23,:]\n",
        "  # Save the modified data back to the file\n",
        "  np.save(file_path, new_brain_data)\n"
      ],
      "metadata": {
        "id": "kP_i_DScc0Xy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "batch_size = 4\n",
        "steps_per_epoch = len(train_idx) // batch_size\n",
        "val_best = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    print(f\"Epoch {epoch+1}/{epochs}\")\n",
        "    loss_tot = 0\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (inputs, targets) in enumerate(dataset.take(steps_per_epoch)):\n",
        "        loss = train_step(inputs, targets)\n",
        "        loss_tot += loss.numpy()\n",
        "        if step % 100 == 0:\n",
        "            print(f\"Step {step}, Loss: {loss_tot/(step+1)}\")\n",
        "\n",
        "    # End of epoch actions...\n",
        "    # Perform validation\n",
        "    for idx in val_idx:\n",
        "        x_batch_val = np.load(\"Brain_\"+str(idx)+\".npy\")\n",
        "        x_batch_val = x_batch_val.transpose(3,0,1,2)\n",
        "        x_batch_val = x_batch_val.reshape(1,*x_batch_val.shape)\n",
        "        # x_batch_val = x_batch_val.reshape(1,96,96,68,10)\n",
        "        #x_batch_val = x_batch_val[4:,:,:,:].reshape(1,6,96,96,68)\n",
        "        x_batch_val = tf.convert_to_tensor(x_batch_val)\n",
        "        x_batch_val = tf.expand_dims(x_batch_val, axis=-1)\n",
        "        y_batch_val = np.load(\"Brain_\"+str(idx)+\"y.npy\")\n",
        "        y_batch_val = int(y_batch_val)\n",
        "        one_hot = np.zeros(10)\n",
        "        one_hot[y_batch_val] = 1\n",
        "        y_batch_val = tf.convert_to_tensor(one_hot)\n",
        "        val_logits = perform_validation(model, x_batch_val, y_batch_val)\n",
        "        # Update validation metrics\n",
        "        val_acc_metric.update_state(y_batch_val, val_logits)\n",
        "    val_acc = val_acc_metric.result()\n",
        "    if(val_acc > val_best):\n",
        "        val_best = val_acc\n",
        "        model.save(\"model_alt.h5\")\n",
        "    val_acc_metric.reset_state()\n",
        "    print(\"Training Loss: %.4f\" % (float(loss_tot)/len(train_idx),))\n",
        "    print(\"Validation acc: %.4f\" % (float(val_acc),))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HKeqHTO2c9Tu",
        "outputId": "972fb885-be1d-4cd7-bc7b-f488227b56ec"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "Step 0, Loss: 2.325343608856201\n",
            "Step 100, Loss: 2.3137859353924743\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5783\n",
            "Validation acc: 0.1081\n",
            "Epoch 2/100\n",
            "Step 0, Loss: 2.3379697799682617\n",
            "Step 100, Loss: 2.3053948407125944\n",
            "Training Loss: 0.5770\n",
            "Validation acc: 0.0811\n",
            "Epoch 3/100\n",
            "Step 0, Loss: 2.3234245777130127\n",
            "Step 100, Loss: 2.3057427807609634\n",
            "Training Loss: 0.5766\n",
            "Validation acc: 0.0901\n",
            "Epoch 4/100\n",
            "Step 0, Loss: 2.311617374420166\n",
            "Step 100, Loss: 2.3032739492926266\n",
            "Training Loss: 0.5763\n",
            "Validation acc: 0.0901\n",
            "Epoch 5/100\n",
            "Step 0, Loss: 2.2883594036102295\n",
            "Step 100, Loss: 2.3017203241291613\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0811\n",
            "Epoch 6/100\n",
            "Step 0, Loss: 2.288486957550049\n",
            "Step 100, Loss: 2.3011993252404848\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0901\n",
            "Epoch 7/100\n",
            "Step 0, Loss: 2.3000266551971436\n",
            "Step 100, Loss: 2.303874882140962\n",
            "Training Loss: 0.5760\n",
            "Validation acc: 0.0631\n",
            "Epoch 8/100\n",
            "Step 0, Loss: 2.2854132652282715\n",
            "Step 100, Loss: 2.3036872254739893\n",
            "Training Loss: 0.5756\n",
            "Validation acc: 0.0901\n",
            "Epoch 9/100\n",
            "Step 0, Loss: 2.347957134246826\n",
            "Step 100, Loss: 2.3021180157614225\n",
            "Training Loss: 0.5756\n",
            "Validation acc: 0.0811\n",
            "Epoch 10/100\n",
            "Step 0, Loss: 2.313872814178467\n",
            "Step 100, Loss: 2.3003409026872994\n",
            "Training Loss: 0.5762\n",
            "Validation acc: 0.0811\n",
            "Epoch 11/100\n",
            "Step 0, Loss: 2.317103624343872\n",
            "Step 100, Loss: 2.3053210843907723\n",
            "Training Loss: 0.5759\n",
            "Validation acc: 0.0901\n",
            "Epoch 12/100\n",
            "Step 0, Loss: 2.2928757667541504\n",
            "Step 100, Loss: 2.3007003954141445\n",
            "Training Loss: 0.5754\n",
            "Validation acc: 0.0991\n",
            "Epoch 13/100\n",
            "Step 0, Loss: 2.3083605766296387\n",
            "Step 100, Loss: 2.3033724775408753\n",
            "Training Loss: 0.5755\n",
            "Validation acc: 0.0811\n",
            "Epoch 14/100\n",
            "Step 0, Loss: 2.2458324432373047\n",
            "Step 100, Loss: 2.3011316403303996\n",
            "Training Loss: 0.5756\n",
            "Validation acc: 0.0811\n",
            "Epoch 15/100\n",
            "Step 0, Loss: 2.28759503364563\n",
            "Step 100, Loss: 2.302667513932332\n",
            "Training Loss: 0.5757\n",
            "Validation acc: 0.0811\n",
            "Epoch 16/100\n",
            "Step 0, Loss: 2.262404441833496\n",
            "Step 100, Loss: 2.302104595864173\n",
            "Training Loss: 0.5754\n",
            "Validation acc: 0.0901\n",
            "Epoch 17/100\n",
            "Step 0, Loss: 2.237515449523926\n",
            "Step 100, Loss: 2.303237513740464\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0901\n",
            "Epoch 18/100\n",
            "Step 0, Loss: 2.321850299835205\n",
            "Step 100, Loss: 2.30058491820156\n",
            "Training Loss: 0.5752\n",
            "Validation acc: 0.0901\n",
            "Epoch 19/100\n",
            "Step 0, Loss: 2.304701805114746\n",
            "Step 100, Loss: 2.299557610313491\n",
            "Training Loss: 0.5752\n",
            "Validation acc: 0.0811\n",
            "Epoch 20/100\n",
            "Step 0, Loss: 2.2783761024475098\n",
            "Step 100, Loss: 2.297766380971021\n",
            "Training Loss: 0.5753\n",
            "Validation acc: 0.1081\n",
            "Epoch 21/100\n",
            "Step 0, Loss: 2.250133514404297\n",
            "Step 100, Loss: 2.297909070949743\n",
            "Training Loss: 0.5751\n",
            "Validation acc: 0.0541\n",
            "Epoch 22/100\n",
            "Step 0, Loss: 2.3629064559936523\n",
            "Step 100, Loss: 2.2936055306160803\n",
            "Training Loss: 0.5748\n",
            "Validation acc: 0.0811\n",
            "Epoch 23/100\n",
            "Step 0, Loss: 2.2309165000915527\n",
            "Step 100, Loss: 2.301190444738558\n",
            "Training Loss: 0.5757\n",
            "Validation acc: 0.0721\n",
            "Epoch 24/100\n",
            "Step 0, Loss: 2.2959144115448\n",
            "Step 100, Loss: 2.299917754560414\n",
            "Training Loss: 0.5751\n",
            "Validation acc: 0.0721\n",
            "Epoch 25/100\n",
            "Step 0, Loss: 2.373500347137451\n",
            "Step 100, Loss: 2.300992285851205\n",
            "Training Loss: 0.5747\n",
            "Validation acc: 0.0811\n",
            "Epoch 26/100\n",
            "Step 0, Loss: 2.321481943130493\n",
            "Step 100, Loss: 2.3391008967220195\n",
            "Training Loss: 0.5825\n",
            "Validation acc: 0.0901\n",
            "Epoch 27/100\n",
            "Step 0, Loss: 2.39088773727417\n",
            "Step 100, Loss: 2.3110669060508804\n",
            "Training Loss: 0.5773\n",
            "Validation acc: 0.0901\n",
            "Epoch 28/100\n",
            "Step 0, Loss: 2.2711422443389893\n",
            "Step 100, Loss: 2.3046432556492267\n",
            "Training Loss: 0.5766\n",
            "Validation acc: 0.0721\n",
            "Epoch 29/100\n",
            "Step 0, Loss: 2.2934513092041016\n",
            "Step 100, Loss: 2.301676301672907\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5760\n",
            "Validation acc: 0.1261\n",
            "Epoch 30/100\n",
            "Step 0, Loss: 2.297624349594116\n",
            "Step 100, Loss: 2.3062448171105716\n",
            "Training Loss: 0.5764\n",
            "Validation acc: 0.0721\n",
            "Epoch 31/100\n",
            "Step 0, Loss: 2.2099359035491943\n",
            "Step 100, Loss: 2.29549237997225\n",
            "Training Loss: 0.5754\n",
            "Validation acc: 0.0901\n",
            "Epoch 32/100\n",
            "Step 0, Loss: 2.3396964073181152\n",
            "Step 100, Loss: 2.299308181989311\n",
            "Training Loss: 0.5758\n",
            "Validation acc: 0.0721\n",
            "Epoch 33/100\n",
            "Step 0, Loss: 2.250347375869751\n",
            "Step 100, Loss: 2.2918919525524175\n",
            "Training Loss: 0.5727\n",
            "Validation acc: 0.0811\n",
            "Epoch 34/100\n",
            "Step 0, Loss: 2.3485493659973145\n",
            "Step 100, Loss: 2.2930456246479904\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5729\n",
            "Validation acc: 0.1441\n",
            "Epoch 35/100\n",
            "Step 0, Loss: 2.384152412414551\n",
            "Step 100, Loss: 2.295515263434684\n",
            "Training Loss: 0.5746\n",
            "Validation acc: 0.1261\n",
            "Epoch 36/100\n",
            "Step 0, Loss: 2.25662899017334\n",
            "Step 100, Loss: 2.2942593569802767\n",
            "Training Loss: 0.5719\n",
            "Validation acc: 0.0901\n",
            "Epoch 37/100\n",
            "Step 0, Loss: 2.3034162521362305\n",
            "Step 100, Loss: 2.2704450588415166\n",
            "Training Loss: 0.5701\n",
            "Validation acc: 0.0811\n",
            "Epoch 38/100\n",
            "Step 0, Loss: 2.2007243633270264\n",
            "Step 100, Loss: 2.278236103529977\n",
            "Training Loss: 0.5691\n",
            "Validation acc: 0.1171\n",
            "Epoch 39/100\n",
            "Step 0, Loss: 2.204221725463867\n",
            "Step 100, Loss: 2.2794576682666743\n",
            "Training Loss: 0.5702\n",
            "Validation acc: 0.0991\n",
            "Epoch 40/100\n",
            "Step 0, Loss: 2.2328436374664307\n",
            "Step 100, Loss: 2.250506574564641\n",
            "Training Loss: 0.5633\n",
            "Validation acc: 0.0991\n",
            "Epoch 41/100\n",
            "Step 0, Loss: 2.3126468658447266\n",
            "Step 100, Loss: 2.248175105245987\n",
            "Training Loss: 0.5635\n",
            "Validation acc: 0.1441\n",
            "Epoch 42/100\n",
            "Step 0, Loss: 1.9069342613220215\n",
            "Step 100, Loss: 2.255868471494996\n",
            "Training Loss: 0.5630\n",
            "Validation acc: 0.0631\n",
            "Epoch 43/100\n",
            "Step 0, Loss: 2.3260650634765625\n",
            "Step 100, Loss: 2.238542480043846\n",
            "Training Loss: 0.5561\n",
            "Validation acc: 0.1081\n",
            "Epoch 44/100\n",
            "Step 0, Loss: 2.073946714401245\n",
            "Step 100, Loss: 2.2062299818095594\n",
            "Training Loss: 0.5554\n",
            "Validation acc: 0.0901\n",
            "Epoch 45/100\n",
            "Step 0, Loss: 2.2243034839630127\n",
            "Step 100, Loss: 2.233189327882068\n",
            "Training Loss: 0.5546\n",
            "Validation acc: 0.1081\n",
            "Epoch 46/100\n",
            "Step 0, Loss: 1.949629783630371\n",
            "Step 100, Loss: 2.2102847925507194\n",
            "Training Loss: 0.5550\n",
            "Validation acc: 0.0901\n",
            "Epoch 47/100\n",
            "Step 0, Loss: 2.248901128768921\n",
            "Step 100, Loss: 2.203033251337486\n",
            "Training Loss: 0.5505\n",
            "Validation acc: 0.0901\n",
            "Epoch 48/100\n",
            "Step 0, Loss: 2.1164138317108154\n",
            "Step 100, Loss: 2.197011160378409\n",
            "Training Loss: 0.5514\n",
            "Validation acc: 0.1261\n",
            "Epoch 49/100\n",
            "Step 0, Loss: 1.966874361038208\n",
            "Step 100, Loss: 2.1960323258201675\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5485\n",
            "Validation acc: 0.1712\n",
            "Epoch 50/100\n",
            "Step 0, Loss: 2.2161927223205566\n",
            "Step 100, Loss: 2.1827642846815656\n",
            "Training Loss: 0.5439\n",
            "Validation acc: 0.0991\n",
            "Epoch 51/100\n",
            "Step 0, Loss: 2.355868101119995\n",
            "Step 100, Loss: 2.156328030151896\n",
            "Training Loss: 0.5391\n",
            "Validation acc: 0.1171\n",
            "Epoch 52/100\n",
            "Step 0, Loss: 2.023317575454712\n",
            "Step 100, Loss: 2.1339491806407964\n",
            "Training Loss: 0.5366\n",
            "Validation acc: 0.1261\n",
            "Epoch 53/100\n",
            "Step 0, Loss: 2.00838303565979\n",
            "Step 100, Loss: 2.125826631442155\n",
            "Training Loss: 0.5351\n",
            "Validation acc: 0.1261\n",
            "Epoch 54/100\n",
            "Step 0, Loss: 2.126699924468994\n",
            "Step 100, Loss: 2.110076603322926\n",
            "Training Loss: 0.5297\n",
            "Validation acc: 0.1171\n",
            "Epoch 55/100\n",
            "Step 0, Loss: 2.445472002029419\n",
            "Step 100, Loss: 2.1372838693090004\n",
            "Training Loss: 0.5356\n",
            "Validation acc: 0.1081\n",
            "Epoch 56/100\n",
            "Step 0, Loss: 2.65710711479187\n",
            "Step 100, Loss: 2.1238521384720754\n",
            "Training Loss: 0.5362\n",
            "Validation acc: 0.1622\n",
            "Epoch 57/100\n",
            "Step 0, Loss: 1.9103951454162598\n",
            "Step 100, Loss: 2.101788699036778\n",
            "Training Loss: 0.5242\n",
            "Validation acc: 0.1261\n",
            "Epoch 58/100\n",
            "Step 0, Loss: 2.1134276390075684\n",
            "Step 100, Loss: 2.1233207754569476\n",
            "Training Loss: 0.5266\n",
            "Validation acc: 0.1441\n",
            "Epoch 59/100\n",
            "Step 0, Loss: 1.502720594406128\n",
            "Step 100, Loss: 2.0877017821415818\n",
            "Training Loss: 0.5198\n",
            "Validation acc: 0.0991\n",
            "Epoch 60/100\n",
            "Step 0, Loss: 2.473055124282837\n",
            "Step 100, Loss: 2.079485299563644\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.5194\n",
            "Validation acc: 0.2072\n",
            "Epoch 61/100\n",
            "Step 0, Loss: 2.1290831565856934\n",
            "Step 100, Loss: 2.0764232956536928\n",
            "Training Loss: 0.5212\n",
            "Validation acc: 0.1982\n",
            "Epoch 62/100\n",
            "Step 0, Loss: 2.0420596599578857\n",
            "Step 100, Loss: 2.0382293344724296\n",
            "Training Loss: 0.5132\n",
            "Validation acc: 0.1622\n",
            "Epoch 63/100\n",
            "Step 0, Loss: 2.166501998901367\n",
            "Step 100, Loss: 2.0541510888845615\n",
            "Training Loss: 0.5152\n",
            "Validation acc: 0.1351\n",
            "Epoch 64/100\n",
            "Step 0, Loss: 1.4346373081207275\n",
            "Step 100, Loss: 1.987649928225149\n",
            "Training Loss: 0.5016\n",
            "Validation acc: 0.1622\n",
            "Epoch 65/100\n",
            "Step 0, Loss: 1.7372350692749023\n",
            "Step 100, Loss: 2.0042912570556792\n",
            "Training Loss: 0.4985\n",
            "Validation acc: 0.1892\n",
            "Epoch 66/100\n",
            "Step 0, Loss: 1.9920457601547241\n",
            "Step 100, Loss: 1.9639292506888362\n",
            "Training Loss: 0.4950\n",
            "Validation acc: 0.1802\n",
            "Epoch 67/100\n",
            "Step 0, Loss: 1.7848058938980103\n",
            "Step 100, Loss: 2.022849644764815\n",
            "Training Loss: 0.4995\n",
            "Validation acc: 0.1892\n",
            "Epoch 68/100\n",
            "Step 0, Loss: 2.0676355361938477\n",
            "Step 100, Loss: 1.9600473288262243\n",
            "Training Loss: 0.4921\n",
            "Validation acc: 0.1802\n",
            "Epoch 69/100\n",
            "Step 0, Loss: 2.191082000732422\n",
            "Step 100, Loss: 1.9777292898385832\n",
            "Training Loss: 0.4959\n",
            "Validation acc: 0.1982\n",
            "Epoch 70/100\n",
            "Step 0, Loss: 2.1025803089141846\n",
            "Step 100, Loss: 1.9354455754308417\n",
            "Training Loss: 0.4799\n",
            "Validation acc: 0.1892\n",
            "Epoch 71/100\n",
            "Step 0, Loss: 2.0001678466796875\n",
            "Step 100, Loss: 1.8791796984058795\n",
            "Training Loss: 0.4789\n",
            "Validation acc: 0.1081\n",
            "Epoch 72/100\n",
            "Step 0, Loss: 1.6507446765899658\n",
            "Step 100, Loss: 1.9382344401708924\n",
            "Training Loss: 0.4805\n",
            "Validation acc: 0.2072\n",
            "Epoch 73/100\n",
            "Step 0, Loss: 1.7194299697875977\n",
            "Step 100, Loss: 1.8888744151238168\n",
            "Training Loss: 0.4764\n",
            "Validation acc: 0.1351\n",
            "Epoch 74/100\n",
            "Step 0, Loss: 2.024305820465088\n",
            "Step 100, Loss: 1.9399318954732159\n",
            "Training Loss: 0.4805\n",
            "Validation acc: 0.1622\n",
            "Epoch 75/100\n",
            "Step 0, Loss: 1.2305305004119873\n",
            "Step 100, Loss: 1.9167077907241217\n",
            "Training Loss: 0.4765\n",
            "Validation acc: 0.1802\n",
            "Epoch 76/100\n",
            "Step 0, Loss: 1.7642626762390137\n",
            "Step 100, Loss: 1.8408600481429902\n",
            "Training Loss: 0.4641\n",
            "Validation acc: 0.1622\n",
            "Epoch 77/100\n",
            "Step 0, Loss: 2.564436435699463\n",
            "Step 100, Loss: 1.801701782953621\n",
            "Training Loss: 0.4556\n",
            "Validation acc: 0.1622\n",
            "Epoch 78/100\n",
            "Step 0, Loss: 1.770503282546997\n",
            "Step 100, Loss: 1.8038895920951767\n",
            "Training Loss: 0.4528\n",
            "Validation acc: 0.1802\n",
            "Epoch 79/100\n",
            "Step 0, Loss: 1.4508434534072876\n",
            "Step 100, Loss: 1.7671232270722341\n",
            "Training Loss: 0.4445\n",
            "Validation acc: 0.1441\n",
            "Epoch 80/100\n",
            "Step 0, Loss: 1.3582600355148315\n",
            "Step 100, Loss: 1.8053750035786393\n",
            "Training Loss: 0.4509\n",
            "Validation acc: 0.1982\n",
            "Epoch 81/100\n",
            "Step 0, Loss: 1.0242496728897095\n",
            "Step 100, Loss: 1.8325939644681346\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Loss: 0.4520\n",
            "Validation acc: 0.2252\n",
            "Epoch 82/100\n",
            "Step 0, Loss: 1.8859370946884155\n",
            "Step 100, Loss: 1.7879321563361894\n",
            "Training Loss: 0.4401\n",
            "Validation acc: 0.2072\n",
            "Epoch 83/100\n",
            "Step 0, Loss: 1.6651077270507812\n",
            "Step 100, Loss: 1.7119918766588267\n",
            "Training Loss: 0.4334\n",
            "Validation acc: 0.2072\n",
            "Epoch 84/100\n",
            "Step 0, Loss: 1.2072865962982178\n",
            "Step 100, Loss: 1.76102212100926\n",
            "Training Loss: 0.4445\n",
            "Validation acc: 0.1982\n",
            "Epoch 85/100\n",
            "Step 0, Loss: 2.128755807876587\n",
            "Step 100, Loss: 1.7249144044252906\n",
            "Training Loss: 0.4296\n",
            "Validation acc: 0.1441\n",
            "Epoch 86/100\n",
            "Step 0, Loss: 1.775880217552185\n",
            "Step 100, Loss: 1.7186187865710494\n",
            "Training Loss: 0.4316\n",
            "Validation acc: 0.1892\n",
            "Epoch 87/100\n",
            "Step 0, Loss: 1.057159662246704\n",
            "Step 100, Loss: 1.709537644197445\n",
            "Training Loss: 0.4197\n",
            "Validation acc: 0.1982\n",
            "Epoch 88/100\n",
            "Step 0, Loss: 1.56492018699646\n",
            "Step 100, Loss: 1.6308653047769377\n",
            "Training Loss: 0.4146\n",
            "Validation acc: 0.2072\n",
            "Epoch 89/100\n",
            "Step 0, Loss: 1.6814519166946411\n",
            "Step 100, Loss: 1.6294129732811804\n",
            "Training Loss: 0.4050\n",
            "Validation acc: 0.2072\n",
            "Epoch 90/100\n",
            "Step 0, Loss: 1.3727846145629883\n",
            "Step 100, Loss: 1.6784694814445948\n",
            "Training Loss: 0.4137\n",
            "Validation acc: 0.1802\n",
            "Epoch 91/100\n",
            "Step 0, Loss: 2.1586825847625732\n",
            "Step 100, Loss: 1.6282102601362927\n",
            "Training Loss: 0.4044\n",
            "Validation acc: 0.2162\n",
            "Epoch 92/100\n",
            "Step 0, Loss: 1.5344717502593994\n",
            "Step 100, Loss: 1.592728839652373\n",
            "Training Loss: 0.4031\n",
            "Validation acc: 0.1712\n",
            "Epoch 93/100\n",
            "Step 0, Loss: 1.11138117313385\n",
            "Step 100, Loss: 1.603959884974036\n",
            "Training Loss: 0.4040\n",
            "Validation acc: 0.1532\n",
            "Epoch 94/100\n",
            "Step 0, Loss: 1.8593051433563232\n",
            "Step 100, Loss: 1.5986288585285149\n",
            "Training Loss: 0.4002\n",
            "Validation acc: 0.1982\n",
            "Epoch 95/100\n",
            "Step 0, Loss: 0.7462859153747559\n",
            "Step 100, Loss: 1.581683064451312\n",
            "Training Loss: 0.3922\n",
            "Validation acc: 0.2072\n",
            "Epoch 96/100\n",
            "Step 0, Loss: 0.9459296464920044\n",
            "Step 100, Loss: 1.5777321696871578\n",
            "Training Loss: 0.3917\n",
            "Validation acc: 0.1351\n",
            "Epoch 97/100\n",
            "Step 0, Loss: 1.8005475997924805\n",
            "Step 100, Loss: 1.5624164791390447\n",
            "Training Loss: 0.3918\n",
            "Validation acc: 0.1441\n",
            "Epoch 98/100\n",
            "Step 0, Loss: 1.2305428981781006\n",
            "Step 100, Loss: 1.4803799508821847\n",
            "Training Loss: 0.3817\n",
            "Validation acc: 0.2072\n",
            "Epoch 99/100\n",
            "Step 0, Loss: 1.337937831878662\n",
            "Step 100, Loss: 1.537131665366711\n",
            "Training Loss: 0.3828\n",
            "Validation acc: 0.1982\n",
            "Epoch 100/100\n",
            "Step 0, Loss: 1.5628564357757568\n",
            "Step 100, Loss: 1.427151086896953\n",
            "Training Loss: 0.3647\n",
            "Validation acc: 0.1261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jEEp2jUDdJyn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Build a simple LSTM model that takes in (594,10) input and output a 10 class classification for 10 temporal steps\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import LSTM, Dense\n",
        "\n",
        "# Build the LSTM model\n",
        "model_r = Sequential()\n",
        "model_r.add(LSTM(units=64, return_sequences=True, input_shape=(10, 611)))  # Adjust units as needed\n",
        "model_r.add(LSTM(units=32, return_sequences=False))\n",
        "model_r.add(Dense(units=10, activation='softmax'))\n",
        "\n",
        "# Compile the model\n",
        "model_r.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model_r.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 293
        },
        "outputId": "1f0c5c10-d6c2-4a98-b694-422f580a6bd0",
        "id": "YwdlVNPpp1v0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"sequential_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_9 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m, \u001b[38;5;34m64\u001b[0m)              │         \u001b[38;5;34m173,056\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_10 (\u001b[38;5;33mLSTM\u001b[0m)                       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)                  │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (\u001b[38;5;33mDense\u001b[0m)                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)                  │             \u001b[38;5;34m330\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ lstm_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)              │         <span style=\"color: #00af00; text-decoration-color: #00af00\">173,056</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)                  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)                  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m185,802\u001b[0m (725.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,802</span> (725.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185,802\u001b[0m (725.79 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,802</span> (725.79 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_LSTM_R = np.zeros((738,10,611))\n",
        "Y_LSTM = np.zeros(738)\n",
        "for i in range(738):\n",
        "  meta_data = np.load(\"Brain_\"+str(i)+\".npy\")\n",
        "  meta_data_y = np.load(\"Brain_\"+str(i)+\"y.npy\")\n",
        "  Y_LSTM[i] = meta_data_y\n",
        "  for j in range(brain_data_RH.shape[1]):\n",
        "    X_LSTM_R[i,:,j] = meta_data[brain_data_RH[0,j]-60,brain_data_RH[1,j]-67,brain_data_RH[2,j]-23,:]\n",
        "\n"
      ],
      "metadata": {
        "id": "EV-rrTaNp1v1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Y_LSTM"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01008805-2832-4f9c-dca6-3fce9bba8750",
        "id": "_jRO-zCwp1v2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([8., 2., 4., 3., 5., 0., 7., 1., 6., 9., 8., 2., 4., 3., 5., 0., 7.,\n",
              "       1., 6., 9., 8., 2., 4., 3., 5., 0., 7., 1., 6., 9., 8., 2., 4., 3.,\n",
              "       5., 0., 7., 1., 6., 9., 8., 0., 7., 2., 6., 8., 4., 9., 3., 1., 5.,\n",
              "       0., 7., 2., 6., 8., 4., 9., 3., 1., 5., 0., 7., 2., 6., 8., 4., 9.,\n",
              "       3., 1., 5., 0., 7., 2., 6., 8., 4., 9., 3., 1., 5., 0., 9., 6., 3.,\n",
              "       2., 4., 8., 5., 1., 0., 7., 9., 6., 3., 2., 4., 8., 5., 1., 0., 7.,\n",
              "       9., 6., 3., 2., 4., 8., 5., 1., 0., 7., 9., 6., 3., 2., 4., 8., 5.,\n",
              "       1., 0., 7., 9., 9., 0., 4., 2., 7., 3., 6., 5., 1., 8., 9., 0., 4.,\n",
              "       2., 7., 3., 6., 5., 1., 8., 9., 0., 4., 2., 7., 3., 6., 5., 1., 8.,\n",
              "       9., 0., 4., 2., 7., 3., 6., 5., 1., 8., 9., 7., 8., 4., 1., 0., 5.,\n",
              "       6., 2., 3., 9., 7., 8., 4., 1., 0., 5., 6., 2., 3., 9., 7., 8., 4.,\n",
              "       1., 0., 5., 6., 2., 3., 9., 7., 8., 4., 1., 0., 5., 6., 2., 3., 9.,\n",
              "       7., 7., 3., 8., 1., 5., 0., 9., 6., 4., 2., 7., 3., 8., 1., 5., 0.,\n",
              "       9., 6., 4., 2., 7., 3., 8., 1., 5., 0., 9., 6., 4., 2., 7., 3., 8.,\n",
              "       1., 5., 0., 9., 6., 4., 2., 7., 5., 3., 6., 8., 9., 1., 5., 4., 0.,\n",
              "       7., 2., 3., 9., 2., 1., 7., 4., 6., 8., 0., 5., 3., 2., 1., 5., 6.,\n",
              "       7., 8., 0., 9., 4., 9., 7., 5., 8., 2., 3., 1., 4., 6., 0., 0., 2.,\n",
              "       0., 5., 9., 4., 7., 6., 8., 1., 3., 2., 6., 5., 9., 0., 8., 7., 3.,\n",
              "       4., 1., 2., 6., 0., 4., 8., 7., 9., 1., 5., 3., 9., 7., 3., 5., 6.,\n",
              "       2., 0., 4., 1., 8., 8., 0., 8., 9., 2., 4., 5., 7., 1., 3., 6., 8.,\n",
              "       1., 0., 7., 4., 2., 5., 3., 9., 6., 0., 3., 2., 6., 8., 1., 7., 4.,\n",
              "       5., 9., 6., 9., 1., 8., 3., 2., 0., 5., 4., 7., 7., 5., 6., 2., 9.,\n",
              "       4., 7., 0., 8., 1., 3., 9., 6., 7., 4., 0., 2., 1., 5., 3., 8., 3.,\n",
              "       4., 9., 1., 7., 6., 8., 0., 2., 5., 8., 3., 6., 7., 4., 2., 5., 1.,\n",
              "       9., 0., 0., 6., 1., 7., 4., 9., 0., 5., 3., 2., 8., 9., 8., 6., 0.,\n",
              "       3., 7., 1., 4., 2., 5., 6., 0., 5., 1., 2., 4., 9., 8., 7., 3., 4.,\n",
              "       6., 1., 0., 2., 8., 9., 5., 3., 7., 7., 4., 1., 2., 6., 7., 5., 9.,\n",
              "       0., 3., 8., 2., 9., 1., 6., 0., 4., 5., 7., 3., 8., 1., 7., 9., 0.,\n",
              "       3., 2., 6., 8., 5., 4., 1., 4., 8., 0., 3., 2., 7., 6., 5., 9., 9.,\n",
              "       8., 9., 4., 1., 6., 5., 7., 0., 2., 3., 0., 6., 2., 4., 8., 5., 1.,\n",
              "       7., 3., 9., 8., 7., 1., 9., 2., 6., 3., 5., 0., 4., 7., 9., 5., 2.,\n",
              "       0., 3., 1., 6., 8., 4., 4., 7., 0., 4., 8., 2., 6., 5., 3., 9., 1.,\n",
              "       5., 1., 4., 3., 2., 9., 8., 6., 7., 0., 9., 3., 7., 6., 0., 5., 8.,\n",
              "       4., 1., 2., 9., 5., 2., 4., 1., 0., 8., 7., 6., 3., 3., 9., 3., 4.,\n",
              "       7., 6., 5., 2., 1., 0., 8., 3., 2., 5., 0., 8., 9., 7., 6., 1., 4.,\n",
              "       6., 5., 9., 7., 3., 2., 8., 4., 1., 0., 3., 9., 2., 4., 7., 5., 0.,\n",
              "       8., 6., 1., 1., 0., 2., 4., 9., 1., 7., 6., 5., 3., 8., 4., 5., 6.,\n",
              "       3., 2., 9., 1., 7., 8., 0., 8., 2., 7., 4., 3., 0., 1., 5., 9., 6.,\n",
              "       7., 4., 5., 3., 2., 8., 0., 9., 6., 1., 1., 3., 1., 2., 5., 7., 4.,\n",
              "       9., 6., 8., 0., 1., 8., 4., 5., 6., 0., 7., 3., 2., 9., 3., 4., 0.,\n",
              "       9., 2., 6., 8., 5., 7., 1., 4., 1., 0., 9., 3., 6., 2., 8., 5., 7.,\n",
              "       7., 1., 7., 0., 2., 6., 8., 5., 9., 4., 3., 4., 1., 8., 5., 6., 3.,\n",
              "       2., 7., 0., 9., 8., 2., 3., 0., 7., 4., 9., 5., 6., 1., 2., 8., 7.,\n",
              "       1., 9., 3., 4., 6., 0., 5.])"
            ]
          },
          "metadata": {},
          "execution_count": 60
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate the training loop for X_LSTM_R and Y_LSTM for model_r\n",
        "\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "# Convert Y_LSTM to one-hot encoding\n",
        "Y_LSTM_cat = to_categorical(Y_LSTM, num_classes=10)\n",
        "\n",
        "# Split data into training, validation, and test sets\n",
        "X_train_r = X_LSTM_R[train_idx]\n",
        "y_train_r = Y_LSTM_cat[train_idx]\n",
        "X_val_r = X_LSTM_R[val_idx]\n",
        "y_val_r = Y_LSTM_cat[val_idx]\n",
        "X_test_r = X_LSTM_R[test_idx]\n",
        "y_test_r = Y_LSTM_cat[test_idx]\n",
        "\n",
        "# Train the LSTM model\n",
        "epochs = 100\n",
        "batch_size = 4\n",
        "history = model_r.fit(\n",
        "    X_train_r, y_train_r,\n",
        "    epochs=epochs,\n",
        "    batch_size=batch_size,\n",
        "    validation_data=(X_val_r, y_val_r)\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "605f398b-8827-4702-f44d-9739164161a5",
        "id": "-RJPykNRp1v2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 7ms/step - accuracy: 0.1089 - loss: 2.3500 - val_accuracy: 0.0721 - val_loss: 2.3042\n",
            "Epoch 2/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0906 - loss: 2.3204 - val_accuracy: 0.0811 - val_loss: 2.3251\n",
            "Epoch 3/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0868 - loss: 2.3035 - val_accuracy: 0.1081 - val_loss: 2.3034\n",
            "Epoch 4/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0739 - loss: 2.3123 - val_accuracy: 0.1081 - val_loss: 2.3115\n",
            "Epoch 5/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0836 - loss: 2.3128 - val_accuracy: 0.1081 - val_loss: 2.3078\n",
            "Epoch 6/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1254 - loss: 2.3066 - val_accuracy: 0.0631 - val_loss: 2.3244\n",
            "Epoch 7/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1081 - loss: 2.3026 - val_accuracy: 0.0991 - val_loss: 2.3138\n",
            "Epoch 8/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1052 - loss: 2.3040 - val_accuracy: 0.1081 - val_loss: 2.3113\n",
            "Epoch 9/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1105 - loss: 2.3038 - val_accuracy: 0.0631 - val_loss: 2.3179\n",
            "Epoch 10/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0851 - loss: 2.3057 - val_accuracy: 0.0631 - val_loss: 2.3121\n",
            "Epoch 11/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0799 - loss: 2.3084 - val_accuracy: 0.0721 - val_loss: 2.3186\n",
            "Epoch 12/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0779 - loss: 2.3050 - val_accuracy: 0.0721 - val_loss: 2.3161\n",
            "Epoch 13/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1143 - loss: 2.3022 - val_accuracy: 0.0811 - val_loss: 2.3099\n",
            "Epoch 14/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0694 - loss: 2.3103 - val_accuracy: 0.0811 - val_loss: 2.3141\n",
            "Epoch 15/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0765 - loss: 2.3064 - val_accuracy: 0.0631 - val_loss: 2.3103\n",
            "Epoch 16/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1342 - loss: 2.3022 - val_accuracy: 0.0631 - val_loss: 2.3146\n",
            "Epoch 17/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1125 - loss: 2.3036 - val_accuracy: 0.0721 - val_loss: 2.3125\n",
            "Epoch 18/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1155 - loss: 2.3060 - val_accuracy: 0.0631 - val_loss: 2.3160\n",
            "Epoch 19/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1257 - loss: 2.2997 - val_accuracy: 0.0541 - val_loss: 2.3133\n",
            "Epoch 20/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1277 - loss: 2.2990 - val_accuracy: 0.1081 - val_loss: 2.3111\n",
            "Epoch 21/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1083 - loss: 2.3053 - val_accuracy: 0.0721 - val_loss: 2.3150\n",
            "Epoch 22/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1259 - loss: 2.3028 - val_accuracy: 0.0811 - val_loss: 2.3112\n",
            "Epoch 23/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0794 - loss: 2.3065 - val_accuracy: 0.0721 - val_loss: 2.3149\n",
            "Epoch 24/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1162 - loss: 2.2995 - val_accuracy: 0.0541 - val_loss: 2.3125\n",
            "Epoch 25/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1146 - loss: 2.3048 - val_accuracy: 0.1081 - val_loss: 2.3156\n",
            "Epoch 26/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0994 - loss: 2.3048 - val_accuracy: 0.0541 - val_loss: 2.3123\n",
            "Epoch 27/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1047 - loss: 2.3056 - val_accuracy: 0.0541 - val_loss: 2.3162\n",
            "Epoch 28/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0976 - loss: 2.3027 - val_accuracy: 0.0541 - val_loss: 2.3191\n",
            "Epoch 29/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1040 - loss: 2.3042 - val_accuracy: 0.0721 - val_loss: 2.3155\n",
            "Epoch 30/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0951 - loss: 2.3079 - val_accuracy: 0.1081 - val_loss: 2.3196\n",
            "Epoch 31/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1247 - loss: 2.2979 - val_accuracy: 0.0631 - val_loss: 2.3114\n",
            "Epoch 32/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1618 - loss: 2.2960 - val_accuracy: 0.0721 - val_loss: 2.3122\n",
            "Epoch 33/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1233 - loss: 2.3034 - val_accuracy: 0.0901 - val_loss: 2.3136\n",
            "Epoch 34/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1013 - loss: 2.2995 - val_accuracy: 0.0541 - val_loss: 2.3187\n",
            "Epoch 35/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1057 - loss: 2.2989 - val_accuracy: 0.0811 - val_loss: 2.3218\n",
            "Epoch 36/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1164 - loss: 2.2993 - val_accuracy: 0.0631 - val_loss: 2.3205\n",
            "Epoch 37/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1063 - loss: 2.2972 - val_accuracy: 0.0631 - val_loss: 2.3207\n",
            "Epoch 38/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1264 - loss: 2.2968 - val_accuracy: 0.0541 - val_loss: 2.3236\n",
            "Epoch 39/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1129 - loss: 2.2954 - val_accuracy: 0.0541 - val_loss: 2.3202\n",
            "Epoch 40/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1285 - loss: 2.2816 - val_accuracy: 0.0631 - val_loss: 2.3255\n",
            "Epoch 41/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.0832 - loss: 2.3077 - val_accuracy: 0.0721 - val_loss: 2.3196\n",
            "Epoch 42/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1521 - loss: 2.2891 - val_accuracy: 0.0631 - val_loss: 2.3216\n",
            "Epoch 43/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1329 - loss: 2.2886 - val_accuracy: 0.0631 - val_loss: 2.3259\n",
            "Epoch 44/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1341 - loss: 2.2868 - val_accuracy: 0.0721 - val_loss: 2.3262\n",
            "Epoch 45/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1315 - loss: 2.2782 - val_accuracy: 0.0901 - val_loss: 2.3244\n",
            "Epoch 46/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1218 - loss: 2.2887 - val_accuracy: 0.0721 - val_loss: 2.3253\n",
            "Epoch 47/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1486 - loss: 2.2823 - val_accuracy: 0.0631 - val_loss: 2.3513\n",
            "Epoch 48/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1343 - loss: 2.2685 - val_accuracy: 0.0901 - val_loss: 2.3426\n",
            "Epoch 49/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1536 - loss: 2.2842 - val_accuracy: 0.0541 - val_loss: 2.3562\n",
            "Epoch 50/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1718 - loss: 2.2463 - val_accuracy: 0.0721 - val_loss: 2.3531\n",
            "Epoch 51/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1314 - loss: 2.2718 - val_accuracy: 0.0901 - val_loss: 2.3237\n",
            "Epoch 52/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1933 - loss: 2.2390 - val_accuracy: 0.0991 - val_loss: 2.3277\n",
            "Epoch 53/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1967 - loss: 2.2415 - val_accuracy: 0.0811 - val_loss: 2.3176\n",
            "Epoch 54/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1497 - loss: 2.2454 - val_accuracy: 0.0811 - val_loss: 2.3166\n",
            "Epoch 55/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2010 - loss: 2.2128 - val_accuracy: 0.1081 - val_loss: 2.3303\n",
            "Epoch 56/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1791 - loss: 2.2455 - val_accuracy: 0.0901 - val_loss: 2.3561\n",
            "Epoch 57/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1611 - loss: 2.2258 - val_accuracy: 0.1532 - val_loss: 2.3147\n",
            "Epoch 58/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1964 - loss: 2.2034 - val_accuracy: 0.1622 - val_loss: 2.3337\n",
            "Epoch 59/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1636 - loss: 2.2165 - val_accuracy: 0.0631 - val_loss: 2.3230\n",
            "Epoch 60/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1518 - loss: 2.1981 - val_accuracy: 0.1622 - val_loss: 2.3023\n",
            "Epoch 61/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2024 - loss: 2.1889 - val_accuracy: 0.1532 - val_loss: 2.3160\n",
            "Epoch 62/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2023 - loss: 2.1581 - val_accuracy: 0.1261 - val_loss: 2.3177\n",
            "Epoch 63/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1988 - loss: 2.1787 - val_accuracy: 0.1261 - val_loss: 2.3000\n",
            "Epoch 64/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1906 - loss: 2.1551 - val_accuracy: 0.0901 - val_loss: 2.3169\n",
            "Epoch 65/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2186 - loss: 2.1406 - val_accuracy: 0.1622 - val_loss: 2.2999\n",
            "Epoch 66/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2101 - loss: 2.1710 - val_accuracy: 0.1622 - val_loss: 2.2965\n",
            "Epoch 67/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2089 - loss: 2.1421 - val_accuracy: 0.1171 - val_loss: 2.3085\n",
            "Epoch 68/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1928 - loss: 2.1528 - val_accuracy: 0.1532 - val_loss: 2.2819\n",
            "Epoch 69/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2049 - loss: 2.1434 - val_accuracy: 0.1892 - val_loss: 2.3140\n",
            "Epoch 70/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1988 - loss: 2.1063 - val_accuracy: 0.1622 - val_loss: 2.3454\n",
            "Epoch 71/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1893 - loss: 2.1249 - val_accuracy: 0.1622 - val_loss: 2.2927\n",
            "Epoch 72/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2212 - loss: 2.0943 - val_accuracy: 0.1622 - val_loss: 2.3203\n",
            "Epoch 73/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1971 - loss: 2.1246 - val_accuracy: 0.1441 - val_loss: 2.2920\n",
            "Epoch 74/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2066 - loss: 2.1172 - val_accuracy: 0.1712 - val_loss: 2.3796\n",
            "Epoch 75/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2026 - loss: 2.1534 - val_accuracy: 0.1892 - val_loss: 2.3254\n",
            "Epoch 76/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2081 - loss: 2.1081 - val_accuracy: 0.1532 - val_loss: 2.3171\n",
            "Epoch 77/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.1972 - loss: 2.0969 - val_accuracy: 0.1802 - val_loss: 2.3140\n",
            "Epoch 78/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2282 - loss: 2.1148 - val_accuracy: 0.2072 - val_loss: 2.3176\n",
            "Epoch 79/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2461 - loss: 2.0843 - val_accuracy: 0.1081 - val_loss: 2.3199\n",
            "Epoch 80/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2092 - loss: 2.0953 - val_accuracy: 0.1441 - val_loss: 2.3391\n",
            "Epoch 81/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2160 - loss: 2.0355 - val_accuracy: 0.1532 - val_loss: 2.3400\n",
            "Epoch 82/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2511 - loss: 2.0778 - val_accuracy: 0.1802 - val_loss: 2.3409\n",
            "Epoch 83/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2245 - loss: 2.0709 - val_accuracy: 0.1441 - val_loss: 2.3395\n",
            "Epoch 84/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2462 - loss: 2.0300 - val_accuracy: 0.1532 - val_loss: 2.4292\n",
            "Epoch 85/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2468 - loss: 2.0996 - val_accuracy: 0.1712 - val_loss: 2.2889\n",
            "Epoch 86/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2351 - loss: 2.0446 - val_accuracy: 0.1261 - val_loss: 2.3893\n",
            "Epoch 87/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2620 - loss: 2.0204 - val_accuracy: 0.1351 - val_loss: 2.3328\n",
            "Epoch 88/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2535 - loss: 1.9834 - val_accuracy: 0.2342 - val_loss: 2.3378\n",
            "Epoch 89/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2557 - loss: 1.9734 - val_accuracy: 0.1261 - val_loss: 2.3219\n",
            "Epoch 90/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2652 - loss: 2.0019 - val_accuracy: 0.2072 - val_loss: 2.3260\n",
            "Epoch 91/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2340 - loss: 2.0325 - val_accuracy: 0.1802 - val_loss: 2.3481\n",
            "Epoch 92/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2812 - loss: 1.9524 - val_accuracy: 0.1802 - val_loss: 2.3576\n",
            "Epoch 93/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2586 - loss: 1.9736 - val_accuracy: 0.1892 - val_loss: 2.3799\n",
            "Epoch 94/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2683 - loss: 1.9524 - val_accuracy: 0.0991 - val_loss: 2.4519\n",
            "Epoch 95/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2941 - loss: 1.9874 - val_accuracy: 0.2162 - val_loss: 2.3707\n",
            "Epoch 96/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2902 - loss: 1.9512 - val_accuracy: 0.1441 - val_loss: 2.3531\n",
            "Epoch 97/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2881 - loss: 1.9397 - val_accuracy: 0.0901 - val_loss: 2.4741\n",
            "Epoch 98/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2720 - loss: 1.9618 - val_accuracy: 0.1351 - val_loss: 2.3972\n",
            "Epoch 99/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.2746 - loss: 1.9243 - val_accuracy: 0.0991 - val_loss: 2.3855\n",
            "Epoch 100/100\n",
            "\u001b[1m129/129\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.3127 - loss: 1.8625 - val_accuracy: 0.2252 - val_loss: 2.4454\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-dgqPj2BqJwm"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}