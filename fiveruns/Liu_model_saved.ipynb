{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "scrolled": true,
        "id": "__zNgAufCM-8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import keras\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from keras.layers import Input, Dense, Flatten, Lambda, Dropout, Activation, LSTM, GRU, \\\n",
        "        TimeDistributed, Convolution1D, MaxPooling1D, Convolution2D, MaxPooling2D, \\\n",
        "        BatchNormalization, GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, \\\n",
        "        ZeroPadding2D, Reshape,  GlobalAveragePooling2D, GlobalMaxPooling2D, AveragePooling2D\n",
        "# from keras.layers.local import LocallyConnected1D\n",
        "# from keras.layers.advanced_activations import ELU\n",
        "from keras.optimizers import Adam, RMSprop\n",
        "from keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping, CSVLogger, TensorBoard\n",
        "from keras import backend as K\n",
        "from keras.models import Model\n",
        "from keras.models import load_model\n",
        "from sklearn.model_selection import train_test_split\n",
        "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
        "%matplotlib inline\n",
        "\n",
        "# import tensorflow.compat.v1 as tf\n",
        "# import keras.backend as KTF\n",
        "# config = tf.ConfigProto()\n",
        "# config.gpu_options.allow_growth=True   #不全部占满显存, 按需分配\n",
        "# sess = tf.Session(config=config)\n",
        "# tf.keras.backend.set_session(sess)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Fbue7CBBFT0O",
        "outputId": "c3e511de-73d1-4f44-8567-4677f556259b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()\n",
        "if device_name != '/device:GPU:0':\n",
        "  raise SystemError('GPU device not found')\n",
        "print('Found GPU at: {}'.format(device_name))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EPMBA_c-GDgO",
        "outputId": "882f6028-2176-4bc4-fd31-98c0212a21b3"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "device_name = tf.test.gpu_device_name()"
      ],
      "metadata": {
        "id": "UZkuYCqgGDl7"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(tf.config.list_physical_devices('GPU'))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1DZWoTiGFzo",
        "outputId": "145a581c-7038-4960-8dde-baa2f3ce09d1"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "cbKWCMYVCM--"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
        "if gpus:\n",
        "  try:\n",
        "    # Set memory growth to true\n",
        "    for gpu in gpus:\n",
        "      tf.config.experimental.set_memory_growth(gpu, True)\n",
        "  except RuntimeError as e:\n",
        "    # Memory growth must be set before GPUs have been initialized\n",
        "    print(e)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "xGM0J2jTCM--",
        "outputId": "9a9d70d4-4f45-457c-e59f-0cff915d0fc2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "#cannot import name 'merge' from 'keras.layers'\n",
        "tf.config.experimental.list_physical_devices('GPU')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "9AHVQ5GuCM-_",
        "outputId": "36785c17-588f-48f6-ccaf-18c1017ed345",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: numpy<2.1.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.1.31)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "scrolled": true,
        "id": "Id3rczCnCM-_"
      },
      "outputs": [],
      "source": [
        "\n",
        "def base_conv_block(num_conv_filters, kernel_size):\n",
        "    def f(input_):\n",
        "        x = BatchNormalization()(input_)\n",
        "        x = Activation('relu')(x)\n",
        "        out = Convolution2D(num_conv_filters, kernel_size, padding='same')(x)\n",
        "        return out\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "scrolled": true,
        "id": "t52l87ldCM-_"
      },
      "outputs": [],
      "source": [
        "def multi_scale_block(num_conv_filters):\n",
        "    def f(input_):\n",
        "        branch1x1 = base_conv_block(num_conv_filters, 1)(input_)\n",
        "\n",
        "        branch3x3 = base_conv_block(num_conv_filters, 1)(input_)\n",
        "        branch3x3 = base_conv_block(num_conv_filters, 3)(branch3x3)\n",
        "\n",
        "        branch5x5 = base_conv_block(num_conv_filters, 1)(input_)\n",
        "        branch5x5 = base_conv_block(num_conv_filters, 5)(branch5x5)\n",
        "\n",
        "        branchpool = MaxPooling2D(pool_size=(3,3), strides=(1,1), padding='same')(input_)\n",
        "        branchpool = base_conv_block(num_conv_filters, 1)(branchpool)\n",
        "\n",
        "        out = concatenate([branch1x1,branch3x3,branch5x5,branchpool], axis=-1)\n",
        "#         out = base_conv_block(num_conv_filters, 1)(out)\n",
        "        return out\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "scrolled": true,
        "id": "GHXmJYNJCM-_"
      },
      "outputs": [],
      "source": [
        "def dense_block(num_dense_blocks, num_conv_filters):\n",
        "    def f(input_):\n",
        "        x = input_\n",
        "        for _ in range(num_dense_blocks):\n",
        "            out = multi_scale_block(num_conv_filters)(x)\n",
        "            x = concatenate([x, out], axis=-1)\n",
        "        return x\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "scrolled": true,
        "id": "9gks73NLCM_A"
      },
      "outputs": [],
      "source": [
        "def transition_block(num_conv_filters):\n",
        "    def f(input_):\n",
        "        x = BatchNormalization()(input_)\n",
        "        x = Activation('relu')(x)\n",
        "        x = Convolution2D(num_conv_filters, 1)(x)\n",
        "        out = AveragePooling2D(pool_size=(2, 2), strides=(2, 2))(x)\n",
        "        return out\n",
        "    return f"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "scrolled": true,
        "id": "r5CsfFfSCM_A"
      },
      "outputs": [],
      "source": [
        "def multi_scale_level_cnn(input_shape, num_dense_blocks, num_conv_filters, num_classes):\n",
        "    model_input = Input(shape=input_shape)\n",
        "\n",
        "    x = Convolution2D(num_conv_filters, 3, padding='same')(model_input)\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = MaxPooling2D(pool_size=(4, 1))(x)\n",
        "\n",
        "    x = dense_block(num_dense_blocks, num_conv_filters)(x)\n",
        "    x = transition_block(num_conv_filters)(x)\n",
        "\n",
        "    x = BatchNormalization()(x)\n",
        "    x = Activation('relu')(x)\n",
        "    x = GlobalAveragePooling2D()(x)\n",
        "\n",
        "    model_output = Dense(num_classes, activation='softmax')(x)\n",
        "\n",
        "    model = Model(inputs=model_input, outputs=model_output)\n",
        "\n",
        "    return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "scrolled": true,
        "id": "ypab0iGoCM_A"
      },
      "outputs": [],
      "source": [
        "def process_data_for_conv2D(X, resize_shape=None):\n",
        "    X_conv2D = []\n",
        "    for sample in X:\n",
        "        sample = np.reshape(sample, newshape=(sample.shape[0], sample.shape[1], 1))\n",
        "        if resize_shape:\n",
        "            sample = resize(sample, output_shape=resize_shape)\n",
        "        X_conv2D.append(sample)\n",
        "    return np.array(X_conv2D, dtype=np.float32)\n",
        "\n",
        "def data_iter(X, y, batch_size):\n",
        "    num_samples = X.shape[0]\n",
        "    idx = list(range(num_samples))\n",
        "    while True:\n",
        "        for i in range(0, num_samples, batch_size):\n",
        "            j = idx[i:min(i+batch_size, num_samples)]\n",
        "            yield X[j, :], y[j, :]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "scrolled": true,
        "id": "m68-fxITCM_A"
      },
      "outputs": [],
      "source": [
        "def train_val_test_split(X, y, train_size, val_size, test_size):\n",
        "    X_train, X_val_test, y_train, y_val_test = train_test_split(X, y, train_size=train_size, stratify=y)\n",
        "    X_val, X_test, y_val, y_test = train_test_split(X_val_test, y_val_test, test_size=test_size/(test_size + val_size), stratify=y_val_test)\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "scrolled": true,
        "id": "kZNZ2_3GCM_A",
        "outputId": "48030d1d-9eb6-4bd1-8f19-03f018024090",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(540, 646, 128, 1)\n",
            "(540, 10)\n"
          ]
        }
      ],
      "source": [
        "# X_melspec = np.load('/share/音乐分类2/GTZAN/without_split_features/melspec_feature_2048.npy')\n",
        "# y = np.load('/share/音乐分类2/GTZAN/onehot_labels.npy')\n",
        "import numpy as np\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "X_melspec = np.load('/content/drive/MyDrive/X_spectro.npy')\n",
        "y = np.load('/content/drive/MyDrive/Y_spectro.npy')\n",
        "y_one = OneHotEncoder().fit_transform(y.reshape(-1, 1))\n",
        "# X_melspec = X_melspec.transpose(0,2,1)\n",
        "X_melspec.shape\n",
        "X_melspec = process_data_for_conv2D(X_melspec)\n",
        "print(X_melspec.shape)\n",
        "print(y_one.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "scrolled": true,
        "id": "P2OHDSZ5CM_A",
        "outputId": "09ff3033-3ec0-4e94-ccf6-865bd080a49a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m646\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
              "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │ \u001b[38;5;34m1\u001b[0m)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (\u001b[38;5;33mConv2D\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m646\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m320\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m646\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m646\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ activation[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]  │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_2        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_4        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,056\u001b[0m │ activation_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,056\u001b[0m │ activation_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_1     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d_2[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d_4[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ max_pooling2d_1[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_1        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_3        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_5        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_6        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,056\u001b[0m │ activation_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_5 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m25,632\u001b[0m │ activation_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_6 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,056\u001b[0m │ activation_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_3[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ conv2d_5[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ conv2d_6[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ max_pooling2d[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m160\u001b[0m)              │            │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m640\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m640\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_8        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_10       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_8 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m5,152\u001b[0m │ activation_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_10 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m5,152\u001b[0m │ activation_10[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_2     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m640\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d_8[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]    │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d_10[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m640\u001b[0m │ max_pooling2d_2[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_7        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_9        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_11       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_12       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m160\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_7 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m5,152\u001b[0m │ activation_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_9 (\u001b[38;5;33mConv2D\u001b[0m)   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m…\u001b[0m │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_11 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m25,632\u001b[0m │ activation_11[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_12 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m5,152\u001b[0m │ activation_12[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_2       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_7[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_9[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],   │\n",
              "│                     │                   │            │ conv2d_11[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ conv2d_12[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_3       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ concatenate_1[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m288\u001b[0m)              │            │ concatenate_2[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,152\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,152\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_14       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_16       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_14 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_14[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_16 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_16[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_3     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mMaxPooling2D\u001b[0m)      │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,152\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d_14[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │        \u001b[38;5;34m128\u001b[0m │ conv2d_16[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,152\u001b[0m │ max_pooling2d_3[\u001b[38;5;34m…\u001b[0m │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_13       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_15       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_17       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_18       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m288\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_13 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_13[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_15 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_15[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_17 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m25,632\u001b[0m │ activation_17[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_18 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m9,248\u001b[0m │ activation_18[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_4       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ conv2d_13[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m128\u001b[0m)              │            │ conv2d_15[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ conv2d_17[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m],  │\n",
              "│                     │                   │            │ conv2d_18[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_5       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ concatenate_3[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mConcatenate\u001b[0m)       │ \u001b[38;5;34m416\u001b[0m)              │            │ concatenate_4[\u001b[38;5;34m0\u001b[0m]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │      \u001b[38;5;34m1,664\u001b[0m │ concatenate_5[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m416\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_19       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m416\u001b[0m)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_19 (\u001b[38;5;33mConv2D\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m161\u001b[0m, \u001b[38;5;34m128\u001b[0m,  │     \u001b[38;5;34m13,344\u001b[0m │ activation_19[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│                     │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ average_pooling2d   │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ conv2d_19[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]   │\n",
              "│ (\u001b[38;5;33mAveragePooling2D\u001b[0m)  │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │        \u001b[38;5;34m128\u001b[0m │ average_pooling2… │\n",
              "│ (\u001b[38;5;33mBatchNormalizatio…\u001b[0m │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_20       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m, \u001b[38;5;34m64\u001b[0m,    │          \u001b[38;5;34m0\u001b[0m │ batch_normalizat… │\n",
              "│ (\u001b[38;5;33mActivation\u001b[0m)        │ \u001b[38;5;34m32\u001b[0m)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)        │          \u001b[38;5;34m0\u001b[0m │ activation_20[\u001b[38;5;34m0\u001b[0m]… │\n",
              "│ (\u001b[38;5;33mGlobalAveragePool…\u001b[0m │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)        │        \u001b[38;5;34m330\u001b[0m │ global_average_p… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">646</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>)                │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">646</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">320</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalization │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">646</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">646</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_2        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_4        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ activation_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ activation_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_1     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ max_pooling2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_1        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_3        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_5        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_6        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ activation_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,056</span> │ activation_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ conv2d_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ conv2d_6[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ max_pooling2d[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_8        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_10       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_8 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,152</span> │ activation_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_10 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,152</span> │ activation_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_2     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_8[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]    │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_10[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">640</span> │ max_pooling2d_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_7        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_9        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_11       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_12       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">160</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,152</span> │ activation_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_9 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_11 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_12 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">5,152</span> │ activation_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_2       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_7[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_9[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],   │\n",
              "│                     │                   │            │ conv2d_11[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ conv2d_12[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_3       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │ concatenate_2[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_14       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_16       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_14 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_16 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ max_pooling2d_3     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)      │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_14[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ conv2d_16[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,152</span> │ max_pooling2d_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">…</span> │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_13       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_15       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_17       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_18       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">288</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_13 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_15 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_17 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">25,632</span> │ activation_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_18 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">9,248</span> │ activation_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_4       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_13[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)              │            │ conv2d_15[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ conv2d_17[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>],  │\n",
              "│                     │                   │            │ conv2d_18[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ concatenate_5       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ concatenate_3[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>)       │ <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)              │            │ concatenate_4[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │      <span style=\"color: #00af00; text-decoration-color: #00af00\">1,664</span> │ concatenate_5[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_19       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">416</span>)              │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ conv2d_19 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">161</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>,  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">13,344</span> │ activation_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ average_pooling2d   │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ conv2d_19[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]   │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">AveragePooling2D</span>)  │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ batch_normalizatio… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │        <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span> │ average_pooling2… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">BatchNormalizatio…</span> │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ activation_20       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>,    │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ batch_normalizat… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Activation</span>)        │ <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)               │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ global_average_poo… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)        │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ activation_20[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePool…</span> │                   │            │                   │\n",
              "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)        │        <span style=\"color: #00af00; text-decoration-color: #00af00\">330</span> │ global_average_p… │\n",
              "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m190,826\u001b[0m (745.41 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">190,826</span> (745.41 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m185,642\u001b[0m (725.16 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">185,642</span> (725.16 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m5,184\u001b[0m (20.25 KB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,184</span> (20.25 KB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#check the architecture of the net\n",
        "model = multi_scale_level_cnn(input_shape=(X_melspec.shape[1], X_melspec.shape[2], X_melspec.shape[3]),\n",
        "                              num_dense_blocks=3, num_conv_filters=32, num_classes=10)\n",
        "# model = get_multi_level_cnn_model_3(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]), num_classes=10)\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HgnJXQaiCM_B",
        "outputId": "0f2f2f60-ce3a-4331-fcf8-3c62020112fe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 1.],\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 1., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 1., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 1.]])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "y_train"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_checkpoint"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnvMGVJxM_wu",
        "outputId": "0c3bcfe3-a82f-478b-9f4b-f3f7d15fa0f6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.ModelCheckpoint at 0x7a56a3a37100>"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Split the dataset into fixed training/validation (480 samples) and test (60 samples)\n",
        "X_train_val = X_melspec[:-60]\n",
        "y_train_val = y_one.toarray()[:-60]\n",
        "X_test_fixed = X_melspec[-60:]\n",
        "y_test_fixed = y_one.toarray()[-60:]\n",
        "\n",
        "k_fold = 5\n",
        "num_classes = 10\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 2\n",
        "lr = 0.01\n",
        "file_name0 = 'GTZAN_model.keras'\n",
        "path = '/content/drive/MyDrive/aime/logs/'\n",
        "csv_name0 = 'GTZAN_csv.csv'\n",
        "train_loss_record = []\n",
        "train_acc_record = []\n",
        "val_loss_record = []\n",
        "val_acc_record = []\n",
        "test_loss_record = []\n",
        "test_acc_record = []\n",
        "\n",
        "for i in range(k_fold):\n",
        "    print('Start %d fold training' % (i + 1))\n",
        "    # Split the training/validation data into train and validation (no test split)\n",
        "    X_train, X_val,y_train, y_val = train_test_split(\n",
        "        X_train_val, y_train_val,\n",
        "        train_size=420/480\n",
        "    )\n",
        "    # Use the fixed test set\n",
        "    X_test, y_test = X_test_fixed, y_test_fixed\n",
        "\n",
        "    file_name = '/content/drive/MyDrive/aime/Extend/' + str(i) + '_fold_' + file_name0\n",
        "    csv_path = path + str(i) + '_fold_' + csv_name0\n",
        "    lr_change = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, min_lr=0.000)\n",
        "    model_checkpoint = ModelCheckpoint(file_name, monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "    early_stopping = EarlyStopping(monitor='loss', min_delta=0.01, patience=10, mode='min')\n",
        "    csv_logger = CSVLogger(csv_path)\n",
        "    callbacks = [lr_change, model_checkpoint, early_stopping, csv_logger]\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model = multi_scale_level_cnn(input_shape=(X_melspec.shape[1], X_melspec.shape[2], X_melspec.shape[3]),\n",
        "                                  num_dense_blocks=3, num_conv_filters=32, num_classes=num_classes)\n",
        "    model.compile(\n",
        "        loss='categorical_crossentropy',\n",
        "        metrics=['accuracy'],\n",
        "        optimizer=opt)\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(X_val, y_val), verbose=1,\n",
        "              callbacks=callbacks)\n",
        "    model_best = load_model(file_name)\n",
        "    train_loss, train_acc = model_best.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
        "    val_loss, val_acc = model_best.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
        "    test_loss, test_acc = model_best.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    train_loss_record.append(train_loss)\n",
        "    train_acc_record.append(train_acc)\n",
        "    val_loss_record.append(val_loss)\n",
        "    val_acc_record.append(val_acc)\n",
        "    test_loss_record.append(test_loss)\n",
        "    test_acc_record.append(test_acc)\n",
        "    print('\\n\\n%d fold train loss %.4f train acc %.4f, val loss %.4f val acc %.4f, test loss %.4f test acc %.4f\\n\\n' %\n",
        "          (i + 1, train_loss, train_acc, val_loss, val_acc, test_loss, test_acc))\n",
        "\n",
        "train_loss_avg = np.mean(np.array(train_loss_record))\n",
        "train_acc_avg = np.mean(np.array(train_acc_record))\n",
        "val_loss_avg = np.mean(np.array(val_loss_record))\n",
        "val_acc_avg = np.mean(np.array(val_acc_record))\n",
        "test_loss_avg = np.mean(np.array(test_loss_record))\n",
        "test_acc_avg = np.mean(np.array(test_acc_record))\n",
        "print('\\n\\n%d fold train loss avg %.4f train acc avg %.4f, val loss avg %.4f val acc avg %.4f, test loss avg %.4f test acc avg %.4f' %\n",
        "      (k_fold, train_loss_avg, train_acc_avg, val_loss_avg, val_acc_avg, test_loss_avg, test_acc_avg))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SH9oeilJI7fE",
        "outputId": "04ac1936-9ab7-4ed6-f472-718e2100aa25"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start 1 fold training\n",
            "Epoch 1/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 43ms/step - accuracy: 0.1693 - loss: 2.2996 - val_accuracy: 0.1500 - val_loss: 4.3671 - learning_rate: 0.0100\n",
            "Epoch 2/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2320 - loss: 2.0696 - val_accuracy: 0.3500 - val_loss: 2.0070 - learning_rate: 0.0100\n",
            "Epoch 3/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2514 - loss: 2.0262 - val_accuracy: 0.2667 - val_loss: 2.4755 - learning_rate: 0.0100\n",
            "Epoch 4/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2464 - loss: 2.0063 - val_accuracy: 0.1333 - val_loss: 2.5234 - learning_rate: 0.0100\n",
            "Epoch 5/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3250 - loss: 1.9228 - val_accuracy: 0.1667 - val_loss: 3.8405 - learning_rate: 0.0100\n",
            "Epoch 6/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2582 - loss: 1.9783 - val_accuracy: 0.2667 - val_loss: 3.0353 - learning_rate: 0.0100\n",
            "Epoch 7/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2923 - loss: 1.8647 - val_accuracy: 0.0667 - val_loss: 14.5663 - learning_rate: 0.0100\n",
            "Epoch 8/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.2959 - loss: 1.9161 - val_accuracy: 0.2833 - val_loss: 1.9568 - learning_rate: 0.0100\n",
            "Epoch 9/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3182 - loss: 1.8087 - val_accuracy: 0.3000 - val_loss: 2.1563 - learning_rate: 0.0100\n",
            "Epoch 10/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3328 - loss: 1.8556 - val_accuracy: 0.2667 - val_loss: 1.8792 - learning_rate: 0.0100\n",
            "Epoch 11/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3711 - loss: 1.7114 - val_accuracy: 0.2667 - val_loss: 2.9469 - learning_rate: 0.0100\n",
            "Epoch 12/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3900 - loss: 1.7131 - val_accuracy: 0.2167 - val_loss: 4.2839 - learning_rate: 0.0100\n",
            "Epoch 13/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3370 - loss: 1.7702 - val_accuracy: 0.2167 - val_loss: 3.6841 - learning_rate: 0.0100\n",
            "Epoch 14/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4033 - loss: 1.6754 - val_accuracy: 0.2833 - val_loss: 2.3739 - learning_rate: 0.0100\n",
            "Epoch 15/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3953 - loss: 1.6184 - val_accuracy: 0.3333 - val_loss: 2.2766 - learning_rate: 0.0100\n",
            "Epoch 16/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4236 - loss: 1.5224 - val_accuracy: 0.1833 - val_loss: 5.8603 - learning_rate: 0.0100\n",
            "Epoch 17/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3754 - loss: 1.6197 - val_accuracy: 0.3000 - val_loss: 3.9460 - learning_rate: 0.0100\n",
            "Epoch 18/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4207 - loss: 1.6139 - val_accuracy: 0.2500 - val_loss: 3.1083 - learning_rate: 0.0100\n",
            "Epoch 19/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4497 - loss: 1.5387 - val_accuracy: 0.2500 - val_loss: 17.5595 - learning_rate: 0.0100\n",
            "Epoch 20/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4488 - loss: 1.5697 - val_accuracy: 0.3333 - val_loss: 2.4732 - learning_rate: 0.0100\n",
            "Epoch 21/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4601 - loss: 1.4576 - val_accuracy: 0.3833 - val_loss: 2.0092 - learning_rate: 0.0100\n",
            "Epoch 22/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4654 - loss: 1.4167 - val_accuracy: 0.3000 - val_loss: 2.6564 - learning_rate: 0.0100\n",
            "Epoch 23/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4401 - loss: 1.4385 - val_accuracy: 0.3500 - val_loss: 6.2782 - learning_rate: 0.0100\n",
            "Epoch 24/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4492 - loss: 1.4495 - val_accuracy: 0.2000 - val_loss: 4.9041 - learning_rate: 0.0100\n",
            "Epoch 25/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4720 - loss: 1.4336 - val_accuracy: 0.3833 - val_loss: 3.0832 - learning_rate: 0.0100\n",
            "Epoch 26/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4919 - loss: 1.2913 - val_accuracy: 0.4667 - val_loss: 1.6912 - learning_rate: 0.0100\n",
            "Epoch 27/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5830 - loss: 1.2471 - val_accuracy: 0.3500 - val_loss: 2.6545 - learning_rate: 0.0100\n",
            "Epoch 28/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5591 - loss: 1.2312 - val_accuracy: 0.3000 - val_loss: 2.0259 - learning_rate: 0.0100\n",
            "Epoch 29/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5471 - loss: 1.2461 - val_accuracy: 0.3500 - val_loss: 2.5808 - learning_rate: 0.0100\n",
            "Epoch 30/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5953 - loss: 1.1214 - val_accuracy: 0.3833 - val_loss: 2.1689 - learning_rate: 0.0100\n",
            "Epoch 31/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5706 - loss: 1.3008 - val_accuracy: 0.4833 - val_loss: 1.9136 - learning_rate: 0.0100\n",
            "Epoch 32/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5670 - loss: 1.1954 - val_accuracy: 0.4000 - val_loss: 2.3699 - learning_rate: 0.0100\n",
            "Epoch 33/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6180 - loss: 1.0898 - val_accuracy: 0.3167 - val_loss: 4.9075 - learning_rate: 0.0100\n",
            "Epoch 34/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6083 - loss: 1.1147 - val_accuracy: 0.3500 - val_loss: 9.3349 - learning_rate: 0.0100\n",
            "Epoch 35/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6557 - loss: 0.9942 - val_accuracy: 0.4667 - val_loss: 1.6436 - learning_rate: 0.0100\n",
            "Epoch 36/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6094 - loss: 1.0398 - val_accuracy: 0.4500 - val_loss: 2.0409 - learning_rate: 0.0100\n",
            "Epoch 37/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6040 - loss: 1.0924 - val_accuracy: 0.5167 - val_loss: 1.8490 - learning_rate: 0.0100\n",
            "Epoch 38/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6700 - loss: 0.9201 - val_accuracy: 0.5500 - val_loss: 1.5431 - learning_rate: 0.0100\n",
            "Epoch 39/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6290 - loss: 1.0846 - val_accuracy: 0.5167 - val_loss: 1.8404 - learning_rate: 0.0100\n",
            "Epoch 40/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5971 - loss: 1.1056 - val_accuracy: 0.4667 - val_loss: 2.2483 - learning_rate: 0.0100\n",
            "Epoch 41/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6275 - loss: 0.9861 - val_accuracy: 0.4333 - val_loss: 2.2924 - learning_rate: 0.0100\n",
            "Epoch 42/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6583 - loss: 0.9690 - val_accuracy: 0.5333 - val_loss: 1.8339 - learning_rate: 0.0100\n",
            "Epoch 43/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6706 - loss: 0.9377 - val_accuracy: 0.4333 - val_loss: 3.8167 - learning_rate: 0.0100\n",
            "Epoch 44/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7210 - loss: 0.9215 - val_accuracy: 0.4000 - val_loss: 2.6980 - learning_rate: 0.0100\n",
            "Epoch 45/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6717 - loss: 0.9867 - val_accuracy: 0.4667 - val_loss: 2.0892 - learning_rate: 0.0100\n",
            "Epoch 46/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6836 - loss: 0.8278 - val_accuracy: 0.5000 - val_loss: 3.2773 - learning_rate: 0.0100\n",
            "Epoch 47/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6398 - loss: 0.9664 - val_accuracy: 0.4333 - val_loss: 2.1290 - learning_rate: 0.0100\n",
            "Epoch 48/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6993 - loss: 0.8720 - val_accuracy: 0.5833 - val_loss: 1.4786 - learning_rate: 0.0100\n",
            "Epoch 49/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6742 - loss: 0.9009 - val_accuracy: 0.3333 - val_loss: 3.0006 - learning_rate: 0.0100\n",
            "Epoch 50/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7180 - loss: 0.8713 - val_accuracy: 0.5167 - val_loss: 1.8532 - learning_rate: 0.0100\n",
            "Epoch 51/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7039 - loss: 0.7427 - val_accuracy: 0.6000 - val_loss: 1.6973 - learning_rate: 0.0100\n",
            "Epoch 52/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7910 - loss: 0.6334 - val_accuracy: 0.5167 - val_loss: 2.1590 - learning_rate: 0.0100\n",
            "Epoch 53/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7906 - loss: 0.6815 - val_accuracy: 0.5500 - val_loss: 1.7112 - learning_rate: 0.0100\n",
            "Epoch 54/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7152 - loss: 0.7919 - val_accuracy: 0.5167 - val_loss: 2.6542 - learning_rate: 0.0100\n",
            "Epoch 55/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7557 - loss: 0.7562 - val_accuracy: 0.6333 - val_loss: 1.9190 - learning_rate: 0.0100\n",
            "Epoch 56/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7434 - loss: 0.6960 - val_accuracy: 0.5500 - val_loss: 2.1046 - learning_rate: 0.0100\n",
            "Epoch 57/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7654 - loss: 0.7232 - val_accuracy: 0.4833 - val_loss: 1.5355 - learning_rate: 0.0100\n",
            "Epoch 58/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7905 - loss: 0.6288 - val_accuracy: 0.5667 - val_loss: 1.5017 - learning_rate: 0.0050\n",
            "Epoch 59/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7937 - loss: 0.5656 - val_accuracy: 0.5167 - val_loss: 1.6866 - learning_rate: 0.0050\n",
            "Epoch 60/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8028 - loss: 0.6029 - val_accuracy: 0.4667 - val_loss: 2.2869 - learning_rate: 0.0050\n",
            "Epoch 61/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8348 - loss: 0.5713 - val_accuracy: 0.6333 - val_loss: 1.4785 - learning_rate: 0.0050\n",
            "Epoch 62/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8550 - loss: 0.5009 - val_accuracy: 0.5833 - val_loss: 1.5460 - learning_rate: 0.0050\n",
            "Epoch 63/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8543 - loss: 0.4856 - val_accuracy: 0.7000 - val_loss: 1.2946 - learning_rate: 0.0050\n",
            "Epoch 64/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8010 - loss: 0.5818 - val_accuracy: 0.5333 - val_loss: 1.6129 - learning_rate: 0.0050\n",
            "Epoch 65/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8556 - loss: 0.4719 - val_accuracy: 0.6500 - val_loss: 1.3998 - learning_rate: 0.0050\n",
            "Epoch 66/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8100 - loss: 0.5516 - val_accuracy: 0.5833 - val_loss: 1.6747 - learning_rate: 0.0050\n",
            "Epoch 67/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8217 - loss: 0.4724 - val_accuracy: 0.5667 - val_loss: 1.5660 - learning_rate: 0.0050\n",
            "Epoch 68/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8451 - loss: 0.5036 - val_accuracy: 0.6833 - val_loss: 1.5012 - learning_rate: 0.0050\n",
            "Epoch 69/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8389 - loss: 0.4604 - val_accuracy: 0.6500 - val_loss: 1.4479 - learning_rate: 0.0050\n",
            "Epoch 70/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8881 - loss: 0.3578 - val_accuracy: 0.6333 - val_loss: 1.7616 - learning_rate: 0.0050\n",
            "Epoch 71/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8328 - loss: 0.4703 - val_accuracy: 0.6833 - val_loss: 1.3290 - learning_rate: 0.0050\n",
            "Epoch 72/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8551 - loss: 0.4946 - val_accuracy: 0.6333 - val_loss: 1.5255 - learning_rate: 0.0050\n",
            "Epoch 73/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8606 - loss: 0.4570 - val_accuracy: 0.5667 - val_loss: 2.2115 - learning_rate: 0.0050\n",
            "Epoch 74/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8414 - loss: 0.5050 - val_accuracy: 0.6167 - val_loss: 1.7228 - learning_rate: 0.0025\n",
            "Epoch 75/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8582 - loss: 0.3863 - val_accuracy: 0.5667 - val_loss: 1.7215 - learning_rate: 0.0025\n",
            "Epoch 76/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8741 - loss: 0.4290 - val_accuracy: 0.6167 - val_loss: 1.6119 - learning_rate: 0.0025\n",
            "Epoch 77/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8916 - loss: 0.3361 - val_accuracy: 0.5500 - val_loss: 1.8568 - learning_rate: 0.0025\n",
            "Epoch 78/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8439 - loss: 0.4071 - val_accuracy: 0.6500 - val_loss: 1.6340 - learning_rate: 0.0025\n",
            "Epoch 79/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9178 - loss: 0.2887 - val_accuracy: 0.5833 - val_loss: 1.6063 - learning_rate: 0.0025\n",
            "Epoch 80/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9366 - loss: 0.2682 - val_accuracy: 0.5167 - val_loss: 1.7010 - learning_rate: 0.0025\n",
            "Epoch 81/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9083 - loss: 0.3330 - val_accuracy: 0.5833 - val_loss: 1.4518 - learning_rate: 0.0012\n",
            "Epoch 82/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8642 - loss: 0.3640 - val_accuracy: 0.6333 - val_loss: 1.4669 - learning_rate: 0.0012\n",
            "Epoch 83/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9139 - loss: 0.3019 - val_accuracy: 0.6833 - val_loss: 1.4424 - learning_rate: 0.0012\n",
            "Epoch 84/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9067 - loss: 0.3256 - val_accuracy: 0.6000 - val_loss: 1.4900 - learning_rate: 0.0012\n",
            "Epoch 85/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9212 - loss: 0.3150 - val_accuracy: 0.6000 - val_loss: 1.6054 - learning_rate: 0.0012\n",
            "Epoch 86/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9263 - loss: 0.2891 - val_accuracy: 0.6000 - val_loss: 1.4733 - learning_rate: 0.0012\n",
            "Epoch 87/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9249 - loss: 0.2763 - val_accuracy: 0.6500 - val_loss: 1.3928 - learning_rate: 0.0012\n",
            "Epoch 88/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9536 - loss: 0.2437 - val_accuracy: 0.6667 - val_loss: 1.4819 - learning_rate: 0.0012\n",
            "Epoch 89/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8998 - loss: 0.2972 - val_accuracy: 0.6333 - val_loss: 1.4572 - learning_rate: 0.0012\n",
            "Epoch 90/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9105 - loss: 0.2930 - val_accuracy: 0.6500 - val_loss: 1.5289 - learning_rate: 0.0012\n",
            "Epoch 91/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9072 - loss: 0.2791 - val_accuracy: 0.6833 - val_loss: 1.3356 - learning_rate: 0.0012\n",
            "Epoch 92/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9218 - loss: 0.2755 - val_accuracy: 0.6500 - val_loss: 1.3169 - learning_rate: 0.0012\n",
            "Epoch 93/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9350 - loss: 0.2467 - val_accuracy: 0.6500 - val_loss: 1.4278 - learning_rate: 0.0012\n",
            "Epoch 94/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9070 - loss: 0.3093 - val_accuracy: 0.7333 - val_loss: 1.3383 - learning_rate: 6.2500e-04\n",
            "Epoch 95/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9501 - loss: 0.2261 - val_accuracy: 0.6500 - val_loss: 1.4059 - learning_rate: 6.2500e-04\n",
            "Epoch 96/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9251 - loss: 0.2487 - val_accuracy: 0.7000 - val_loss: 1.3283 - learning_rate: 6.2500e-04\n",
            "Epoch 97/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9054 - loss: 0.2760 - val_accuracy: 0.6667 - val_loss: 1.3478 - learning_rate: 6.2500e-04\n",
            "Epoch 98/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9349 - loss: 0.2480 - val_accuracy: 0.6833 - val_loss: 1.3739 - learning_rate: 6.2500e-04\n",
            "Epoch 99/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9453 - loss: 0.2374 - val_accuracy: 0.7000 - val_loss: 1.3734 - learning_rate: 6.2500e-04\n",
            "Epoch 100/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.9578 - loss: 0.2069 - val_accuracy: 0.7167 - val_loss: 1.3797 - learning_rate: 6.2500e-04\n",
            "\n",
            "\n",
            "1 fold train loss 0.5303 train acc 0.8310, val loss 1.3383 val acc 0.7333, test loss 1.0650 test acc 0.7167\n",
            "\n",
            "\n",
            "Start 2 fold training\n",
            "Epoch 1/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 43ms/step - accuracy: 0.1531 - loss: 2.3040 - val_accuracy: 0.1667 - val_loss: 2.8113 - learning_rate: 0.0100\n",
            "Epoch 2/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.2379 - loss: 2.1350 - val_accuracy: 0.0500 - val_loss: 9.0803 - learning_rate: 0.0100\n",
            "Epoch 3/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2393 - loss: 2.0307 - val_accuracy: 0.3000 - val_loss: 3.7459 - learning_rate: 0.0100\n",
            "Epoch 4/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2885 - loss: 1.9251 - val_accuracy: 0.3333 - val_loss: 2.3490 - learning_rate: 0.0100\n",
            "Epoch 5/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3008 - loss: 1.9188 - val_accuracy: 0.2000 - val_loss: 3.0513 - learning_rate: 0.0100\n",
            "Epoch 6/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3060 - loss: 1.9940 - val_accuracy: 0.2000 - val_loss: 2.6809 - learning_rate: 0.0100\n",
            "Epoch 7/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3949 - loss: 1.7677 - val_accuracy: 0.2667 - val_loss: 3.7661 - learning_rate: 0.0100\n",
            "Epoch 8/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2834 - loss: 1.8315 - val_accuracy: 0.3500 - val_loss: 1.9933 - learning_rate: 0.0100\n",
            "Epoch 9/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3175 - loss: 1.8701 - val_accuracy: 0.3500 - val_loss: 1.7185 - learning_rate: 0.0100\n",
            "Epoch 10/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3770 - loss: 1.7550 - val_accuracy: 0.2667 - val_loss: 2.8981 - learning_rate: 0.0100\n",
            "Epoch 11/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.3638 - loss: 1.7096 - val_accuracy: 0.3833 - val_loss: 1.7634 - learning_rate: 0.0100\n",
            "Epoch 12/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.3866 - loss: 1.6067 - val_accuracy: 0.4833 - val_loss: 1.5823 - learning_rate: 0.0100\n",
            "Epoch 13/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3761 - loss: 1.6640 - val_accuracy: 0.3167 - val_loss: 5.0168 - learning_rate: 0.0100\n",
            "Epoch 14/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.3953 - loss: 1.5571 - val_accuracy: 0.3000 - val_loss: 1.7377 - learning_rate: 0.0100\n",
            "Epoch 15/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4351 - loss: 1.5374 - val_accuracy: 0.4000 - val_loss: 3.2463 - learning_rate: 0.0100\n",
            "Epoch 16/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4532 - loss: 1.4807 - val_accuracy: 0.2667 - val_loss: 1.8417 - learning_rate: 0.0100\n",
            "Epoch 17/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4825 - loss: 1.4024 - val_accuracy: 0.4333 - val_loss: 2.0606 - learning_rate: 0.0100\n",
            "Epoch 18/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4435 - loss: 1.5120 - val_accuracy: 0.3167 - val_loss: 2.1665 - learning_rate: 0.0100\n",
            "Epoch 19/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5092 - loss: 1.3072 - val_accuracy: 0.3333 - val_loss: 2.2462 - learning_rate: 0.0100\n",
            "Epoch 20/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5099 - loss: 1.3391 - val_accuracy: 0.3500 - val_loss: 1.8219 - learning_rate: 0.0100\n",
            "Epoch 21/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5082 - loss: 1.2773 - val_accuracy: 0.5167 - val_loss: 1.5531 - learning_rate: 0.0100\n",
            "Epoch 22/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5508 - loss: 1.2267 - val_accuracy: 0.4000 - val_loss: 2.3678 - learning_rate: 0.0100\n",
            "Epoch 23/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5419 - loss: 1.2126 - val_accuracy: 0.4833 - val_loss: 1.7087 - learning_rate: 0.0100\n",
            "Epoch 24/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5133 - loss: 1.2878 - val_accuracy: 0.3500 - val_loss: 2.3405 - learning_rate: 0.0100\n",
            "Epoch 25/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5638 - loss: 1.2504 - val_accuracy: 0.6000 - val_loss: 1.3363 - learning_rate: 0.0100\n",
            "Epoch 26/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5898 - loss: 1.1471 - val_accuracy: 0.5167 - val_loss: 1.2437 - learning_rate: 0.0050\n",
            "Epoch 27/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5995 - loss: 1.1221 - val_accuracy: 0.4167 - val_loss: 2.9524 - learning_rate: 0.0050\n",
            "Epoch 28/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6292 - loss: 1.0676 - val_accuracy: 0.5000 - val_loss: 1.8080 - learning_rate: 0.0050\n",
            "Epoch 29/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6287 - loss: 1.0245 - val_accuracy: 0.5333 - val_loss: 1.4450 - learning_rate: 0.0050\n",
            "Epoch 30/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6310 - loss: 1.0269 - val_accuracy: 0.4833 - val_loss: 1.4567 - learning_rate: 0.0050\n",
            "Epoch 31/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6669 - loss: 0.9505 - val_accuracy: 0.5500 - val_loss: 1.3282 - learning_rate: 0.0050\n",
            "Epoch 32/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6857 - loss: 0.9360 - val_accuracy: 0.4333 - val_loss: 1.4313 - learning_rate: 0.0050\n",
            "Epoch 33/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6668 - loss: 0.9782 - val_accuracy: 0.4333 - val_loss: 2.2179 - learning_rate: 0.0050\n",
            "Epoch 34/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6535 - loss: 0.9861 - val_accuracy: 0.6167 - val_loss: 1.1555 - learning_rate: 0.0050\n",
            "Epoch 35/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6497 - loss: 0.9622 - val_accuracy: 0.5833 - val_loss: 1.1789 - learning_rate: 0.0050\n",
            "Epoch 36/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6828 - loss: 0.9132 - val_accuracy: 0.4167 - val_loss: 2.4246 - learning_rate: 0.0050\n",
            "Epoch 37/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6566 - loss: 0.9673 - val_accuracy: 0.5667 - val_loss: 1.8798 - learning_rate: 0.0050\n",
            "Epoch 38/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6899 - loss: 0.9022 - val_accuracy: 0.5500 - val_loss: 1.3138 - learning_rate: 0.0050\n",
            "Epoch 39/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7021 - loss: 0.9106 - val_accuracy: 0.5500 - val_loss: 1.5945 - learning_rate: 0.0050\n",
            "Epoch 40/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6740 - loss: 0.8932 - val_accuracy: 0.5667 - val_loss: 1.3901 - learning_rate: 0.0050\n",
            "Epoch 41/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6956 - loss: 0.8773 - val_accuracy: 0.5000 - val_loss: 1.7569 - learning_rate: 0.0050\n",
            "Epoch 42/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6851 - loss: 0.8795 - val_accuracy: 0.5000 - val_loss: 1.4826 - learning_rate: 0.0050\n",
            "Epoch 43/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6515 - loss: 0.9438 - val_accuracy: 0.4667 - val_loss: 2.5095 - learning_rate: 0.0050\n",
            "Epoch 44/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6886 - loss: 0.8200 - val_accuracy: 0.4667 - val_loss: 1.7597 - learning_rate: 0.0050\n",
            "Epoch 45/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7057 - loss: 0.7684 - val_accuracy: 0.5500 - val_loss: 2.5101 - learning_rate: 0.0050\n",
            "Epoch 46/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7050 - loss: 0.8777 - val_accuracy: 0.5500 - val_loss: 1.1765 - learning_rate: 0.0050\n",
            "Epoch 47/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7176 - loss: 0.8285 - val_accuracy: 0.6333 - val_loss: 1.0117 - learning_rate: 0.0050\n",
            "Epoch 48/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7585 - loss: 0.7425 - val_accuracy: 0.6500 - val_loss: 1.1040 - learning_rate: 0.0025\n",
            "Epoch 49/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7221 - loss: 0.8056 - val_accuracy: 0.6000 - val_loss: 1.1255 - learning_rate: 0.0025\n",
            "Epoch 50/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7887 - loss: 0.7267 - val_accuracy: 0.6000 - val_loss: 1.1751 - learning_rate: 0.0025\n",
            "Epoch 51/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7495 - loss: 0.7232 - val_accuracy: 0.6500 - val_loss: 1.0299 - learning_rate: 0.0025\n",
            "Epoch 52/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7726 - loss: 0.6595 - val_accuracy: 0.6333 - val_loss: 0.9998 - learning_rate: 0.0012\n",
            "Epoch 53/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7779 - loss: 0.6230 - val_accuracy: 0.6000 - val_loss: 1.0658 - learning_rate: 0.0012\n",
            "Epoch 54/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.7985 - loss: 0.6505 - val_accuracy: 0.6333 - val_loss: 1.0673 - learning_rate: 0.0012\n",
            "Epoch 55/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8064 - loss: 0.6870 - val_accuracy: 0.6333 - val_loss: 0.9888 - learning_rate: 0.0012\n",
            "Epoch 56/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7730 - loss: 0.6351 - val_accuracy: 0.5667 - val_loss: 1.2047 - learning_rate: 0.0012\n",
            "Epoch 57/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7995 - loss: 0.6632 - val_accuracy: 0.6500 - val_loss: 1.0893 - learning_rate: 0.0012\n",
            "Epoch 58/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 32ms/step - accuracy: 0.7923 - loss: 0.6383 - val_accuracy: 0.6667 - val_loss: 0.9929 - learning_rate: 6.2500e-04\n",
            "Epoch 59/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8334 - loss: 0.5855 - val_accuracy: 0.6667 - val_loss: 1.0587 - learning_rate: 6.2500e-04\n",
            "Epoch 60/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8173 - loss: 0.5845 - val_accuracy: 0.6500 - val_loss: 1.0379 - learning_rate: 6.2500e-04\n",
            "Epoch 61/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8707 - loss: 0.5277 - val_accuracy: 0.6333 - val_loss: 1.0626 - learning_rate: 6.2500e-04\n",
            "Epoch 62/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8010 - loss: 0.6331 - val_accuracy: 0.6167 - val_loss: 1.0613 - learning_rate: 6.2500e-04\n",
            "Epoch 63/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7909 - loss: 0.6134 - val_accuracy: 0.7000 - val_loss: 0.9488 - learning_rate: 6.2500e-04\n",
            "Epoch 64/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8157 - loss: 0.5468 - val_accuracy: 0.7167 - val_loss: 0.9327 - learning_rate: 6.2500e-04\n",
            "Epoch 65/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7960 - loss: 0.6236 - val_accuracy: 0.6000 - val_loss: 1.0457 - learning_rate: 6.2500e-04\n",
            "Epoch 66/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8000 - loss: 0.6613 - val_accuracy: 0.6667 - val_loss: 1.0277 - learning_rate: 6.2500e-04\n",
            "Epoch 67/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8191 - loss: 0.6310 - val_accuracy: 0.6167 - val_loss: 1.0864 - learning_rate: 3.1250e-04\n",
            "Epoch 68/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8234 - loss: 0.5784 - val_accuracy: 0.6167 - val_loss: 1.0931 - learning_rate: 3.1250e-04\n",
            "Epoch 69/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8409 - loss: 0.5477 - val_accuracy: 0.6000 - val_loss: 1.0639 - learning_rate: 3.1250e-04\n",
            "Epoch 70/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8137 - loss: 0.5715 - val_accuracy: 0.6167 - val_loss: 1.0188 - learning_rate: 3.1250e-04\n",
            "Epoch 71/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8316 - loss: 0.5432 - val_accuracy: 0.6000 - val_loss: 1.0208 - learning_rate: 3.1250e-04\n",
            "Epoch 72/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8335 - loss: 0.6119 - val_accuracy: 0.6000 - val_loss: 1.0126 - learning_rate: 1.5625e-04\n",
            "Epoch 73/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8585 - loss: 0.5128 - val_accuracy: 0.6167 - val_loss: 0.9739 - learning_rate: 1.5625e-04\n",
            "Epoch 74/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8446 - loss: 0.5662 - val_accuracy: 0.6000 - val_loss: 1.0149 - learning_rate: 1.5625e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8109 - loss: 0.6065 - val_accuracy: 0.6167 - val_loss: 1.0102 - learning_rate: 1.5625e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8124 - loss: 0.5905 - val_accuracy: 0.6000 - val_loss: 1.0265 - learning_rate: 1.5625e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8120 - loss: 0.5804 - val_accuracy: 0.6500 - val_loss: 0.9929 - learning_rate: 1.5625e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8246 - loss: 0.5847 - val_accuracy: 0.6000 - val_loss: 1.0235 - learning_rate: 7.8125e-05\n",
            "Epoch 79/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8086 - loss: 0.5697 - val_accuracy: 0.6167 - val_loss: 1.0386 - learning_rate: 7.8125e-05\n",
            "Epoch 80/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8022 - loss: 0.5846 - val_accuracy: 0.5833 - val_loss: 1.0130 - learning_rate: 7.8125e-05\n",
            "Epoch 81/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8422 - loss: 0.5464 - val_accuracy: 0.6667 - val_loss: 0.9921 - learning_rate: 3.9062e-05\n",
            "Epoch 82/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8293 - loss: 0.5646 - val_accuracy: 0.6333 - val_loss: 0.9997 - learning_rate: 3.9062e-05\n",
            "Epoch 83/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7904 - loss: 0.6006 - val_accuracy: 0.6500 - val_loss: 1.0161 - learning_rate: 3.9062e-05\n",
            "Epoch 84/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8456 - loss: 0.5325 - val_accuracy: 0.6000 - val_loss: 1.0652 - learning_rate: 1.9531e-05\n",
            "\n",
            "\n",
            "2 fold train loss 0.7030 train acc 0.7929, val loss 0.9327 val acc 0.7167, test loss 1.0293 test acc 0.6667\n",
            "\n",
            "\n",
            "Start 3 fold training\n",
            "Epoch 1/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 43ms/step - accuracy: 0.1865 - loss: 2.2906 - val_accuracy: 0.3000 - val_loss: 2.8869 - learning_rate: 0.0100\n",
            "Epoch 2/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.2468 - loss: 2.0971 - val_accuracy: 0.0667 - val_loss: 5.4701 - learning_rate: 0.0100\n",
            "Epoch 3/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2484 - loss: 2.0333 - val_accuracy: 0.3167 - val_loss: 1.8963 - learning_rate: 0.0100\n",
            "Epoch 4/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.2845 - loss: 2.0365 - val_accuracy: 0.3167 - val_loss: 1.8647 - learning_rate: 0.0100\n",
            "Epoch 5/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2739 - loss: 1.9046 - val_accuracy: 0.4167 - val_loss: 1.9641 - learning_rate: 0.0100\n",
            "Epoch 6/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.2760 - loss: 1.8659 - val_accuracy: 0.3500 - val_loss: 1.7868 - learning_rate: 0.0100\n",
            "Epoch 7/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4152 - loss: 1.7220 - val_accuracy: 0.4000 - val_loss: 1.6876 - learning_rate: 0.0100\n",
            "Epoch 8/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3999 - loss: 1.6667 - val_accuracy: 0.3500 - val_loss: 1.6646 - learning_rate: 0.0100\n",
            "Epoch 9/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3364 - loss: 1.7931 - val_accuracy: 0.3500 - val_loss: 1.8918 - learning_rate: 0.0100\n",
            "Epoch 10/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3654 - loss: 1.7157 - val_accuracy: 0.3167 - val_loss: 1.9508 - learning_rate: 0.0100\n",
            "Epoch 11/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4291 - loss: 1.5978 - val_accuracy: 0.2833 - val_loss: 2.8356 - learning_rate: 0.0100\n",
            "Epoch 12/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3626 - loss: 1.6975 - val_accuracy: 0.2500 - val_loss: 3.0636 - learning_rate: 0.0100\n",
            "Epoch 13/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3940 - loss: 1.6132 - val_accuracy: 0.2333 - val_loss: 2.1375 - learning_rate: 0.0100\n",
            "Epoch 14/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4596 - loss: 1.5706 - val_accuracy: 0.4167 - val_loss: 3.0844 - learning_rate: 0.0100\n",
            "Epoch 15/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4661 - loss: 1.4748 - val_accuracy: 0.2333 - val_loss: 3.5351 - learning_rate: 0.0100\n",
            "Epoch 16/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4416 - loss: 1.5160 - val_accuracy: 0.3167 - val_loss: 2.8311 - learning_rate: 0.0100\n",
            "Epoch 17/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4734 - loss: 1.4628 - val_accuracy: 0.4667 - val_loss: 1.4926 - learning_rate: 0.0100\n",
            "Epoch 18/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4686 - loss: 1.4438 - val_accuracy: 0.3833 - val_loss: 3.9182 - learning_rate: 0.0100\n",
            "Epoch 19/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5004 - loss: 1.4197 - val_accuracy: 0.2667 - val_loss: 1.9598 - learning_rate: 0.0100\n",
            "Epoch 20/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4361 - loss: 1.4421 - val_accuracy: 0.4500 - val_loss: 1.5025 - learning_rate: 0.0100\n",
            "Epoch 21/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5502 - loss: 1.3641 - val_accuracy: 0.4667 - val_loss: 1.7482 - learning_rate: 0.0100\n",
            "Epoch 22/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5453 - loss: 1.2971 - val_accuracy: 0.4167 - val_loss: 2.4332 - learning_rate: 0.0100\n",
            "Epoch 23/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5391 - loss: 1.2735 - val_accuracy: 0.3667 - val_loss: 2.7317 - learning_rate: 0.0100\n",
            "Epoch 24/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5448 - loss: 1.2249 - val_accuracy: 0.4833 - val_loss: 1.7352 - learning_rate: 0.0100\n",
            "Epoch 25/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4974 - loss: 1.3201 - val_accuracy: 0.5167 - val_loss: 1.4174 - learning_rate: 0.0100\n",
            "Epoch 26/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5785 - loss: 1.1941 - val_accuracy: 0.4667 - val_loss: 2.1218 - learning_rate: 0.0100\n",
            "Epoch 27/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5295 - loss: 1.2057 - val_accuracy: 0.4167 - val_loss: 2.0867 - learning_rate: 0.0100\n",
            "Epoch 28/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6118 - loss: 1.1084 - val_accuracy: 0.5833 - val_loss: 1.4394 - learning_rate: 0.0050\n",
            "Epoch 29/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6143 - loss: 1.0365 - val_accuracy: 0.5667 - val_loss: 1.7334 - learning_rate: 0.0050\n",
            "Epoch 30/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6208 - loss: 1.0094 - val_accuracy: 0.4833 - val_loss: 1.9013 - learning_rate: 0.0050\n",
            "Epoch 31/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6616 - loss: 1.0369 - val_accuracy: 0.5667 - val_loss: 1.5168 - learning_rate: 0.0050\n",
            "Epoch 32/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6545 - loss: 0.9898 - val_accuracy: 0.3833 - val_loss: 2.6764 - learning_rate: 0.0050\n",
            "Epoch 33/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6272 - loss: 1.0088 - val_accuracy: 0.6000 - val_loss: 1.6636 - learning_rate: 0.0050\n",
            "Epoch 34/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6912 - loss: 0.9169 - val_accuracy: 0.5333 - val_loss: 1.9241 - learning_rate: 0.0050\n",
            "Epoch 35/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6270 - loss: 1.0368 - val_accuracy: 0.5833 - val_loss: 1.9406 - learning_rate: 0.0050\n",
            "Epoch 36/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7243 - loss: 0.8640 - val_accuracy: 0.5667 - val_loss: 1.8811 - learning_rate: 0.0050\n",
            "Epoch 37/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7151 - loss: 0.8160 - val_accuracy: 0.5500 - val_loss: 1.9657 - learning_rate: 0.0050\n",
            "Epoch 38/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6551 - loss: 0.9086 - val_accuracy: 0.5500 - val_loss: 2.1284 - learning_rate: 0.0050\n",
            "Epoch 39/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7177 - loss: 0.8935 - val_accuracy: 0.5333 - val_loss: 2.1856 - learning_rate: 0.0050\n",
            "Epoch 40/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7112 - loss: 0.8202 - val_accuracy: 0.5833 - val_loss: 1.7691 - learning_rate: 0.0050\n",
            "Epoch 41/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7096 - loss: 0.8297 - val_accuracy: 0.5833 - val_loss: 1.2665 - learning_rate: 0.0050\n",
            "Epoch 42/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6380 - loss: 0.9491 - val_accuracy: 0.6500 - val_loss: 1.4779 - learning_rate: 0.0050\n",
            "Epoch 43/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7394 - loss: 0.7961 - val_accuracy: 0.4167 - val_loss: 5.9881 - learning_rate: 0.0050\n",
            "Epoch 44/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7089 - loss: 0.8378 - val_accuracy: 0.5500 - val_loss: 1.6953 - learning_rate: 0.0050\n",
            "Epoch 45/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7569 - loss: 0.8127 - val_accuracy: 0.5167 - val_loss: 1.9628 - learning_rate: 0.0050\n",
            "Epoch 46/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7202 - loss: 0.7940 - val_accuracy: 0.5833 - val_loss: 1.8218 - learning_rate: 0.0050\n",
            "Epoch 47/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7325 - loss: 0.7643 - val_accuracy: 0.6500 - val_loss: 1.3110 - learning_rate: 0.0050\n",
            "Epoch 48/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7550 - loss: 0.6844 - val_accuracy: 0.6500 - val_loss: 1.3487 - learning_rate: 0.0050\n",
            "Epoch 49/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7247 - loss: 0.7936 - val_accuracy: 0.5500 - val_loss: 1.4843 - learning_rate: 0.0050\n",
            "Epoch 50/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7384 - loss: 0.7404 - val_accuracy: 0.5667 - val_loss: 1.4247 - learning_rate: 0.0050\n",
            "Epoch 51/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7240 - loss: 0.7801 - val_accuracy: 0.3833 - val_loss: 2.6468 - learning_rate: 0.0050\n",
            "Epoch 52/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8011 - loss: 0.6215 - val_accuracy: 0.6167 - val_loss: 1.2022 - learning_rate: 0.0050\n",
            "Epoch 53/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6973 - loss: 0.7951 - val_accuracy: 0.3500 - val_loss: 2.9382 - learning_rate: 0.0050\n",
            "Epoch 54/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7554 - loss: 0.6743 - val_accuracy: 0.5167 - val_loss: 2.4432 - learning_rate: 0.0050\n",
            "Epoch 55/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8109 - loss: 0.5853 - val_accuracy: 0.5500 - val_loss: 1.5577 - learning_rate: 0.0050\n",
            "Epoch 56/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8238 - loss: 0.6236 - val_accuracy: 0.6333 - val_loss: 1.4209 - learning_rate: 0.0050\n",
            "Epoch 57/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7308 - loss: 0.7171 - val_accuracy: 0.6667 - val_loss: 1.9933 - learning_rate: 0.0050\n",
            "Epoch 58/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7616 - loss: 0.6620 - val_accuracy: 0.4833 - val_loss: 2.4247 - learning_rate: 0.0050\n",
            "Epoch 59/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7670 - loss: 0.6761 - val_accuracy: 0.6333 - val_loss: 1.1485 - learning_rate: 0.0025\n",
            "Epoch 60/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8325 - loss: 0.5155 - val_accuracy: 0.6000 - val_loss: 1.7121 - learning_rate: 0.0025\n",
            "Epoch 61/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8144 - loss: 0.5372 - val_accuracy: 0.6667 - val_loss: 1.4359 - learning_rate: 0.0025\n",
            "Epoch 62/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8097 - loss: 0.5686 - val_accuracy: 0.6000 - val_loss: 2.3309 - learning_rate: 0.0025\n",
            "Epoch 63/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8330 - loss: 0.4942 - val_accuracy: 0.6333 - val_loss: 1.3975 - learning_rate: 0.0025\n",
            "Epoch 64/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8490 - loss: 0.5110 - val_accuracy: 0.6500 - val_loss: 1.5030 - learning_rate: 0.0025\n",
            "Epoch 65/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8036 - loss: 0.4929 - val_accuracy: 0.6167 - val_loss: 1.2418 - learning_rate: 0.0025\n",
            "Epoch 66/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8823 - loss: 0.4836 - val_accuracy: 0.6500 - val_loss: 1.4483 - learning_rate: 0.0012\n",
            "Epoch 67/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8089 - loss: 0.5116 - val_accuracy: 0.6333 - val_loss: 1.9168 - learning_rate: 0.0012\n",
            "Epoch 68/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8377 - loss: 0.4589 - val_accuracy: 0.6667 - val_loss: 1.3966 - learning_rate: 0.0012\n",
            "Epoch 69/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8848 - loss: 0.4065 - val_accuracy: 0.6500 - val_loss: 1.4984 - learning_rate: 0.0012\n",
            "Epoch 70/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8506 - loss: 0.4337 - val_accuracy: 0.6167 - val_loss: 1.6395 - learning_rate: 0.0012\n",
            "Epoch 71/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8575 - loss: 0.5130 - val_accuracy: 0.6500 - val_loss: 1.5798 - learning_rate: 0.0012\n",
            "Epoch 72/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8552 - loss: 0.4511 - val_accuracy: 0.5667 - val_loss: 1.8329 - learning_rate: 0.0012\n",
            "Epoch 73/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8055 - loss: 0.5353 - val_accuracy: 0.6500 - val_loss: 1.6615 - learning_rate: 0.0012\n",
            "Epoch 74/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8520 - loss: 0.4919 - val_accuracy: 0.6833 - val_loss: 1.2064 - learning_rate: 0.0012\n",
            "Epoch 75/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8212 - loss: 0.5361 - val_accuracy: 0.6833 - val_loss: 1.3376 - learning_rate: 6.2500e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.9034 - loss: 0.3968 - val_accuracy: 0.7000 - val_loss: 1.4134 - learning_rate: 6.2500e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8717 - loss: 0.4145 - val_accuracy: 0.6500 - val_loss: 1.6149 - learning_rate: 6.2500e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8720 - loss: 0.4423 - val_accuracy: 0.6333 - val_loss: 1.5251 - learning_rate: 6.2500e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9002 - loss: 0.3657 - val_accuracy: 0.6833 - val_loss: 1.6019 - learning_rate: 6.2500e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8813 - loss: 0.4102 - val_accuracy: 0.6333 - val_loss: 1.6027 - learning_rate: 3.1250e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8701 - loss: 0.4093 - val_accuracy: 0.6667 - val_loss: 1.4576 - learning_rate: 3.1250e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8462 - loss: 0.4200 - val_accuracy: 0.6333 - val_loss: 1.4667 - learning_rate: 3.1250e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8562 - loss: 0.3911 - val_accuracy: 0.6833 - val_loss: 1.5232 - learning_rate: 3.1250e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8514 - loss: 0.3960 - val_accuracy: 0.6500 - val_loss: 1.4921 - learning_rate: 3.1250e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8695 - loss: 0.3930 - val_accuracy: 0.6833 - val_loss: 1.5601 - learning_rate: 3.1250e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.8859 - loss: 0.4109 - val_accuracy: 0.6500 - val_loss: 1.6183 - learning_rate: 3.1250e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8910 - loss: 0.3727 - val_accuracy: 0.6333 - val_loss: 1.5823 - learning_rate: 3.1250e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8697 - loss: 0.4064 - val_accuracy: 0.6500 - val_loss: 1.5845 - learning_rate: 1.5625e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9128 - loss: 0.3633 - val_accuracy: 0.6667 - val_loss: 1.5975 - learning_rate: 1.5625e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8754 - loss: 0.4194 - val_accuracy: 0.6500 - val_loss: 1.6099 - learning_rate: 1.5625e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9037 - loss: 0.3858 - val_accuracy: 0.6333 - val_loss: 1.6039 - learning_rate: 1.5625e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9029 - loss: 0.3556 - val_accuracy: 0.6167 - val_loss: 1.5188 - learning_rate: 1.5625e-04\n",
            "Epoch 93/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9112 - loss: 0.3735 - val_accuracy: 0.6500 - val_loss: 1.5630 - learning_rate: 7.8125e-05\n",
            "Epoch 94/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8916 - loss: 0.3958 - val_accuracy: 0.6333 - val_loss: 1.5473 - learning_rate: 7.8125e-05\n",
            "Epoch 95/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8580 - loss: 0.3886 - val_accuracy: 0.6500 - val_loss: 1.5177 - learning_rate: 7.8125e-05\n",
            "Epoch 96/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8921 - loss: 0.3570 - val_accuracy: 0.6500 - val_loss: 1.4853 - learning_rate: 7.8125e-05\n",
            "Epoch 97/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8963 - loss: 0.3894 - val_accuracy: 0.6667 - val_loss: 1.5008 - learning_rate: 3.9062e-05\n",
            "Epoch 98/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9041 - loss: 0.3399 - val_accuracy: 0.6667 - val_loss: 1.4828 - learning_rate: 3.9062e-05\n",
            "Epoch 99/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8565 - loss: 0.4017 - val_accuracy: 0.6500 - val_loss: 1.5049 - learning_rate: 3.9062e-05\n",
            "Epoch 100/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9272 - loss: 0.3449 - val_accuracy: 0.6500 - val_loss: 1.4628 - learning_rate: 1.9531e-05\n",
            "\n",
            "\n",
            "3 fold train loss 0.5118 train acc 0.8429, val loss 1.4134 val acc 0.7000, test loss 0.9179 test acc 0.6667\n",
            "\n",
            "\n",
            "Start 4 fold training\n",
            "Epoch 1/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 43ms/step - accuracy: 0.1414 - loss: 2.3129 - val_accuracy: 0.2167 - val_loss: 3.3652 - learning_rate: 0.0100\n",
            "Epoch 2/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2209 - loss: 2.0917 - val_accuracy: 0.3167 - val_loss: 1.7007 - learning_rate: 0.0100\n",
            "Epoch 3/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2879 - loss: 1.9812 - val_accuracy: 0.3667 - val_loss: 1.7609 - learning_rate: 0.0100\n",
            "Epoch 4/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3175 - loss: 1.9384 - val_accuracy: 0.3333 - val_loss: 1.9515 - learning_rate: 0.0100\n",
            "Epoch 5/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3031 - loss: 1.9258 - val_accuracy: 0.3333 - val_loss: 2.2458 - learning_rate: 0.0100\n",
            "Epoch 6/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3139 - loss: 1.7937 - val_accuracy: 0.1667 - val_loss: 11.0757 - learning_rate: 0.0100\n",
            "Epoch 7/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.3225 - loss: 1.8484 - val_accuracy: 0.4167 - val_loss: 1.6958 - learning_rate: 0.0100\n",
            "Epoch 8/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.3595 - loss: 1.7003 - val_accuracy: 0.5000 - val_loss: 1.4275 - learning_rate: 0.0100\n",
            "Epoch 9/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4458 - loss: 1.5407 - val_accuracy: 0.4333 - val_loss: 1.6049 - learning_rate: 0.0100\n",
            "Epoch 10/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4166 - loss: 1.6662 - val_accuracy: 0.4833 - val_loss: 1.7695 - learning_rate: 0.0100\n",
            "Epoch 11/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3557 - loss: 1.7657 - val_accuracy: 0.4000 - val_loss: 2.0083 - learning_rate: 0.0100\n",
            "Epoch 12/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4474 - loss: 1.4712 - val_accuracy: 0.4833 - val_loss: 1.4559 - learning_rate: 0.0100\n",
            "Epoch 13/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3633 - loss: 1.6067 - val_accuracy: 0.3833 - val_loss: 2.2210 - learning_rate: 0.0100\n",
            "Epoch 14/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4367 - loss: 1.5327 - val_accuracy: 0.3167 - val_loss: 1.7334 - learning_rate: 0.0100\n",
            "Epoch 15/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4839 - loss: 1.4621 - val_accuracy: 0.5000 - val_loss: 1.9915 - learning_rate: 0.0100\n",
            "Epoch 16/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5144 - loss: 1.3754 - val_accuracy: 0.2667 - val_loss: 2.0464 - learning_rate: 0.0100\n",
            "Epoch 17/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4623 - loss: 1.4452 - val_accuracy: 0.4500 - val_loss: 1.4333 - learning_rate: 0.0100\n",
            "Epoch 18/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4658 - loss: 1.3851 - val_accuracy: 0.4667 - val_loss: 1.2651 - learning_rate: 0.0100\n",
            "Epoch 19/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4842 - loss: 1.3162 - val_accuracy: 0.6000 - val_loss: 1.2524 - learning_rate: 0.0100\n",
            "Epoch 20/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5312 - loss: 1.2420 - val_accuracy: 0.4500 - val_loss: 2.4704 - learning_rate: 0.0100\n",
            "Epoch 21/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5940 - loss: 1.1931 - val_accuracy: 0.5000 - val_loss: 1.5064 - learning_rate: 0.0100\n",
            "Epoch 22/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5387 - loss: 1.3031 - val_accuracy: 0.4000 - val_loss: 2.2222 - learning_rate: 0.0100\n",
            "Epoch 23/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5610 - loss: 1.2121 - val_accuracy: 0.5000 - val_loss: 1.3506 - learning_rate: 0.0100\n",
            "Epoch 24/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5287 - loss: 1.2427 - val_accuracy: 0.3500 - val_loss: 3.2325 - learning_rate: 0.0100\n",
            "Epoch 25/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5355 - loss: 1.2748 - val_accuracy: 0.3500 - val_loss: 1.5547 - learning_rate: 0.0100\n",
            "Epoch 26/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5550 - loss: 1.2246 - val_accuracy: 0.3333 - val_loss: 5.8200 - learning_rate: 0.0100\n",
            "Epoch 27/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5654 - loss: 1.2117 - val_accuracy: 0.4500 - val_loss: 1.6232 - learning_rate: 0.0100\n",
            "Epoch 28/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5217 - loss: 1.2025 - val_accuracy: 0.4500 - val_loss: 1.8922 - learning_rate: 0.0100\n",
            "Epoch 29/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6119 - loss: 1.0582 - val_accuracy: 0.4000 - val_loss: 4.1926 - learning_rate: 0.0100\n",
            "Epoch 30/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5982 - loss: 1.0749 - val_accuracy: 0.3667 - val_loss: 2.3440 - learning_rate: 0.0100\n",
            "Epoch 31/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5809 - loss: 1.1188 - val_accuracy: 0.6333 - val_loss: 1.1612 - learning_rate: 0.0100\n",
            "Epoch 32/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6472 - loss: 0.9516 - val_accuracy: 0.4833 - val_loss: 2.1330 - learning_rate: 0.0100\n",
            "Epoch 33/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6186 - loss: 1.1207 - val_accuracy: 0.4833 - val_loss: 2.6798 - learning_rate: 0.0100\n",
            "Epoch 34/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7016 - loss: 0.9069 - val_accuracy: 0.4833 - val_loss: 2.0158 - learning_rate: 0.0100\n",
            "Epoch 35/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6821 - loss: 0.9014 - val_accuracy: 0.4667 - val_loss: 3.4635 - learning_rate: 0.0100\n",
            "Epoch 36/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6567 - loss: 0.9765 - val_accuracy: 0.3833 - val_loss: 4.6060 - learning_rate: 0.0100\n",
            "Epoch 37/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6121 - loss: 0.9592 - val_accuracy: 0.4167 - val_loss: 1.7874 - learning_rate: 0.0100\n",
            "Epoch 38/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6748 - loss: 0.9251 - val_accuracy: 0.3000 - val_loss: 2.8546 - learning_rate: 0.0100\n",
            "Epoch 39/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6523 - loss: 0.9663 - val_accuracy: 0.6167 - val_loss: 1.1428 - learning_rate: 0.0100\n",
            "Epoch 40/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7018 - loss: 0.9146 - val_accuracy: 0.6333 - val_loss: 2.0047 - learning_rate: 0.0100\n",
            "Epoch 41/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6715 - loss: 0.9186 - val_accuracy: 0.5167 - val_loss: 1.5901 - learning_rate: 0.0100\n",
            "Epoch 42/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6938 - loss: 0.8899 - val_accuracy: 0.4667 - val_loss: 1.7430 - learning_rate: 0.0100\n",
            "Epoch 43/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6929 - loss: 0.8228 - val_accuracy: 0.5000 - val_loss: 1.8501 - learning_rate: 0.0100\n",
            "Epoch 44/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7465 - loss: 0.7667 - val_accuracy: 0.5833 - val_loss: 1.4346 - learning_rate: 0.0100\n",
            "Epoch 45/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7574 - loss: 0.7577 - val_accuracy: 0.3667 - val_loss: 4.0384 - learning_rate: 0.0100\n",
            "Epoch 46/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6864 - loss: 0.8467 - val_accuracy: 0.6000 - val_loss: 1.6028 - learning_rate: 0.0100\n",
            "Epoch 47/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6686 - loss: 0.8449 - val_accuracy: 0.4667 - val_loss: 2.0331 - learning_rate: 0.0100\n",
            "Epoch 48/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7268 - loss: 0.7393 - val_accuracy: 0.6667 - val_loss: 1.3068 - learning_rate: 0.0050\n",
            "Epoch 49/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7923 - loss: 0.6219 - val_accuracy: 0.5833 - val_loss: 1.7046 - learning_rate: 0.0050\n",
            "Epoch 50/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7747 - loss: 0.5908 - val_accuracy: 0.6167 - val_loss: 1.3667 - learning_rate: 0.0050\n",
            "Epoch 51/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7543 - loss: 0.7127 - val_accuracy: 0.6000 - val_loss: 1.3988 - learning_rate: 0.0050\n",
            "Epoch 52/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7914 - loss: 0.6113 - val_accuracy: 0.5500 - val_loss: 1.6277 - learning_rate: 0.0050\n",
            "Epoch 53/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7646 - loss: 0.6985 - val_accuracy: 0.6333 - val_loss: 1.0775 - learning_rate: 0.0050\n",
            "Epoch 54/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7719 - loss: 0.6384 - val_accuracy: 0.6000 - val_loss: 1.1532 - learning_rate: 0.0050\n",
            "Epoch 55/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8000 - loss: 0.6149 - val_accuracy: 0.6500 - val_loss: 1.1737 - learning_rate: 0.0050\n",
            "Epoch 56/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7749 - loss: 0.5876 - val_accuracy: 0.6167 - val_loss: 1.6502 - learning_rate: 0.0050\n",
            "Epoch 57/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7767 - loss: 0.5813 - val_accuracy: 0.6000 - val_loss: 1.2532 - learning_rate: 0.0050\n",
            "Epoch 58/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8051 - loss: 0.5284 - val_accuracy: 0.5667 - val_loss: 1.3692 - learning_rate: 0.0050\n",
            "Epoch 59/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8100 - loss: 0.5601 - val_accuracy: 0.6667 - val_loss: 1.1585 - learning_rate: 0.0050\n",
            "Epoch 60/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8327 - loss: 0.5330 - val_accuracy: 0.6833 - val_loss: 0.9579 - learning_rate: 0.0050\n",
            "Epoch 61/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8223 - loss: 0.5206 - val_accuracy: 0.6667 - val_loss: 1.5955 - learning_rate: 0.0050\n",
            "Epoch 62/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8122 - loss: 0.5183 - val_accuracy: 0.5667 - val_loss: 1.5655 - learning_rate: 0.0050\n",
            "Epoch 63/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8373 - loss: 0.4933 - val_accuracy: 0.6833 - val_loss: 1.3248 - learning_rate: 0.0050\n",
            "Epoch 64/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8106 - loss: 0.5207 - val_accuracy: 0.6167 - val_loss: 1.3182 - learning_rate: 0.0025\n",
            "Epoch 65/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8192 - loss: 0.5225 - val_accuracy: 0.6000 - val_loss: 1.5267 - learning_rate: 0.0025\n",
            "Epoch 66/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8530 - loss: 0.4365 - val_accuracy: 0.6667 - val_loss: 1.0598 - learning_rate: 0.0025\n",
            "Epoch 67/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8731 - loss: 0.4522 - val_accuracy: 0.6333 - val_loss: 1.3470 - learning_rate: 0.0025\n",
            "Epoch 68/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8569 - loss: 0.4168 - val_accuracy: 0.6333 - val_loss: 1.3190 - learning_rate: 0.0025\n",
            "Epoch 69/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8838 - loss: 0.3881 - val_accuracy: 0.6500 - val_loss: 1.2120 - learning_rate: 0.0025\n",
            "Epoch 70/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8989 - loss: 0.3708 - val_accuracy: 0.7000 - val_loss: 1.1072 - learning_rate: 0.0025\n",
            "Epoch 71/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8594 - loss: 0.4190 - val_accuracy: 0.6833 - val_loss: 0.9019 - learning_rate: 0.0012\n",
            "Epoch 72/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9254 - loss: 0.3426 - val_accuracy: 0.6000 - val_loss: 1.1250 - learning_rate: 0.0012\n",
            "Epoch 73/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8906 - loss: 0.3800 - val_accuracy: 0.6667 - val_loss: 0.9541 - learning_rate: 0.0012\n",
            "Epoch 74/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8725 - loss: 0.3637 - val_accuracy: 0.6333 - val_loss: 1.1409 - learning_rate: 0.0012\n",
            "Epoch 75/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8692 - loss: 0.3890 - val_accuracy: 0.7000 - val_loss: 0.8921 - learning_rate: 0.0012\n",
            "Epoch 76/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9081 - loss: 0.3481 - val_accuracy: 0.6833 - val_loss: 0.9161 - learning_rate: 0.0012\n",
            "Epoch 77/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8840 - loss: 0.3533 - val_accuracy: 0.7000 - val_loss: 1.0161 - learning_rate: 0.0012\n",
            "Epoch 78/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8703 - loss: 0.3782 - val_accuracy: 0.6500 - val_loss: 0.9636 - learning_rate: 0.0012\n",
            "Epoch 79/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8762 - loss: 0.3551 - val_accuracy: 0.7000 - val_loss: 0.9739 - learning_rate: 0.0012\n",
            "Epoch 80/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8837 - loss: 0.3457 - val_accuracy: 0.7000 - val_loss: 0.8978 - learning_rate: 0.0012\n",
            "Epoch 81/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8989 - loss: 0.3440 - val_accuracy: 0.6667 - val_loss: 0.8611 - learning_rate: 6.2500e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9060 - loss: 0.3208 - val_accuracy: 0.6667 - val_loss: 0.8807 - learning_rate: 6.2500e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9149 - loss: 0.2986 - val_accuracy: 0.7000 - val_loss: 0.8900 - learning_rate: 6.2500e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9109 - loss: 0.3391 - val_accuracy: 0.6833 - val_loss: 0.9214 - learning_rate: 6.2500e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.8812 - loss: 0.3566 - val_accuracy: 0.7167 - val_loss: 0.9086 - learning_rate: 6.2500e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8846 - loss: 0.3635 - val_accuracy: 0.7167 - val_loss: 0.9400 - learning_rate: 6.2500e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9083 - loss: 0.3232 - val_accuracy: 0.6833 - val_loss: 0.9433 - learning_rate: 3.1250e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8761 - loss: 0.3995 - val_accuracy: 0.6833 - val_loss: 0.8885 - learning_rate: 3.1250e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8994 - loss: 0.3297 - val_accuracy: 0.6667 - val_loss: 0.8874 - learning_rate: 3.1250e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9015 - loss: 0.3447 - val_accuracy: 0.7000 - val_loss: 0.8915 - learning_rate: 1.5625e-04\n",
            "Epoch 91/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9036 - loss: 0.3380 - val_accuracy: 0.6833 - val_loss: 0.8605 - learning_rate: 1.5625e-04\n",
            "Epoch 92/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9058 - loss: 0.3161 - val_accuracy: 0.6667 - val_loss: 0.8625 - learning_rate: 1.5625e-04\n",
            "Epoch 93/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9004 - loss: 0.3450 - val_accuracy: 0.6667 - val_loss: 0.8864 - learning_rate: 1.5625e-04\n",
            "Epoch 94/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9169 - loss: 0.3068 - val_accuracy: 0.6833 - val_loss: 0.8599 - learning_rate: 1.5625e-04\n",
            "Epoch 95/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8612 - loss: 0.3657 - val_accuracy: 0.6667 - val_loss: 0.8802 - learning_rate: 7.8125e-05\n",
            "Epoch 96/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8847 - loss: 0.3451 - val_accuracy: 0.6833 - val_loss: 0.8883 - learning_rate: 7.8125e-05\n",
            "Epoch 97/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9193 - loss: 0.2951 - val_accuracy: 0.6667 - val_loss: 0.8764 - learning_rate: 7.8125e-05\n",
            "Epoch 98/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9235 - loss: 0.3024 - val_accuracy: 0.6667 - val_loss: 0.9076 - learning_rate: 3.9062e-05\n",
            "Epoch 99/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.9157 - loss: 0.3052 - val_accuracy: 0.6667 - val_loss: 0.9010 - learning_rate: 3.9062e-05\n",
            "Epoch 100/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8990 - loss: 0.3206 - val_accuracy: 0.6500 - val_loss: 0.8941 - learning_rate: 3.9062e-05\n",
            "\n",
            "\n",
            "4 fold train loss 0.4471 train acc 0.8643, val loss 0.9086 val acc 0.7167, test loss 0.9253 test acc 0.7500\n",
            "\n",
            "\n",
            "Start 5 fold training\n",
            "Epoch 1/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 44ms/step - accuracy: 0.1284 - loss: 2.3142 - val_accuracy: 0.0833 - val_loss: 10.3800 - learning_rate: 0.0100\n",
            "Epoch 2/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.1959 - loss: 2.1655 - val_accuracy: 0.2333 - val_loss: 1.9040 - learning_rate: 0.0100\n",
            "Epoch 3/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.1824 - loss: 2.0970 - val_accuracy: 0.2667 - val_loss: 1.9977 - learning_rate: 0.0100\n",
            "Epoch 4/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2920 - loss: 2.0363 - val_accuracy: 0.3667 - val_loss: 1.6598 - learning_rate: 0.0100\n",
            "Epoch 5/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.2997 - loss: 1.9589 - val_accuracy: 0.3333 - val_loss: 1.8243 - learning_rate: 0.0100\n",
            "Epoch 6/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3711 - loss: 1.8184 - val_accuracy: 0.3000 - val_loss: 1.8325 - learning_rate: 0.0100\n",
            "Epoch 7/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.2835 - loss: 1.8917 - val_accuracy: 0.3833 - val_loss: 1.5356 - learning_rate: 0.0100\n",
            "Epoch 8/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3463 - loss: 1.7228 - val_accuracy: 0.2500 - val_loss: 2.0649 - learning_rate: 0.0100\n",
            "Epoch 9/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.3335 - loss: 1.7947 - val_accuracy: 0.4000 - val_loss: 1.9332 - learning_rate: 0.0100\n",
            "Epoch 10/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3305 - loss: 1.7740 - val_accuracy: 0.3667 - val_loss: 1.8545 - learning_rate: 0.0100\n",
            "Epoch 11/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3854 - loss: 1.6524 - val_accuracy: 0.2000 - val_loss: 3.2949 - learning_rate: 0.0100\n",
            "Epoch 12/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4609 - loss: 1.6274 - val_accuracy: 0.3333 - val_loss: 1.5678 - learning_rate: 0.0100\n",
            "Epoch 13/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4548 - loss: 1.5581 - val_accuracy: 0.4333 - val_loss: 1.7747 - learning_rate: 0.0100\n",
            "Epoch 14/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.3779 - loss: 1.6276 - val_accuracy: 0.4167 - val_loss: 5.4759 - learning_rate: 0.0100\n",
            "Epoch 15/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.4025 - loss: 1.5460 - val_accuracy: 0.4333 - val_loss: 1.5486 - learning_rate: 0.0100\n",
            "Epoch 16/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4105 - loss: 1.5310 - val_accuracy: 0.5000 - val_loss: 1.8407 - learning_rate: 0.0100\n",
            "Epoch 17/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5054 - loss: 1.3913 - val_accuracy: 0.4333 - val_loss: 1.5172 - learning_rate: 0.0100\n",
            "Epoch 18/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4826 - loss: 1.3679 - val_accuracy: 0.3667 - val_loss: 2.1461 - learning_rate: 0.0100\n",
            "Epoch 19/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5221 - loss: 1.3471 - val_accuracy: 0.4667 - val_loss: 1.6580 - learning_rate: 0.0100\n",
            "Epoch 20/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5336 - loss: 1.2929 - val_accuracy: 0.5500 - val_loss: 1.4012 - learning_rate: 0.0100\n",
            "Epoch 21/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4860 - loss: 1.3173 - val_accuracy: 0.3667 - val_loss: 4.0669 - learning_rate: 0.0100\n",
            "Epoch 22/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.4961 - loss: 1.2814 - val_accuracy: 0.5833 - val_loss: 1.1606 - learning_rate: 0.0100\n",
            "Epoch 23/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4733 - loss: 1.3746 - val_accuracy: 0.3667 - val_loss: 4.5412 - learning_rate: 0.0100\n",
            "Epoch 24/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.4987 - loss: 1.3161 - val_accuracy: 0.5500 - val_loss: 1.3548 - learning_rate: 0.0100\n",
            "Epoch 25/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5509 - loss: 1.2583 - val_accuracy: 0.4833 - val_loss: 1.3900 - learning_rate: 0.0100\n",
            "Epoch 26/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6124 - loss: 1.1508 - val_accuracy: 0.5333 - val_loss: 1.5371 - learning_rate: 0.0100\n",
            "Epoch 27/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6226 - loss: 1.0930 - val_accuracy: 0.6333 - val_loss: 1.3121 - learning_rate: 0.0100\n",
            "Epoch 28/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.5571 - loss: 1.1873 - val_accuracy: 0.6667 - val_loss: 1.3420 - learning_rate: 0.0100\n",
            "Epoch 29/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.5470 - loss: 1.1993 - val_accuracy: 0.5833 - val_loss: 1.5439 - learning_rate: 0.0100\n",
            "Epoch 30/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.5796 - loss: 1.1337 - val_accuracy: 0.5667 - val_loss: 1.4348 - learning_rate: 0.0100\n",
            "Epoch 31/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6389 - loss: 1.0795 - val_accuracy: 0.6167 - val_loss: 1.7292 - learning_rate: 0.0050\n",
            "Epoch 32/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6336 - loss: 0.9879 - val_accuracy: 0.6000 - val_loss: 1.3238 - learning_rate: 0.0050\n",
            "Epoch 33/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 28ms/step - accuracy: 0.6209 - loss: 1.0173 - val_accuracy: 0.5833 - val_loss: 1.1382 - learning_rate: 0.0050\n",
            "Epoch 34/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6467 - loss: 0.8959 - val_accuracy: 0.6167 - val_loss: 1.3493 - learning_rate: 0.0050\n",
            "Epoch 35/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6725 - loss: 0.9727 - val_accuracy: 0.6167 - val_loss: 1.2589 - learning_rate: 0.0050\n",
            "Epoch 36/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6697 - loss: 0.8978 - val_accuracy: 0.6333 - val_loss: 1.1757 - learning_rate: 0.0050\n",
            "Epoch 37/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7037 - loss: 0.9213 - val_accuracy: 0.6000 - val_loss: 1.4826 - learning_rate: 0.0050\n",
            "Epoch 38/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6696 - loss: 0.8601 - val_accuracy: 0.5667 - val_loss: 1.9608 - learning_rate: 0.0050\n",
            "Epoch 39/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7109 - loss: 0.8493 - val_accuracy: 0.6500 - val_loss: 1.1503 - learning_rate: 0.0050\n",
            "Epoch 40/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6966 - loss: 0.9229 - val_accuracy: 0.6333 - val_loss: 1.2564 - learning_rate: 0.0050\n",
            "Epoch 41/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.6809 - loss: 0.9072 - val_accuracy: 0.7333 - val_loss: 0.9755 - learning_rate: 0.0050\n",
            "Epoch 42/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6713 - loss: 0.8215 - val_accuracy: 0.6667 - val_loss: 1.0748 - learning_rate: 0.0050\n",
            "Epoch 43/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7314 - loss: 0.7784 - val_accuracy: 0.6167 - val_loss: 1.8405 - learning_rate: 0.0050\n",
            "Epoch 44/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6981 - loss: 0.8157 - val_accuracy: 0.6667 - val_loss: 1.3092 - learning_rate: 0.0050\n",
            "Epoch 45/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7405 - loss: 0.8685 - val_accuracy: 0.6333 - val_loss: 1.4178 - learning_rate: 0.0050\n",
            "Epoch 46/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.6823 - loss: 0.8564 - val_accuracy: 0.7000 - val_loss: 1.0139 - learning_rate: 0.0050\n",
            "Epoch 47/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7467 - loss: 0.7551 - val_accuracy: 0.6667 - val_loss: 1.1869 - learning_rate: 0.0025\n",
            "Epoch 48/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7519 - loss: 0.7207 - val_accuracy: 0.6833 - val_loss: 0.9168 - learning_rate: 0.0025\n",
            "Epoch 49/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7697 - loss: 0.6541 - val_accuracy: 0.6833 - val_loss: 1.1358 - learning_rate: 0.0025\n",
            "Epoch 50/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7447 - loss: 0.7226 - val_accuracy: 0.7000 - val_loss: 1.0481 - learning_rate: 0.0025\n",
            "Epoch 51/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7353 - loss: 0.6801 - val_accuracy: 0.7000 - val_loss: 0.8993 - learning_rate: 0.0025\n",
            "Epoch 52/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7617 - loss: 0.6531 - val_accuracy: 0.6833 - val_loss: 0.8949 - learning_rate: 0.0025\n",
            "Epoch 53/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7644 - loss: 0.6427 - val_accuracy: 0.6667 - val_loss: 0.9622 - learning_rate: 0.0025\n",
            "Epoch 54/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7536 - loss: 0.6619 - val_accuracy: 0.7333 - val_loss: 0.9248 - learning_rate: 0.0025\n",
            "Epoch 55/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7545 - loss: 0.6563 - val_accuracy: 0.6833 - val_loss: 0.9545 - learning_rate: 0.0025\n",
            "Epoch 56/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7677 - loss: 0.6719 - val_accuracy: 0.7000 - val_loss: 1.0469 - learning_rate: 0.0025\n",
            "Epoch 57/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7705 - loss: 0.6956 - val_accuracy: 0.7500 - val_loss: 0.9151 - learning_rate: 0.0025\n",
            "Epoch 58/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8077 - loss: 0.6010 - val_accuracy: 0.6833 - val_loss: 0.8767 - learning_rate: 0.0012\n",
            "Epoch 59/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7947 - loss: 0.6045 - val_accuracy: 0.7667 - val_loss: 0.8785 - learning_rate: 0.0012\n",
            "Epoch 60/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8152 - loss: 0.6356 - val_accuracy: 0.7500 - val_loss: 0.9455 - learning_rate: 0.0012\n",
            "Epoch 61/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7776 - loss: 0.6250 - val_accuracy: 0.7000 - val_loss: 1.1420 - learning_rate: 0.0012\n",
            "Epoch 62/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7828 - loss: 0.6075 - val_accuracy: 0.7333 - val_loss: 0.9119 - learning_rate: 0.0012\n",
            "Epoch 63/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7827 - loss: 0.6301 - val_accuracy: 0.7500 - val_loss: 0.8817 - learning_rate: 0.0012\n",
            "Epoch 64/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8215 - loss: 0.5919 - val_accuracy: 0.7000 - val_loss: 0.9824 - learning_rate: 0.0012\n",
            "Epoch 65/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8242 - loss: 0.5569 - val_accuracy: 0.7333 - val_loss: 0.9769 - learning_rate: 0.0012\n",
            "Epoch 66/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7522 - loss: 0.6401 - val_accuracy: 0.7500 - val_loss: 0.8743 - learning_rate: 0.0012\n",
            "Epoch 67/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8222 - loss: 0.5748 - val_accuracy: 0.7167 - val_loss: 0.8863 - learning_rate: 0.0012\n",
            "Epoch 68/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 30ms/step - accuracy: 0.7990 - loss: 0.5902 - val_accuracy: 0.7833 - val_loss: 0.8848 - learning_rate: 0.0012\n",
            "Epoch 69/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8100 - loss: 0.5707 - val_accuracy: 0.6833 - val_loss: 0.9498 - learning_rate: 0.0012\n",
            "Epoch 70/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7850 - loss: 0.6049 - val_accuracy: 0.7500 - val_loss: 0.9472 - learning_rate: 0.0012\n",
            "Epoch 71/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8269 - loss: 0.5065 - val_accuracy: 0.7000 - val_loss: 0.9847 - learning_rate: 0.0012\n",
            "Epoch 72/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8020 - loss: 0.5747 - val_accuracy: 0.7333 - val_loss: 0.9140 - learning_rate: 0.0012\n",
            "Epoch 73/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8410 - loss: 0.4960 - val_accuracy: 0.7333 - val_loss: 0.9124 - learning_rate: 0.0012\n",
            "Epoch 74/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8608 - loss: 0.4517 - val_accuracy: 0.7167 - val_loss: 0.9598 - learning_rate: 6.2500e-04\n",
            "Epoch 75/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8263 - loss: 0.5316 - val_accuracy: 0.6833 - val_loss: 0.9523 - learning_rate: 6.2500e-04\n",
            "Epoch 76/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8493 - loss: 0.5358 - val_accuracy: 0.7333 - val_loss: 0.8605 - learning_rate: 6.2500e-04\n",
            "Epoch 77/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8407 - loss: 0.5124 - val_accuracy: 0.7333 - val_loss: 0.9662 - learning_rate: 6.2500e-04\n",
            "Epoch 78/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8172 - loss: 0.5038 - val_accuracy: 0.7167 - val_loss: 0.9922 - learning_rate: 6.2500e-04\n",
            "Epoch 79/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8606 - loss: 0.4659 - val_accuracy: 0.7667 - val_loss: 0.8817 - learning_rate: 6.2500e-04\n",
            "Epoch 80/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8415 - loss: 0.5264 - val_accuracy: 0.7167 - val_loss: 0.9279 - learning_rate: 6.2500e-04\n",
            "Epoch 81/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8745 - loss: 0.4600 - val_accuracy: 0.7667 - val_loss: 0.8866 - learning_rate: 6.2500e-04\n",
            "Epoch 82/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8078 - loss: 0.5229 - val_accuracy: 0.7500 - val_loss: 0.8866 - learning_rate: 6.2500e-04\n",
            "Epoch 83/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8420 - loss: 0.4654 - val_accuracy: 0.7500 - val_loss: 0.9236 - learning_rate: 3.1250e-04\n",
            "Epoch 84/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8592 - loss: 0.4475 - val_accuracy: 0.7667 - val_loss: 0.9098 - learning_rate: 3.1250e-04\n",
            "Epoch 85/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8527 - loss: 0.4576 - val_accuracy: 0.7333 - val_loss: 0.9074 - learning_rate: 3.1250e-04\n",
            "Epoch 86/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8648 - loss: 0.4767 - val_accuracy: 0.7167 - val_loss: 0.9005 - learning_rate: 3.1250e-04\n",
            "Epoch 87/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8677 - loss: 0.4435 - val_accuracy: 0.7500 - val_loss: 0.8893 - learning_rate: 1.5625e-04\n",
            "Epoch 88/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8379 - loss: 0.4643 - val_accuracy: 0.7333 - val_loss: 0.9423 - learning_rate: 1.5625e-04\n",
            "Epoch 89/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8733 - loss: 0.4551 - val_accuracy: 0.7333 - val_loss: 0.9514 - learning_rate: 1.5625e-04\n",
            "Epoch 90/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8355 - loss: 0.4938 - val_accuracy: 0.7167 - val_loss: 0.9377 - learning_rate: 7.8125e-05\n",
            "Epoch 91/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.7959 - loss: 0.5000 - val_accuracy: 0.7333 - val_loss: 0.9374 - learning_rate: 7.8125e-05\n",
            "Epoch 92/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8407 - loss: 0.5098 - val_accuracy: 0.7167 - val_loss: 0.9204 - learning_rate: 7.8125e-05\n",
            "Epoch 93/100\n",
            "\u001b[1m210/210\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 29ms/step - accuracy: 0.8646 - loss: 0.4887 - val_accuracy: 0.7167 - val_loss: 0.9472 - learning_rate: 3.9062e-05\n",
            "\n",
            "\n",
            "5 fold train loss 0.6112 train acc 0.8071, val loss 0.8848 val acc 0.7833, test loss 1.0684 test acc 0.7000\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5 fold train loss avg 0.5607 train acc avg 0.8276, val loss avg 1.0956 val acc avg 0.7300, test loss avg 1.0012 test acc avg 0.7000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: Calculate mean and standard deviation for 0.7, 0.75, 0.667,0.667,0.7167\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "data = [0.7, 0.75, 0.667, 0.667, 0.7167]\n",
        "\n",
        "mean = np.mean(data)\n",
        "std_dev = np.std(data)\n",
        "\n",
        "print(f\"Mean: {mean}\")\n",
        "print(f\"Standard Deviation: {std_dev}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mOznWmYXKES",
        "outputId": "1a6e2a43-f812-4aeb-fb51-5acc0067dc59"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean: 0.70014\n",
            "Standard Deviation: 0.031485844438413894\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Previous possible duplicate runs"
      ],
      "metadata": {
        "id": "D32ETaBQXFSG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-nsAy-ooCM_B",
        "outputId": "c9db78fa-bc11-4c60-bf07-c839e853fed1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start 1 fold training\n",
            "Epoch 1/100\n",
            "294/294 [==============================] - 18s 27ms/step - loss: 2.2254 - accuracy: 0.1701 - val_loss: 2.6042 - val_accuracy: 0.1918 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 2.0878 - accuracy: 0.2466 - val_loss: 1.7142 - val_accuracy: 0.4247 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 2.0283 - accuracy: 0.2789 - val_loss: 1.6732 - val_accuracy: 0.3699 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.9214 - accuracy: 0.2976 - val_loss: 1.6404 - val_accuracy: 0.3562 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.8724 - accuracy: 0.3129 - val_loss: 1.6418 - val_accuracy: 0.3699 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.7805 - accuracy: 0.3588 - val_loss: 1.7996 - val_accuracy: 0.2466 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.7260 - accuracy: 0.3520 - val_loss: 2.2511 - val_accuracy: 0.3151 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.6337 - accuracy: 0.3912 - val_loss: 1.4185 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.5908 - accuracy: 0.4269 - val_loss: 1.3783 - val_accuracy: 0.4658 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.5598 - accuracy: 0.4320 - val_loss: 1.2816 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4863 - accuracy: 0.4354 - val_loss: 1.5009 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4705 - accuracy: 0.4592 - val_loss: 1.3753 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.5310 - accuracy: 0.4184 - val_loss: 1.9604 - val_accuracy: 0.3288 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4109 - accuracy: 0.4575 - val_loss: 1.4235 - val_accuracy: 0.4795 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3698 - accuracy: 0.4643 - val_loss: 1.7897 - val_accuracy: 0.4110 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3446 - accuracy: 0.4932 - val_loss: 1.3669 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3149 - accuracy: 0.5017 - val_loss: 1.4030 - val_accuracy: 0.5205 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3042 - accuracy: 0.5153 - val_loss: 1.3914 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2246 - accuracy: 0.5476 - val_loss: 1.3381 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2293 - accuracy: 0.5425 - val_loss: 1.9132 - val_accuracy: 0.3562 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.2065 - accuracy: 0.5697 - val_loss: 1.4291 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1666 - accuracy: 0.5714 - val_loss: 1.4458 - val_accuracy: 0.4795 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1490 - accuracy: 0.5731 - val_loss: 1.3277 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0946 - accuracy: 0.6122 - val_loss: 1.4349 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0714 - accuracy: 0.6156 - val_loss: 2.1161 - val_accuracy: 0.3288 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0594 - accuracy: 0.6105 - val_loss: 1.6270 - val_accuracy: 0.4384 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0343 - accuracy: 0.5935 - val_loss: 1.4664 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9996 - accuracy: 0.6361 - val_loss: 1.4266 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.9779 - accuracy: 0.6599 - val_loss: 1.2973 - val_accuracy: 0.6301 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.9491 - accuracy: 0.6616 - val_loss: 1.1508 - val_accuracy: 0.6849 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8945 - accuracy: 0.6633 - val_loss: 1.4259 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9714 - accuracy: 0.6327 - val_loss: 1.5190 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9004 - accuracy: 0.6803 - val_loss: 1.2846 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8881 - accuracy: 0.6684 - val_loss: 1.4861 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8535 - accuracy: 0.6922 - val_loss: 1.5560 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9141 - accuracy: 0.6786 - val_loss: 1.2677 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8347 - accuracy: 0.7075 - val_loss: 1.5319 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8232 - accuracy: 0.7058 - val_loss: 1.5083 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8102 - accuracy: 0.7160 - val_loss: 1.5003 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8168 - accuracy: 0.6922 - val_loss: 1.4447 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7793 - accuracy: 0.7228 - val_loss: 1.1411 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7669 - accuracy: 0.7092 - val_loss: 1.3502 - val_accuracy: 0.6849 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7841 - accuracy: 0.7228 - val_loss: 1.2978 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.7118 - accuracy: 0.7415 - val_loss: 1.1533 - val_accuracy: 0.7123 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7383 - accuracy: 0.7330 - val_loss: 1.5833 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7156 - accuracy: 0.7347 - val_loss: 1.4148 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.6734 - accuracy: 0.7517 - val_loss: 1.3455 - val_accuracy: 0.7260 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7087 - accuracy: 0.7466 - val_loss: 1.2319 - val_accuracy: 0.6849 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6632 - accuracy: 0.7466 - val_loss: 1.5662 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6725 - accuracy: 0.7500 - val_loss: 1.2730 - val_accuracy: 0.5890 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6491 - accuracy: 0.7704 - val_loss: 1.3354 - val_accuracy: 0.7123 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6366 - accuracy: 0.7653 - val_loss: 1.2947 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6465 - accuracy: 0.7687 - val_loss: 1.3421 - val_accuracy: 0.6712 - lr: 0.0100\n",
            "Epoch 54/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 0.5960 - accuracy: 0.7823 - val_loss: 1.5069 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 55/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5994 - accuracy: 0.7874 - val_loss: 1.6290 - val_accuracy: 0.5890 - lr: 0.0100\n",
            "Epoch 56/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6361 - accuracy: 0.7823 - val_loss: 1.4064 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 57/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5155 - accuracy: 0.8061 - val_loss: 1.3527 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 58/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5451 - accuracy: 0.7840 - val_loss: 1.2316 - val_accuracy: 0.6712 - lr: 0.0100\n",
            "Epoch 59/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5749 - accuracy: 0.7823 - val_loss: 1.1825 - val_accuracy: 0.7123 - lr: 0.0100\n",
            "Epoch 60/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5278 - accuracy: 0.8112 - val_loss: 2.2416 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 61/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.4255 - accuracy: 0.8503 - val_loss: 1.0860 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 62/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4278 - accuracy: 0.8452 - val_loss: 0.9994 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 63/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4163 - accuracy: 0.8639 - val_loss: 1.0115 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 64/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.4045 - accuracy: 0.8639 - val_loss: 1.0727 - val_accuracy: 0.8082 - lr: 0.0050\n",
            "Epoch 65/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4297 - accuracy: 0.8554 - val_loss: 1.0816 - val_accuracy: 0.7123 - lr: 0.0050\n",
            "Epoch 66/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3870 - accuracy: 0.8520 - val_loss: 1.0433 - val_accuracy: 0.7397 - lr: 0.0050\n",
            "Epoch 67/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3524 - accuracy: 0.8844 - val_loss: 1.1014 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 68/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3736 - accuracy: 0.8724 - val_loss: 2.0517 - val_accuracy: 0.5616 - lr: 0.0050\n",
            "Epoch 69/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3491 - accuracy: 0.8724 - val_loss: 1.1025 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 70/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3583 - accuracy: 0.8741 - val_loss: 1.0146 - val_accuracy: 0.8082 - lr: 0.0050\n",
            "Epoch 71/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3878 - accuracy: 0.8571 - val_loss: 1.3111 - val_accuracy: 0.6712 - lr: 0.0050\n",
            "Epoch 72/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3559 - accuracy: 0.8673 - val_loss: 1.0280 - val_accuracy: 0.7808 - lr: 0.0050\n",
            "Epoch 73/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3292 - accuracy: 0.8895 - val_loss: 1.1286 - val_accuracy: 0.7808 - lr: 0.0025\n",
            "Epoch 74/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2849 - accuracy: 0.9082 - val_loss: 1.0475 - val_accuracy: 0.7945 - lr: 0.0025\n",
            "Epoch 75/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2897 - accuracy: 0.8878 - val_loss: 1.1479 - val_accuracy: 0.7260 - lr: 0.0025\n",
            "Epoch 76/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2580 - accuracy: 0.9235 - val_loss: 0.9153 - val_accuracy: 0.8082 - lr: 0.0025\n",
            "Epoch 77/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2663 - accuracy: 0.9218 - val_loss: 1.0537 - val_accuracy: 0.8082 - lr: 0.0025\n",
            "Epoch 78/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.2609 - accuracy: 0.9167 - val_loss: 1.0900 - val_accuracy: 0.8356 - lr: 0.0025\n",
            "Epoch 79/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2745 - accuracy: 0.9150 - val_loss: 1.0619 - val_accuracy: 0.7945 - lr: 0.0025\n",
            "Epoch 80/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2554 - accuracy: 0.9099 - val_loss: 0.9719 - val_accuracy: 0.8219 - lr: 0.0012\n",
            "Epoch 81/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2408 - accuracy: 0.9252 - val_loss: 1.0524 - val_accuracy: 0.8082 - lr: 0.0012\n",
            "Epoch 82/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2126 - accuracy: 0.9371 - val_loss: 0.9497 - val_accuracy: 0.8219 - lr: 0.0012\n",
            "Epoch 83/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2215 - accuracy: 0.9371 - val_loss: 0.9396 - val_accuracy: 0.8356 - lr: 0.0012\n",
            "Epoch 84/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2290 - accuracy: 0.9201 - val_loss: 0.9684 - val_accuracy: 0.8082 - lr: 0.0012\n",
            "Epoch 85/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2377 - accuracy: 0.9218 - val_loss: 1.0067 - val_accuracy: 0.8219 - lr: 0.0012\n",
            "Epoch 86/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2353 - accuracy: 0.9167 - val_loss: 0.9869 - val_accuracy: 0.8356 - lr: 6.2500e-04\n",
            "Epoch 87/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2160 - accuracy: 0.9405 - val_loss: 0.9800 - val_accuracy: 0.8356 - lr: 6.2500e-04\n",
            "Epoch 88/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1894 - accuracy: 0.9490 - val_loss: 1.0033 - val_accuracy: 0.8219 - lr: 6.2500e-04\n",
            "Epoch 89/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2150 - accuracy: 0.9388 - val_loss: 1.0004 - val_accuracy: 0.8082 - lr: 6.2500e-04\n",
            "Epoch 90/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2056 - accuracy: 0.9439 - val_loss: 1.0356 - val_accuracy: 0.8219 - lr: 6.2500e-04\n",
            "Epoch 91/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2100 - accuracy: 0.9405 - val_loss: 0.9283 - val_accuracy: 0.8219 - lr: 6.2500e-04\n",
            "Epoch 92/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2039 - accuracy: 0.9337 - val_loss: 0.9527 - val_accuracy: 0.8219 - lr: 3.1250e-04\n",
            "Epoch 93/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1849 - accuracy: 0.9592 - val_loss: 1.0140 - val_accuracy: 0.8219 - lr: 3.1250e-04\n",
            "Epoch 94/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.1732 - accuracy: 0.9592 - val_loss: 1.0129 - val_accuracy: 0.8493 - lr: 3.1250e-04\n",
            "Epoch 95/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1793 - accuracy: 0.9592 - val_loss: 1.0095 - val_accuracy: 0.8356 - lr: 3.1250e-04\n",
            "Epoch 96/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1890 - accuracy: 0.9473 - val_loss: 1.0033 - val_accuracy: 0.8493 - lr: 3.1250e-04\n",
            "Epoch 97/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1917 - accuracy: 0.9507 - val_loss: 0.9877 - val_accuracy: 0.8219 - lr: 3.1250e-04\n",
            "Epoch 98/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1957 - accuracy: 0.9422 - val_loss: 1.0028 - val_accuracy: 0.8356 - lr: 1.5625e-04\n",
            "Epoch 99/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1783 - accuracy: 0.9490 - val_loss: 0.9842 - val_accuracy: 0.8219 - lr: 1.5625e-04\n",
            "Epoch 100/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1914 - accuracy: 0.9371 - val_loss: 0.9969 - val_accuracy: 0.8356 - lr: 1.5625e-04\n",
            "\n",
            "\n",
            "1 fold train loss 0.2686 train acc 0.9337, val loss 1.0129 val acc 0.8493, test loss 0.7008 test acc 0.7973\n",
            "\n",
            "\n",
            "Start 2 fold training\n",
            "Epoch 1/100\n",
            "294/294 [==============================] - 19s 27ms/step - loss: 2.2410 - accuracy: 0.1633 - val_loss: 1.9854 - val_accuracy: 0.2603 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 2.0651 - accuracy: 0.2704 - val_loss: 2.6948 - val_accuracy: 0.2466 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.9251 - accuracy: 0.3214 - val_loss: 2.1792 - val_accuracy: 0.3425 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.9132 - accuracy: 0.2993 - val_loss: 2.2870 - val_accuracy: 0.2740 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.8325 - accuracy: 0.3537 - val_loss: 1.7961 - val_accuracy: 0.3836 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.7775 - accuracy: 0.3418 - val_loss: 2.7262 - val_accuracy: 0.2740 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 1.7639 - accuracy: 0.3537 - val_loss: 2.6546 - val_accuracy: 0.3288 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.7050 - accuracy: 0.3963 - val_loss: 1.4887 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6659 - accuracy: 0.4082 - val_loss: 1.6141 - val_accuracy: 0.4384 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6406 - accuracy: 0.4320 - val_loss: 1.5554 - val_accuracy: 0.3973 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.5445 - accuracy: 0.4507 - val_loss: 1.5759 - val_accuracy: 0.4521 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.4683 - accuracy: 0.4524 - val_loss: 1.4198 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4830 - accuracy: 0.4405 - val_loss: 1.8347 - val_accuracy: 0.3288 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3858 - accuracy: 0.4932 - val_loss: 1.5637 - val_accuracy: 0.4521 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3677 - accuracy: 0.4728 - val_loss: 1.4757 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3479 - accuracy: 0.5068 - val_loss: 1.3648 - val_accuracy: 0.5205 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2608 - accuracy: 0.5442 - val_loss: 1.7336 - val_accuracy: 0.4795 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2929 - accuracy: 0.5187 - val_loss: 2.1081 - val_accuracy: 0.3562 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2232 - accuracy: 0.5459 - val_loss: 1.7283 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1729 - accuracy: 0.5663 - val_loss: 1.6741 - val_accuracy: 0.4384 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1807 - accuracy: 0.5527 - val_loss: 1.4356 - val_accuracy: 0.4521 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1016 - accuracy: 0.5935 - val_loss: 2.1331 - val_accuracy: 0.3973 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1411 - accuracy: 0.5918 - val_loss: 1.6946 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.0841 - accuracy: 0.6190 - val_loss: 1.0617 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0364 - accuracy: 0.6310 - val_loss: 1.2930 - val_accuracy: 0.6164 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0174 - accuracy: 0.6139 - val_loss: 1.1745 - val_accuracy: 0.6164 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0555 - accuracy: 0.6122 - val_loss: 2.1097 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.9664 - accuracy: 0.6446 - val_loss: 0.9951 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9931 - accuracy: 0.6105 - val_loss: 1.2633 - val_accuracy: 0.6301 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9659 - accuracy: 0.6446 - val_loss: 1.4419 - val_accuracy: 0.6301 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9757 - accuracy: 0.6412 - val_loss: 1.5053 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9559 - accuracy: 0.6276 - val_loss: 1.0939 - val_accuracy: 0.6164 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9153 - accuracy: 0.6820 - val_loss: 1.0960 - val_accuracy: 0.6164 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9430 - accuracy: 0.6412 - val_loss: 1.2351 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.8306 - accuracy: 0.6973 - val_loss: 0.8889 - val_accuracy: 0.6986 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8615 - accuracy: 0.6837 - val_loss: 1.1157 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8566 - accuracy: 0.7075 - val_loss: 0.9960 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7692 - accuracy: 0.7058 - val_loss: 1.1539 - val_accuracy: 0.6986 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8154 - accuracy: 0.7279 - val_loss: 1.0521 - val_accuracy: 0.5890 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7656 - accuracy: 0.7228 - val_loss: 0.9787 - val_accuracy: 0.6164 - lr: 0.0100\n",
            "Epoch 41/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8220 - accuracy: 0.6820 - val_loss: 1.2637 - val_accuracy: 0.6301 - lr: 0.0100\n",
            "Epoch 42/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7345 - accuracy: 0.7381 - val_loss: 0.9805 - val_accuracy: 0.6986 - lr: 0.0100\n",
            "Epoch 43/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7511 - accuracy: 0.7279 - val_loss: 0.9858 - val_accuracy: 0.6849 - lr: 0.0100\n",
            "Epoch 44/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7391 - accuracy: 0.7347 - val_loss: 1.1482 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 45/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7034 - accuracy: 0.7296 - val_loss: 1.1059 - val_accuracy: 0.6164 - lr: 0.0100\n",
            "Epoch 46/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6661 - accuracy: 0.7500 - val_loss: 1.8567 - val_accuracy: 0.5205 - lr: 0.0100\n",
            "Epoch 47/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6709 - accuracy: 0.7687 - val_loss: 1.4730 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 48/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7310 - accuracy: 0.7296 - val_loss: 1.2158 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 49/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.5936 - accuracy: 0.7874 - val_loss: 0.8462 - val_accuracy: 0.7397 - lr: 0.0100\n",
            "Epoch 50/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6821 - accuracy: 0.7466 - val_loss: 1.7231 - val_accuracy: 0.6986 - lr: 0.0100\n",
            "Epoch 51/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6148 - accuracy: 0.7993 - val_loss: 1.0954 - val_accuracy: 0.6712 - lr: 0.0100\n",
            "Epoch 52/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6311 - accuracy: 0.7704 - val_loss: 1.8523 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 53/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5080 - accuracy: 0.8333 - val_loss: 0.9646 - val_accuracy: 0.6438 - lr: 0.0050\n",
            "Epoch 54/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.4803 - accuracy: 0.8384 - val_loss: 0.7929 - val_accuracy: 0.7808 - lr: 0.0050\n",
            "Epoch 55/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4485 - accuracy: 0.8520 - val_loss: 0.8941 - val_accuracy: 0.7260 - lr: 0.0050\n",
            "Epoch 56/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4350 - accuracy: 0.8418 - val_loss: 0.8058 - val_accuracy: 0.8219 - lr: 0.0050\n",
            "Epoch 57/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4379 - accuracy: 0.8367 - val_loss: 0.8312 - val_accuracy: 0.8082 - lr: 0.0050\n",
            "Epoch 58/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4528 - accuracy: 0.8588 - val_loss: 0.7978 - val_accuracy: 0.6986 - lr: 0.0050\n",
            "Epoch 59/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 0.4082 - accuracy: 0.8673 - val_loss: 0.7735 - val_accuracy: 0.7945 - lr: 0.0050\n",
            "Epoch 60/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4046 - accuracy: 0.8520 - val_loss: 0.7957 - val_accuracy: 0.7260 - lr: 0.0050\n",
            "Epoch 61/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3971 - accuracy: 0.8741 - val_loss: 0.7676 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 62/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3774 - accuracy: 0.8673 - val_loss: 0.9385 - val_accuracy: 0.7397 - lr: 0.0050\n",
            "Epoch 63/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3647 - accuracy: 0.8759 - val_loss: 0.8931 - val_accuracy: 0.7397 - lr: 0.0050\n",
            "Epoch 64/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3385 - accuracy: 0.8878 - val_loss: 0.9309 - val_accuracy: 0.7808 - lr: 0.0050\n",
            "Epoch 65/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3389 - accuracy: 0.8912 - val_loss: 0.9119 - val_accuracy: 0.7671 - lr: 0.0050\n",
            "Epoch 66/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3315 - accuracy: 0.8895 - val_loss: 1.1156 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 67/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3391 - accuracy: 0.8656 - val_loss: 0.8560 - val_accuracy: 0.7671 - lr: 0.0050\n",
            "Epoch 68/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3697 - accuracy: 0.8741 - val_loss: 1.1782 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 69/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3169 - accuracy: 0.9014 - val_loss: 0.9299 - val_accuracy: 0.6986 - lr: 0.0050\n",
            "Epoch 70/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3374 - accuracy: 0.8793 - val_loss: 1.0244 - val_accuracy: 0.6986 - lr: 0.0050\n",
            "Epoch 71/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3041 - accuracy: 0.9065 - val_loss: 0.6937 - val_accuracy: 0.7945 - lr: 0.0050\n",
            "Epoch 72/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3047 - accuracy: 0.8997 - val_loss: 0.7599 - val_accuracy: 0.7945 - lr: 0.0050\n",
            "Epoch 73/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2646 - accuracy: 0.9269 - val_loss: 0.9444 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 74/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2659 - accuracy: 0.9031 - val_loss: 0.8049 - val_accuracy: 0.7260 - lr: 0.0050\n",
            "Epoch 75/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.2882 - accuracy: 0.9184 - val_loss: 0.7646 - val_accuracy: 0.8356 - lr: 0.0050\n",
            "Epoch 76/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2706 - accuracy: 0.9082 - val_loss: 0.8903 - val_accuracy: 0.8082 - lr: 0.0050\n",
            "Epoch 77/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2024 - accuracy: 0.9439 - val_loss: 0.7775 - val_accuracy: 0.7671 - lr: 0.0025\n",
            "Epoch 78/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.1980 - accuracy: 0.9405 - val_loss: 0.8330 - val_accuracy: 0.8493 - lr: 0.0025\n",
            "Epoch 79/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1987 - accuracy: 0.9354 - val_loss: 0.7018 - val_accuracy: 0.8356 - lr: 0.0025\n",
            "Epoch 80/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1917 - accuracy: 0.9473 - val_loss: 0.7880 - val_accuracy: 0.8219 - lr: 0.0025\n",
            "Epoch 81/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1915 - accuracy: 0.9371 - val_loss: 0.6423 - val_accuracy: 0.8219 - lr: 0.0025\n",
            "Epoch 82/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.1783 - accuracy: 0.9558 - val_loss: 0.6011 - val_accuracy: 0.8904 - lr: 0.0025\n",
            "Epoch 83/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1617 - accuracy: 0.9643 - val_loss: 0.7319 - val_accuracy: 0.8356 - lr: 0.0025\n",
            "Epoch 84/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1793 - accuracy: 0.9439 - val_loss: 0.7440 - val_accuracy: 0.8082 - lr: 0.0025\n",
            "Epoch 85/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1718 - accuracy: 0.9609 - val_loss: 0.6419 - val_accuracy: 0.8904 - lr: 0.0025\n",
            "Epoch 86/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1710 - accuracy: 0.9490 - val_loss: 0.7865 - val_accuracy: 0.7808 - lr: 0.0025\n",
            "Epoch 87/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 0.1472 - accuracy: 0.9592 - val_loss: 0.7265 - val_accuracy: 0.8356 - lr: 0.0012\n",
            "Epoch 88/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1347 - accuracy: 0.9626 - val_loss: 0.6046 - val_accuracy: 0.8630 - lr: 0.0012\n",
            "Epoch 89/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1225 - accuracy: 0.9643 - val_loss: 0.6202 - val_accuracy: 0.8219 - lr: 0.0012\n",
            "Epoch 90/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1229 - accuracy: 0.9728 - val_loss: 0.7028 - val_accuracy: 0.8493 - lr: 0.0012\n",
            "Epoch 91/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1213 - accuracy: 0.9694 - val_loss: 0.6472 - val_accuracy: 0.8493 - lr: 0.0012\n",
            "Epoch 92/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.0991 - accuracy: 0.9779 - val_loss: 0.6523 - val_accuracy: 0.8493 - lr: 0.0012\n",
            "Epoch 93/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1323 - accuracy: 0.9694 - val_loss: 0.7135 - val_accuracy: 0.8082 - lr: 0.0012\n",
            "Epoch 94/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1271 - accuracy: 0.9694 - val_loss: 0.7289 - val_accuracy: 0.7945 - lr: 0.0012\n",
            "Epoch 95/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1246 - accuracy: 0.9592 - val_loss: 0.6311 - val_accuracy: 0.8767 - lr: 0.0012\n",
            "Epoch 96/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 0.1181 - accuracy: 0.9728 - val_loss: 0.6367 - val_accuracy: 0.8493 - lr: 6.2500e-04\n",
            "Epoch 97/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.0949 - accuracy: 0.9830 - val_loss: 0.6657 - val_accuracy: 0.8630 - lr: 6.2500e-04\n",
            "Epoch 98/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1016 - accuracy: 0.9762 - val_loss: 0.6731 - val_accuracy: 0.8630 - lr: 6.2500e-04\n",
            "Epoch 99/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.0920 - accuracy: 0.9762 - val_loss: 0.7425 - val_accuracy: 0.8493 - lr: 6.2500e-04\n",
            "Epoch 100/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1018 - accuracy: 0.9813 - val_loss: 0.6599 - val_accuracy: 0.8493 - lr: 6.2500e-04\n",
            "\n",
            "\n",
            "2 fold train loss 0.3556 train acc 0.9031, val loss 0.6011 val acc 0.8904, test loss 0.8653 test acc 0.7973\n",
            "\n",
            "\n",
            "Start 3 fold training\n",
            "Epoch 1/100\n",
            "294/294 [==============================] - 18s 27ms/step - loss: 2.1961 - accuracy: 0.2058 - val_loss: 3.9246 - val_accuracy: 0.2055 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 2.0354 - accuracy: 0.2653 - val_loss: 2.8060 - val_accuracy: 0.2329 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.9583 - accuracy: 0.2789 - val_loss: 1.8081 - val_accuracy: 0.2877 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.9494 - accuracy: 0.3027 - val_loss: 1.7469 - val_accuracy: 0.3151 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.9266 - accuracy: 0.3044 - val_loss: 2.0300 - val_accuracy: 0.3151 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.7980 - accuracy: 0.3452 - val_loss: 3.0378 - val_accuracy: 0.3014 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.7497 - accuracy: 0.3963 - val_loss: 1.9881 - val_accuracy: 0.3562 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6931 - accuracy: 0.3861 - val_loss: 2.0312 - val_accuracy: 0.3014 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 1.6098 - accuracy: 0.3912 - val_loss: 3.3685 - val_accuracy: 0.3151 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.5830 - accuracy: 0.4116 - val_loss: 1.9338 - val_accuracy: 0.2877 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4897 - accuracy: 0.4320 - val_loss: 1.8854 - val_accuracy: 0.3425 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 1.4559 - accuracy: 0.4779 - val_loss: 2.3263 - val_accuracy: 0.3425 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4242 - accuracy: 0.4677 - val_loss: 2.6169 - val_accuracy: 0.2466 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3663 - accuracy: 0.5068 - val_loss: 1.7401 - val_accuracy: 0.4384 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.5047 - accuracy: 0.4507 - val_loss: 3.5205 - val_accuracy: 0.2740 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3451 - accuracy: 0.5051 - val_loss: 2.2208 - val_accuracy: 0.3288 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3045 - accuracy: 0.5357 - val_loss: 2.3052 - val_accuracy: 0.3562 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1780 - accuracy: 0.5629 - val_loss: 1.9892 - val_accuracy: 0.4110 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2316 - accuracy: 0.5527 - val_loss: 1.6095 - val_accuracy: 0.4247 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.2094 - accuracy: 0.5731 - val_loss: 1.5391 - val_accuracy: 0.4658 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1800 - accuracy: 0.5595 - val_loss: 2.2429 - val_accuracy: 0.4110 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.0824 - accuracy: 0.6139 - val_loss: 1.4012 - val_accuracy: 0.5342 - lr: 0.0050\n",
            "Epoch 23/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0247 - accuracy: 0.6139 - val_loss: 1.7259 - val_accuracy: 0.4521 - lr: 0.0050\n",
            "Epoch 24/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9830 - accuracy: 0.6480 - val_loss: 1.6119 - val_accuracy: 0.4932 - lr: 0.0050\n",
            "Epoch 25/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9572 - accuracy: 0.6701 - val_loss: 1.8852 - val_accuracy: 0.4932 - lr: 0.0050\n",
            "Epoch 26/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.9386 - accuracy: 0.6650 - val_loss: 1.5351 - val_accuracy: 0.5479 - lr: 0.0050\n",
            "Epoch 27/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9299 - accuracy: 0.6854 - val_loss: 1.8867 - val_accuracy: 0.4658 - lr: 0.0050\n",
            "Epoch 28/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9762 - accuracy: 0.6446 - val_loss: 1.6680 - val_accuracy: 0.4932 - lr: 0.0050\n",
            "Epoch 29/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.8432 - accuracy: 0.7075 - val_loss: 1.5508 - val_accuracy: 0.5616 - lr: 0.0050\n",
            "Epoch 30/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8276 - accuracy: 0.7109 - val_loss: 1.6933 - val_accuracy: 0.5068 - lr: 0.0050\n",
            "Epoch 31/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8616 - accuracy: 0.6888 - val_loss: 1.7825 - val_accuracy: 0.4521 - lr: 0.0050\n",
            "Epoch 32/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8306 - accuracy: 0.7075 - val_loss: 1.9361 - val_accuracy: 0.4795 - lr: 0.0050\n",
            "Epoch 33/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8741 - accuracy: 0.6905 - val_loss: 1.8614 - val_accuracy: 0.4658 - lr: 0.0050\n",
            "Epoch 34/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7233 - accuracy: 0.7602 - val_loss: 1.3985 - val_accuracy: 0.5342 - lr: 0.0025\n",
            "Epoch 35/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7030 - accuracy: 0.7636 - val_loss: 1.4499 - val_accuracy: 0.5342 - lr: 0.0025\n",
            "Epoch 36/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7043 - accuracy: 0.7602 - val_loss: 1.7597 - val_accuracy: 0.5068 - lr: 0.0025\n",
            "Epoch 37/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6956 - accuracy: 0.7704 - val_loss: 1.7622 - val_accuracy: 0.5205 - lr: 0.0025\n",
            "Epoch 38/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.6723 - accuracy: 0.7721 - val_loss: 1.4993 - val_accuracy: 0.6027 - lr: 0.0025\n",
            "Epoch 39/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6392 - accuracy: 0.7789 - val_loss: 1.4555 - val_accuracy: 0.5616 - lr: 0.0025\n",
            "Epoch 40/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6078 - accuracy: 0.7738 - val_loss: 1.5267 - val_accuracy: 0.6027 - lr: 0.0025\n",
            "Epoch 41/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6485 - accuracy: 0.7857 - val_loss: 1.3509 - val_accuracy: 0.5753 - lr: 0.0025\n",
            "Epoch 42/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6298 - accuracy: 0.7704 - val_loss: 1.3514 - val_accuracy: 0.5890 - lr: 0.0025\n",
            "Epoch 43/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6378 - accuracy: 0.7857 - val_loss: 1.5860 - val_accuracy: 0.5616 - lr: 0.0025\n",
            "Epoch 44/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.5540 - accuracy: 0.8163 - val_loss: 1.1884 - val_accuracy: 0.6164 - lr: 0.0012\n",
            "Epoch 45/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5594 - accuracy: 0.8078 - val_loss: 1.3751 - val_accuracy: 0.5616 - lr: 0.0012\n",
            "Epoch 46/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5532 - accuracy: 0.8316 - val_loss: 1.3124 - val_accuracy: 0.5753 - lr: 0.0012\n",
            "Epoch 47/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5798 - accuracy: 0.8129 - val_loss: 1.3429 - val_accuracy: 0.6027 - lr: 0.0012\n",
            "Epoch 48/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5269 - accuracy: 0.8282 - val_loss: 1.3183 - val_accuracy: 0.6164 - lr: 0.0012\n",
            "Epoch 49/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5189 - accuracy: 0.8231 - val_loss: 1.2674 - val_accuracy: 0.6027 - lr: 0.0012\n",
            "Epoch 50/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5107 - accuracy: 0.8435 - val_loss: 1.2500 - val_accuracy: 0.5753 - lr: 0.0012\n",
            "Epoch 51/100\n",
            "294/294 [==============================] - 8s 27ms/step - loss: 0.5178 - accuracy: 0.8367 - val_loss: 1.0530 - val_accuracy: 0.6849 - lr: 0.0012\n",
            "Epoch 52/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4981 - accuracy: 0.8418 - val_loss: 1.2852 - val_accuracy: 0.6027 - lr: 0.0012\n",
            "Epoch 53/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4584 - accuracy: 0.8622 - val_loss: 1.2924 - val_accuracy: 0.5890 - lr: 0.0012\n",
            "Epoch 54/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5092 - accuracy: 0.8367 - val_loss: 1.3522 - val_accuracy: 0.5753 - lr: 0.0012\n",
            "Epoch 55/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5091 - accuracy: 0.8384 - val_loss: 1.1575 - val_accuracy: 0.6027 - lr: 0.0012\n",
            "Epoch 56/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5073 - accuracy: 0.8316 - val_loss: 1.1470 - val_accuracy: 0.6301 - lr: 0.0012\n",
            "Epoch 57/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4931 - accuracy: 0.8333 - val_loss: 1.2168 - val_accuracy: 0.6575 - lr: 6.2500e-04\n",
            "Epoch 58/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4574 - accuracy: 0.8588 - val_loss: 1.2235 - val_accuracy: 0.6164 - lr: 6.2500e-04\n",
            "Epoch 59/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4683 - accuracy: 0.8503 - val_loss: 1.3536 - val_accuracy: 0.6438 - lr: 6.2500e-04\n",
            "Epoch 60/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4318 - accuracy: 0.8690 - val_loss: 1.3306 - val_accuracy: 0.6575 - lr: 6.2500e-04\n",
            "Epoch 61/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4258 - accuracy: 0.8656 - val_loss: 1.1770 - val_accuracy: 0.6849 - lr: 6.2500e-04\n",
            "Epoch 62/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4331 - accuracy: 0.8690 - val_loss: 1.1158 - val_accuracy: 0.6301 - lr: 6.2500e-04\n",
            "Epoch 63/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3926 - accuracy: 0.8861 - val_loss: 1.3626 - val_accuracy: 0.5890 - lr: 6.2500e-04\n",
            "Epoch 64/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4003 - accuracy: 0.8793 - val_loss: 1.3657 - val_accuracy: 0.5890 - lr: 6.2500e-04\n",
            "Epoch 65/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4144 - accuracy: 0.8776 - val_loss: 1.1735 - val_accuracy: 0.6027 - lr: 6.2500e-04\n",
            "Epoch 66/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4193 - accuracy: 0.8639 - val_loss: 1.1682 - val_accuracy: 0.6712 - lr: 6.2500e-04\n",
            "Epoch 67/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3930 - accuracy: 0.8861 - val_loss: 1.3589 - val_accuracy: 0.5890 - lr: 3.1250e-04\n",
            "Epoch 68/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3826 - accuracy: 0.8963 - val_loss: 1.3003 - val_accuracy: 0.6164 - lr: 3.1250e-04\n",
            "Epoch 69/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3988 - accuracy: 0.8776 - val_loss: 1.3036 - val_accuracy: 0.6027 - lr: 3.1250e-04\n",
            "Epoch 70/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4173 - accuracy: 0.8776 - val_loss: 1.3417 - val_accuracy: 0.5753 - lr: 3.1250e-04\n",
            "Epoch 71/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3938 - accuracy: 0.8844 - val_loss: 1.2253 - val_accuracy: 0.6438 - lr: 3.1250e-04\n",
            "Epoch 72/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3904 - accuracy: 0.8844 - val_loss: 1.3335 - val_accuracy: 0.6027 - lr: 1.5625e-04\n",
            "Epoch 73/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4011 - accuracy: 0.8759 - val_loss: 1.2411 - val_accuracy: 0.6164 - lr: 1.5625e-04\n",
            "\n",
            "\n",
            "3 fold train loss 0.6966 train acc 0.7806, val loss 1.0530 val acc 0.6849, test loss 1.1698 test acc 0.6081\n",
            "\n",
            "\n",
            "Start 4 fold training\n",
            "Epoch 1/100\n",
            "294/294 [==============================] - 18s 27ms/step - loss: 2.2324 - accuracy: 0.1701 - val_loss: 4.6191 - val_accuracy: 0.1370 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 2.0544 - accuracy: 0.2364 - val_loss: 1.8796 - val_accuracy: 0.3836 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.9511 - accuracy: 0.2891 - val_loss: 1.5051 - val_accuracy: 0.3973 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.8388 - accuracy: 0.3027 - val_loss: 2.2520 - val_accuracy: 0.3973 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.8517 - accuracy: 0.3333 - val_loss: 3.1473 - val_accuracy: 0.2877 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.7863 - accuracy: 0.3418 - val_loss: 1.6302 - val_accuracy: 0.4110 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.7208 - accuracy: 0.3690 - val_loss: 2.4128 - val_accuracy: 0.3425 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6771 - accuracy: 0.3878 - val_loss: 2.0433 - val_accuracy: 0.3699 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6337 - accuracy: 0.3963 - val_loss: 3.6855 - val_accuracy: 0.2329 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6097 - accuracy: 0.3997 - val_loss: 3.6605 - val_accuracy: 0.3699 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.5462 - accuracy: 0.4388 - val_loss: 4.4989 - val_accuracy: 0.3699 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4704 - accuracy: 0.4575 - val_loss: 1.6878 - val_accuracy: 0.3425 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4717 - accuracy: 0.4371 - val_loss: 1.7605 - val_accuracy: 0.3973 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.4327 - accuracy: 0.4762 - val_loss: 1.9606 - val_accuracy: 0.4247 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3615 - accuracy: 0.4728 - val_loss: 3.0404 - val_accuracy: 0.3562 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3192 - accuracy: 0.5238 - val_loss: 4.5300 - val_accuracy: 0.3836 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.2703 - accuracy: 0.5527 - val_loss: 1.3341 - val_accuracy: 0.4658 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1988 - accuracy: 0.5459 - val_loss: 1.3560 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2049 - accuracy: 0.5442 - val_loss: 5.1585 - val_accuracy: 0.2877 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1431 - accuracy: 0.5595 - val_loss: 1.5873 - val_accuracy: 0.4521 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1494 - accuracy: 0.5765 - val_loss: 1.5803 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0818 - accuracy: 0.6003 - val_loss: 3.8778 - val_accuracy: 0.3151 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.0774 - accuracy: 0.6071 - val_loss: 1.0200 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0937 - accuracy: 0.6071 - val_loss: 1.3616 - val_accuracy: 0.5205 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0557 - accuracy: 0.6105 - val_loss: 1.2426 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0780 - accuracy: 0.6122 - val_loss: 1.0114 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9340 - accuracy: 0.6905 - val_loss: 1.5425 - val_accuracy: 0.5342 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9485 - accuracy: 0.6701 - val_loss: 1.1059 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9490 - accuracy: 0.6650 - val_loss: 1.5931 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9454 - accuracy: 0.6497 - val_loss: 2.3828 - val_accuracy: 0.4795 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.7938 - accuracy: 0.7177 - val_loss: 0.8988 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 32/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7847 - accuracy: 0.7211 - val_loss: 1.1731 - val_accuracy: 0.6438 - lr: 0.0050\n",
            "Epoch 33/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7659 - accuracy: 0.7415 - val_loss: 1.0739 - val_accuracy: 0.5890 - lr: 0.0050\n",
            "Epoch 34/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7768 - accuracy: 0.7347 - val_loss: 1.4767 - val_accuracy: 0.5753 - lr: 0.0050\n",
            "Epoch 35/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.7285 - accuracy: 0.7279 - val_loss: 0.8346 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 36/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7267 - accuracy: 0.7500 - val_loss: 0.8970 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 37/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6955 - accuracy: 0.7636 - val_loss: 0.9483 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 38/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7042 - accuracy: 0.7381 - val_loss: 4.6712 - val_accuracy: 0.4521 - lr: 0.0050\n",
            "Epoch 39/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6583 - accuracy: 0.7874 - val_loss: 0.8827 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 40/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6958 - accuracy: 0.7534 - val_loss: 0.9477 - val_accuracy: 0.6438 - lr: 0.0050\n",
            "Epoch 41/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6512 - accuracy: 0.7891 - val_loss: 1.5405 - val_accuracy: 0.5342 - lr: 0.0050\n",
            "Epoch 42/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6761 - accuracy: 0.7670 - val_loss: 0.8632 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 43/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6724 - accuracy: 0.7466 - val_loss: 1.0494 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 44/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6175 - accuracy: 0.7602 - val_loss: 1.1930 - val_accuracy: 0.5616 - lr: 0.0050\n",
            "Epoch 45/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6211 - accuracy: 0.7925 - val_loss: 1.0345 - val_accuracy: 0.6438 - lr: 0.0050\n",
            "Epoch 46/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6500 - accuracy: 0.7806 - val_loss: 1.3211 - val_accuracy: 0.5890 - lr: 0.0050\n",
            "Epoch 47/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5785 - accuracy: 0.8112 - val_loss: 1.0061 - val_accuracy: 0.6986 - lr: 0.0050\n",
            "Epoch 48/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6208 - accuracy: 0.7772 - val_loss: 0.9322 - val_accuracy: 0.6712 - lr: 0.0050\n",
            "Epoch 49/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6011 - accuracy: 0.7823 - val_loss: 1.0543 - val_accuracy: 0.6301 - lr: 0.0050\n",
            "Epoch 50/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5408 - accuracy: 0.8197 - val_loss: 0.9378 - val_accuracy: 0.7123 - lr: 0.0050\n",
            "Epoch 51/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5714 - accuracy: 0.7891 - val_loss: 0.7154 - val_accuracy: 0.7123 - lr: 0.0050\n",
            "Epoch 52/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5670 - accuracy: 0.7993 - val_loss: 0.9485 - val_accuracy: 0.6301 - lr: 0.0050\n",
            "Epoch 53/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5775 - accuracy: 0.7857 - val_loss: 0.8388 - val_accuracy: 0.6986 - lr: 0.0050\n",
            "Epoch 54/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.4664 - accuracy: 0.8367 - val_loss: 0.5843 - val_accuracy: 0.7945 - lr: 0.0025\n",
            "Epoch 55/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4388 - accuracy: 0.8486 - val_loss: 0.6270 - val_accuracy: 0.7260 - lr: 0.0025\n",
            "Epoch 56/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4557 - accuracy: 0.8435 - val_loss: 0.7388 - val_accuracy: 0.7397 - lr: 0.0025\n",
            "Epoch 57/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4441 - accuracy: 0.8520 - val_loss: 0.6309 - val_accuracy: 0.7123 - lr: 0.0025\n",
            "Epoch 58/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4327 - accuracy: 0.8537 - val_loss: 0.6342 - val_accuracy: 0.7671 - lr: 0.0025\n",
            "Epoch 59/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4194 - accuracy: 0.8571 - val_loss: 0.5512 - val_accuracy: 0.7671 - lr: 0.0025\n",
            "Epoch 60/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4148 - accuracy: 0.8724 - val_loss: 0.8872 - val_accuracy: 0.7397 - lr: 0.0025\n",
            "Epoch 61/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.4426 - accuracy: 0.8588 - val_loss: 0.6612 - val_accuracy: 0.8356 - lr: 0.0025\n",
            "Epoch 62/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3835 - accuracy: 0.8741 - val_loss: 0.7180 - val_accuracy: 0.7945 - lr: 0.0025\n",
            "Epoch 63/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3811 - accuracy: 0.8639 - val_loss: 0.6575 - val_accuracy: 0.7123 - lr: 0.0025\n",
            "Epoch 64/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3875 - accuracy: 0.8639 - val_loss: 0.7075 - val_accuracy: 0.7808 - lr: 0.0025\n",
            "Epoch 65/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3994 - accuracy: 0.8724 - val_loss: 0.6499 - val_accuracy: 0.7534 - lr: 0.0025\n",
            "Epoch 66/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3595 - accuracy: 0.8912 - val_loss: 0.7289 - val_accuracy: 0.7397 - lr: 0.0025\n",
            "Epoch 67/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3569 - accuracy: 0.8827 - val_loss: 0.6909 - val_accuracy: 0.7671 - lr: 0.0025\n",
            "Epoch 68/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3674 - accuracy: 0.8707 - val_loss: 0.8522 - val_accuracy: 0.7123 - lr: 0.0025\n",
            "Epoch 69/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3498 - accuracy: 0.8827 - val_loss: 0.8333 - val_accuracy: 0.7534 - lr: 0.0025\n",
            "Epoch 70/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3711 - accuracy: 0.8759 - val_loss: 0.8411 - val_accuracy: 0.6712 - lr: 0.0025\n",
            "Epoch 71/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3507 - accuracy: 0.8895 - val_loss: 0.8133 - val_accuracy: 0.6712 - lr: 0.0025\n",
            "Epoch 72/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3156 - accuracy: 0.8980 - val_loss: 0.7468 - val_accuracy: 0.6438 - lr: 0.0025\n",
            "Epoch 73/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3428 - accuracy: 0.8929 - val_loss: 0.6387 - val_accuracy: 0.7397 - lr: 0.0025\n",
            "Epoch 74/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3254 - accuracy: 0.9031 - val_loss: 0.7341 - val_accuracy: 0.7671 - lr: 0.0025\n",
            "Epoch 75/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2831 - accuracy: 0.9184 - val_loss: 0.6392 - val_accuracy: 0.7808 - lr: 0.0025\n",
            "Epoch 76/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3113 - accuracy: 0.9014 - val_loss: 0.6392 - val_accuracy: 0.7534 - lr: 0.0025\n",
            "Epoch 77/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3204 - accuracy: 0.9014 - val_loss: 0.9312 - val_accuracy: 0.7260 - lr: 0.0025\n",
            "Epoch 78/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2946 - accuracy: 0.9082 - val_loss: 0.5409 - val_accuracy: 0.8082 - lr: 0.0025\n",
            "Epoch 79/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2543 - accuracy: 0.9167 - val_loss: 0.5032 - val_accuracy: 0.7671 - lr: 0.0012\n",
            "Epoch 80/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2206 - accuracy: 0.9439 - val_loss: 0.4856 - val_accuracy: 0.8356 - lr: 0.0012\n",
            "Epoch 81/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2198 - accuracy: 0.9456 - val_loss: 0.5446 - val_accuracy: 0.7945 - lr: 0.0012\n",
            "Epoch 82/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2257 - accuracy: 0.9320 - val_loss: 0.6392 - val_accuracy: 0.7945 - lr: 0.0012\n",
            "Epoch 83/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2460 - accuracy: 0.9303 - val_loss: 0.6306 - val_accuracy: 0.7534 - lr: 0.0012\n",
            "Epoch 84/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2223 - accuracy: 0.9422 - val_loss: 0.5329 - val_accuracy: 0.8356 - lr: 0.0012\n",
            "Epoch 85/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1944 - accuracy: 0.9439 - val_loss: 0.5275 - val_accuracy: 0.8219 - lr: 6.2500e-04\n",
            "Epoch 86/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1964 - accuracy: 0.9490 - val_loss: 0.5036 - val_accuracy: 0.8356 - lr: 6.2500e-04\n",
            "Epoch 87/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2062 - accuracy: 0.9405 - val_loss: 0.5616 - val_accuracy: 0.8219 - lr: 6.2500e-04\n",
            "Epoch 88/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1823 - accuracy: 0.9592 - val_loss: 0.5718 - val_accuracy: 0.7945 - lr: 6.2500e-04\n",
            "Epoch 89/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2070 - accuracy: 0.9456 - val_loss: 0.5205 - val_accuracy: 0.8219 - lr: 6.2500e-04\n",
            "Epoch 90/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2212 - accuracy: 0.9320 - val_loss: 0.4978 - val_accuracy: 0.8356 - lr: 6.2500e-04\n",
            "Epoch 91/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2048 - accuracy: 0.9490 - val_loss: 0.5669 - val_accuracy: 0.8082 - lr: 6.2500e-04\n",
            "Epoch 92/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1768 - accuracy: 0.9558 - val_loss: 0.5339 - val_accuracy: 0.8219 - lr: 3.1250e-04\n",
            "Epoch 93/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2067 - accuracy: 0.9456 - val_loss: 0.5761 - val_accuracy: 0.7945 - lr: 3.1250e-04\n",
            "Epoch 94/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1635 - accuracy: 0.9626 - val_loss: 0.5698 - val_accuracy: 0.8082 - lr: 3.1250e-04\n",
            "Epoch 95/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1849 - accuracy: 0.9490 - val_loss: 0.5043 - val_accuracy: 0.7808 - lr: 3.1250e-04\n",
            "Epoch 96/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1861 - accuracy: 0.9456 - val_loss: 0.5411 - val_accuracy: 0.8082 - lr: 3.1250e-04\n",
            "Epoch 97/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1638 - accuracy: 0.9643 - val_loss: 0.5471 - val_accuracy: 0.7945 - lr: 3.1250e-04\n",
            "Epoch 98/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1874 - accuracy: 0.9524 - val_loss: 0.4995 - val_accuracy: 0.8219 - lr: 1.5625e-04\n",
            "Epoch 99/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1559 - accuracy: 0.9626 - val_loss: 0.5557 - val_accuracy: 0.7808 - lr: 1.5625e-04\n",
            "Epoch 100/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1778 - accuracy: 0.9524 - val_loss: 0.5454 - val_accuracy: 0.8082 - lr: 1.5625e-04\n",
            "\n",
            "\n",
            "4 fold train loss 0.6283 train acc 0.7993, val loss 0.6612 val acc 0.8356, test loss 1.0241 test acc 0.7568\n",
            "\n",
            "\n",
            "Start 5 fold training\n",
            "Epoch 1/100\n",
            "294/294 [==============================] - 18s 29ms/step - loss: 2.2212 - accuracy: 0.1854 - val_loss: 2.8374 - val_accuracy: 0.3425 - lr: 0.0100\n",
            "Epoch 2/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 2.0770 - accuracy: 0.2517 - val_loss: 1.6326 - val_accuracy: 0.3836 - lr: 0.0100\n",
            "Epoch 3/100\n",
            "294/294 [==============================] - 7s 26ms/step - loss: 1.9874 - accuracy: 0.3010 - val_loss: 2.0149 - val_accuracy: 0.4247 - lr: 0.0100\n",
            "Epoch 4/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 1.9311 - accuracy: 0.3010 - val_loss: 1.5200 - val_accuracy: 0.3836 - lr: 0.0100\n",
            "Epoch 5/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.8745 - accuracy: 0.3367 - val_loss: 1.6107 - val_accuracy: 0.3699 - lr: 0.0100\n",
            "Epoch 6/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.8500 - accuracy: 0.3231 - val_loss: 1.3851 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 7/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.8011 - accuracy: 0.3554 - val_loss: 1.3940 - val_accuracy: 0.4658 - lr: 0.0100\n",
            "Epoch 8/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6816 - accuracy: 0.4099 - val_loss: 1.3818 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 9/100\n",
            "294/294 [==============================] - 7s 24ms/step - loss: 1.6545 - accuracy: 0.4014 - val_loss: 1.5326 - val_accuracy: 0.4795 - lr: 0.0100\n",
            "Epoch 10/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.6339 - accuracy: 0.4337 - val_loss: 1.6147 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 11/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.5915 - accuracy: 0.4269 - val_loss: 1.3001 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 12/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.4907 - accuracy: 0.4660 - val_loss: 1.9785 - val_accuracy: 0.3973 - lr: 0.0100\n",
            "Epoch 13/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.4786 - accuracy: 0.4575 - val_loss: 1.2513 - val_accuracy: 0.5890 - lr: 0.0100\n",
            "Epoch 14/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3647 - accuracy: 0.5034 - val_loss: 1.4689 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 15/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.3402 - accuracy: 0.5136 - val_loss: 1.1287 - val_accuracy: 0.6301 - lr: 0.0100\n",
            "Epoch 16/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.3143 - accuracy: 0.5272 - val_loss: 1.2201 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 17/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.2420 - accuracy: 0.5374 - val_loss: 1.2203 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 18/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.1740 - accuracy: 0.5935 - val_loss: 1.0559 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 19/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1821 - accuracy: 0.5663 - val_loss: 1.3292 - val_accuracy: 0.5068 - lr: 0.0100\n",
            "Epoch 20/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1407 - accuracy: 0.5629 - val_loss: 1.4480 - val_accuracy: 0.5205 - lr: 0.0100\n",
            "Epoch 21/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1475 - accuracy: 0.5799 - val_loss: 1.4219 - val_accuracy: 0.4932 - lr: 0.0100\n",
            "Epoch 22/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.1916 - accuracy: 0.5680 - val_loss: 1.0078 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 23/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.0946 - accuracy: 0.6173 - val_loss: 1.0482 - val_accuracy: 0.6712 - lr: 0.0100\n",
            "Epoch 24/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.1245 - accuracy: 0.5918 - val_loss: 1.1407 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 25/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9980 - accuracy: 0.6939 - val_loss: 1.1698 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 26/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 1.0067 - accuracy: 0.6276 - val_loss: 1.3710 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 27/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9689 - accuracy: 0.6548 - val_loss: 1.4662 - val_accuracy: 0.6027 - lr: 0.0100\n",
            "Epoch 28/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 1.0004 - accuracy: 0.6616 - val_loss: 0.9442 - val_accuracy: 0.6986 - lr: 0.0100\n",
            "Epoch 29/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9273 - accuracy: 0.6633 - val_loss: 1.0506 - val_accuracy: 0.6301 - lr: 0.0100\n",
            "Epoch 30/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9457 - accuracy: 0.6786 - val_loss: 1.4213 - val_accuracy: 0.5753 - lr: 0.0100\n",
            "Epoch 31/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.9451 - accuracy: 0.6718 - val_loss: 1.1686 - val_accuracy: 0.6575 - lr: 0.0100\n",
            "Epoch 32/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.8201 - accuracy: 0.7126 - val_loss: 0.9706 - val_accuracy: 0.7260 - lr: 0.0100\n",
            "Epoch 33/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8790 - accuracy: 0.6854 - val_loss: 0.8793 - val_accuracy: 0.6849 - lr: 0.0100\n",
            "Epoch 34/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8337 - accuracy: 0.7007 - val_loss: 0.7529 - val_accuracy: 0.7260 - lr: 0.0100\n",
            "Epoch 35/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.8198 - accuracy: 0.6888 - val_loss: 2.0712 - val_accuracy: 0.5479 - lr: 0.0100\n",
            "Epoch 36/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7407 - accuracy: 0.7262 - val_loss: 1.0248 - val_accuracy: 0.6438 - lr: 0.0100\n",
            "Epoch 37/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.7677 - accuracy: 0.7364 - val_loss: 0.9700 - val_accuracy: 0.7671 - lr: 0.0100\n",
            "Epoch 38/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7566 - accuracy: 0.7364 - val_loss: 1.4917 - val_accuracy: 0.5616 - lr: 0.0100\n",
            "Epoch 39/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.7526 - accuracy: 0.7228 - val_loss: 0.9398 - val_accuracy: 0.6712 - lr: 0.0100\n",
            "Epoch 40/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.6360 - accuracy: 0.7823 - val_loss: 0.9613 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 41/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5997 - accuracy: 0.8027 - val_loss: 1.0355 - val_accuracy: 0.6986 - lr: 0.0050\n",
            "Epoch 42/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5760 - accuracy: 0.8027 - val_loss: 0.7803 - val_accuracy: 0.7671 - lr: 0.0050\n",
            "Epoch 43/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5747 - accuracy: 0.8095 - val_loss: 0.7316 - val_accuracy: 0.7534 - lr: 0.0050\n",
            "Epoch 44/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5693 - accuracy: 0.8061 - val_loss: 0.9781 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 45/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5557 - accuracy: 0.8095 - val_loss: 1.0772 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 46/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5687 - accuracy: 0.8027 - val_loss: 1.0354 - val_accuracy: 0.7260 - lr: 0.0050\n",
            "Epoch 47/100\n",
            "294/294 [==============================] - 8s 26ms/step - loss: 0.5265 - accuracy: 0.8214 - val_loss: 0.7397 - val_accuracy: 0.7808 - lr: 0.0050\n",
            "Epoch 48/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4939 - accuracy: 0.8384 - val_loss: 1.0443 - val_accuracy: 0.7397 - lr: 0.0050\n",
            "Epoch 49/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4699 - accuracy: 0.8486 - val_loss: 1.3877 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 50/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5171 - accuracy: 0.8214 - val_loss: 1.1900 - val_accuracy: 0.6438 - lr: 0.0050\n",
            "Epoch 51/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.5364 - accuracy: 0.8146 - val_loss: 1.0585 - val_accuracy: 0.7123 - lr: 0.0050\n",
            "Epoch 52/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4443 - accuracy: 0.8537 - val_loss: 1.2526 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 53/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4950 - accuracy: 0.8299 - val_loss: 1.3311 - val_accuracy: 0.6575 - lr: 0.0050\n",
            "Epoch 54/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4722 - accuracy: 0.8435 - val_loss: 1.3962 - val_accuracy: 0.7123 - lr: 0.0050\n",
            "Epoch 55/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4611 - accuracy: 0.8452 - val_loss: 1.1783 - val_accuracy: 0.6849 - lr: 0.0050\n",
            "Epoch 56/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.4176 - accuracy: 0.8656 - val_loss: 0.9982 - val_accuracy: 0.7123 - lr: 0.0025\n",
            "Epoch 57/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3516 - accuracy: 0.9031 - val_loss: 0.9460 - val_accuracy: 0.7260 - lr: 0.0025\n",
            "Epoch 58/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3805 - accuracy: 0.8690 - val_loss: 0.8003 - val_accuracy: 0.7534 - lr: 0.0025\n",
            "Epoch 59/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3631 - accuracy: 0.8724 - val_loss: 1.7131 - val_accuracy: 0.6712 - lr: 0.0025\n",
            "Epoch 60/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3409 - accuracy: 0.8946 - val_loss: 1.1144 - val_accuracy: 0.7671 - lr: 0.0025\n",
            "Epoch 61/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3508 - accuracy: 0.9031 - val_loss: 1.1455 - val_accuracy: 0.7260 - lr: 0.0025\n",
            "Epoch 62/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3572 - accuracy: 0.8793 - val_loss: 0.9273 - val_accuracy: 0.7534 - lr: 0.0025\n",
            "Epoch 63/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3502 - accuracy: 0.8946 - val_loss: 1.2476 - val_accuracy: 0.7123 - lr: 0.0025\n",
            "Epoch 64/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3043 - accuracy: 0.9218 - val_loss: 1.0343 - val_accuracy: 0.7534 - lr: 0.0012\n",
            "Epoch 65/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2987 - accuracy: 0.9303 - val_loss: 1.0847 - val_accuracy: 0.7123 - lr: 0.0012\n",
            "Epoch 66/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2959 - accuracy: 0.9235 - val_loss: 1.1401 - val_accuracy: 0.7123 - lr: 0.0012\n",
            "Epoch 67/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.3244 - accuracy: 0.8963 - val_loss: 1.0950 - val_accuracy: 0.7260 - lr: 0.0012\n",
            "Epoch 68/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2811 - accuracy: 0.9133 - val_loss: 1.1891 - val_accuracy: 0.6712 - lr: 0.0012\n",
            "Epoch 69/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2740 - accuracy: 0.9235 - val_loss: 0.9223 - val_accuracy: 0.7397 - lr: 0.0012\n",
            "Epoch 70/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2943 - accuracy: 0.9167 - val_loss: 0.8743 - val_accuracy: 0.7534 - lr: 0.0012\n",
            "Epoch 71/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2700 - accuracy: 0.9218 - val_loss: 1.2132 - val_accuracy: 0.7260 - lr: 0.0012\n",
            "Epoch 72/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2943 - accuracy: 0.9031 - val_loss: 1.0954 - val_accuracy: 0.7397 - lr: 0.0012\n",
            "Epoch 73/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2662 - accuracy: 0.9286 - val_loss: 1.1034 - val_accuracy: 0.7260 - lr: 0.0012\n",
            "Epoch 74/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2458 - accuracy: 0.9320 - val_loss: 1.1066 - val_accuracy: 0.7123 - lr: 0.0012\n",
            "Epoch 75/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2576 - accuracy: 0.9218 - val_loss: 1.0568 - val_accuracy: 0.7397 - lr: 0.0012\n",
            "Epoch 76/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2580 - accuracy: 0.9320 - val_loss: 1.1890 - val_accuracy: 0.6986 - lr: 0.0012\n",
            "Epoch 77/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2674 - accuracy: 0.9235 - val_loss: 1.2215 - val_accuracy: 0.7260 - lr: 0.0012\n",
            "Epoch 78/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2562 - accuracy: 0.9235 - val_loss: 0.9286 - val_accuracy: 0.7671 - lr: 6.2500e-04\n",
            "Epoch 79/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2309 - accuracy: 0.9286 - val_loss: 1.0600 - val_accuracy: 0.7808 - lr: 6.2500e-04\n",
            "Epoch 80/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2072 - accuracy: 0.9490 - val_loss: 1.1464 - val_accuracy: 0.7123 - lr: 6.2500e-04\n",
            "Epoch 81/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2292 - accuracy: 0.9388 - val_loss: 1.1293 - val_accuracy: 0.7260 - lr: 6.2500e-04\n",
            "Epoch 82/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2436 - accuracy: 0.9303 - val_loss: 1.0257 - val_accuracy: 0.7671 - lr: 6.2500e-04\n",
            "Epoch 83/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2413 - accuracy: 0.9286 - val_loss: 1.0543 - val_accuracy: 0.7671 - lr: 6.2500e-04\n",
            "Epoch 84/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2105 - accuracy: 0.9388 - val_loss: 0.9986 - val_accuracy: 0.7671 - lr: 3.1250e-04\n",
            "Epoch 85/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2155 - accuracy: 0.9490 - val_loss: 1.0504 - val_accuracy: 0.7671 - lr: 3.1250e-04\n",
            "Epoch 86/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2205 - accuracy: 0.9388 - val_loss: 0.9147 - val_accuracy: 0.7534 - lr: 3.1250e-04\n",
            "Epoch 87/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.1991 - accuracy: 0.9422 - val_loss: 0.9464 - val_accuracy: 0.7534 - lr: 1.5625e-04\n",
            "Epoch 88/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2157 - accuracy: 0.9337 - val_loss: 0.9842 - val_accuracy: 0.7671 - lr: 1.5625e-04\n",
            "Epoch 89/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2024 - accuracy: 0.9456 - val_loss: 0.9906 - val_accuracy: 0.7671 - lr: 1.5625e-04\n",
            "Epoch 90/100\n",
            "294/294 [==============================] - 7s 25ms/step - loss: 0.2216 - accuracy: 0.9473 - val_loss: 0.9632 - val_accuracy: 0.7808 - lr: 1.5625e-04\n",
            "\n",
            "\n",
            "5 fold train loss 0.6183 train acc 0.8112, val loss 0.7397 val acc 0.7808, test loss 0.7964 test acc 0.7027\n",
            "\n",
            "\n",
            "\n",
            "\n",
            "5 fold train loss avg 0.5135 train acc avg 0.8456, val loss avg 0.8136 val acc avg 0.8082, test loss avg 0.9113 test acc avg 0.7324\n"
          ]
        }
      ],
      "source": [
        "#without data argumatent\n",
        "k_fold = 5\n",
        "num_classes = 10\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 2\n",
        "lr = 0.01\n",
        "file_name0 = 'GTZAN_model.keras'\n",
        "path  = '/content/drive/MyDrive/aime/logs/'\n",
        "csv_name0 = 'GTZAN_csv.csv'\n",
        "train_loss_record = []\n",
        "train_acc_record = []\n",
        "val_loss_record = []\n",
        "val_acc_record = []\n",
        "test_loss_record = []\n",
        "test_acc_record = []\n",
        "for i in range(k_fold):\n",
        "    print('Start %d fold training' % (i+1))\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X_melspec, y_one.toarray(), train_size=train_size,\n",
        "                                                                          val_size=val_size, test_size=test_size)\n",
        "    file_name = '/content/drive/MyDrive/aime/Extend/'+str(i)+'_fold_'+file_name0\n",
        "#     log_path  = path+str(i)+'_fold_'+'tensorboard_log'\n",
        "    csv_path  = path+str(i)+'_fold_'+ csv_name0\n",
        "    lr_change = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=3, min_lr=0.000)\n",
        "    model_checkpoint = ModelCheckpoint(file_name, monitor='val_accuracy', save_best_only=True, mode='max')\n",
        "    early_stopping = EarlyStopping(monitor='loss', min_delta=0.01, patience=10, mode='min')\n",
        "    csv_logger = CSVLogger(csv_path)\n",
        "#     tb_cb = TensorBoard(log_dir=log_path, write_images=1, histogram_freq=1)\n",
        "    callbacks =[lr_change, model_checkpoint, early_stopping,csv_logger]\n",
        "    opt = Adam(learning_rate=lr)\n",
        "    model = multi_scale_level_cnn(input_shape=(X_melspec.shape[1], X_melspec.shape[2], X_melspec.shape[3]),\n",
        "                              num_dense_blocks=3, num_conv_filters=32, num_classes=num_classes)\n",
        "    model.compile(\n",
        "                loss='categorical_crossentropy',\n",
        "                metrics=['accuracy'],\n",
        "                optimizer=opt)\n",
        "    model.fit(X_train, y_train, batch_size=batch_size, epochs=epochs,\n",
        "              validation_data=(X_val, y_val), verbose=1,\n",
        "              callbacks=callbacks)\n",
        "    model_best = load_model(file_name)\n",
        "    train_loss, train_acc = model_best.evaluate(X_train, y_train, batch_size=batch_size, verbose=0)\n",
        "    val_loss, val_acc = model_best.evaluate(X_val, y_val, batch_size=batch_size, verbose=0)\n",
        "    test_loss, test_acc = model_best.evaluate(X_test, y_test, batch_size=batch_size, verbose=0)\n",
        "\n",
        "    train_loss_record.append(train_loss)\n",
        "    train_acc_record.append(train_acc)\n",
        "    val_loss_record.append(val_loss)\n",
        "    val_acc_record.append(val_acc)\n",
        "    test_loss_record.append(test_loss)\n",
        "    test_acc_record.append(test_acc)\n",
        "    print('\\n\\n%d fold train loss %.4f train acc %.4f, val loss %.4f val acc %.4f, test loss %.4f test acc %.4f\\n\\n' %\n",
        "          (i+1, train_loss, train_acc, val_loss, val_acc, test_loss, test_acc))\n",
        "train_loss_avg = np.mean(np.array(train_loss_record))\n",
        "train_acc_avg = np.mean(np.array(train_acc_record))\n",
        "val_loss_avg = np.mean(np.array(val_loss_record))\n",
        "val_acc_avg = np.mean(np.array(val_acc_record))\n",
        "test_loss_avg = np.mean(np.array(test_loss_record))\n",
        "test_acc_avg = np.mean(np.array(test_acc_record))\n",
        "print('\\n\\n%d fold train loss avg %.4f train acc avg %.4f, val loss avg %.4f val acc avg %.4f, test loss avg %.4f test acc avg %.4f' %\n",
        "  (k_fold, train_loss_avg, train_acc_avg, val_loss_avg, val_acc_avg, test_loss_avg, test_acc_avg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "IBqmdblQCM_B"
      },
      "outputs": [],
      "source": [
        "\n",
        "num_classes = 13\n",
        "\n",
        "train_size = 0.8\n",
        "val_size = 0.1\n",
        "test_size = 0.1\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "lr = 0.01\n",
        "train_loss_record = []\n",
        "train_acc_record = []\n",
        "val_loss_record = []\n",
        "val_acc_record = []\n",
        "test_loss_record = []\n",
        "test_acc_record = []\n",
        "for i in range(k_fold):\n",
        "    print('Start %d fold training' % (i+1))\n",
        "    X_train, y_train, X_val, y_val, X_test, y_test = train_val_test_split(X_melspec, y, train_size=train_size,\n",
        "                                                                          val_size=val_size, test_size=test_size)\n",
        "    model_best = load_model(file_name)\n",
        "    train_loss, train_acc = model_best.evaluate(X_train, y_train,batch_size=batch_size,verbose=0)\n",
        "    val_loss, val_acc = model_best.evaluate(X_val, y_val, batch_size=batch_size,verbose=0)\n",
        "    test_loss, test_acc = model_best.evaluate(X_test, y_test,batch_size=batch_size, verbose=0)\n",
        "\n",
        "    train_loss_record.append(train_loss)\n",
        "    train_acc_record.append(train_acc)\n",
        "    val_loss_record.append(val_loss)\n",
        "    val_acc_record.append(val_acc)\n",
        "    test_loss_record.append(test_loss)\n",
        "    test_acc_record.append(test_acc)\n",
        "    print('\\n\\n%d fold train loss %.4f train acc %.4f, val loss %.4f val acc %.4f, test loss %.4f test acc %.4f\\n\\n' %\n",
        "          (i+1, train_loss, train_acc, val_loss, val_acc, test_loss, test_acc))\n",
        "\n",
        "train_loss_avg = np.mean(np.array(train_loss_record))\n",
        "train_acc_avg = np.mean(np.array(train_acc_record))\n",
        "val_loss_avg = np.mean(np.array(val_loss_record))\n",
        "val_acc_avg = np.mean(np.array(val_acc_record))\n",
        "test_loss_avg = np.mean(np.array(test_loss_record))\n",
        "test_acc_avg = np.mean(np.array(test_acc_record))\n",
        "print('\\n\\n%d fold train loss avg %.4f train acc avg %.4f, val loss avg %.4f val acc avg %.4f, test loss avg %.4f test acc avg %.4f' %\n",
        "  (k_fold, train_loss_avg, train_acc_avg, val_loss_avg, val_acc_avg, test_loss_avg, test_acc_avg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "scrolled": true,
        "id": "bed83hZiCM_B"
      },
      "outputs": [],
      "source": [
        "model = multi_scale_level_cnn(input_shape=(X_train.shape[1], X_train.shape[2], X_train.shape[3]),\n",
        "                              num_dense_blocks=3, num_conv_filters=32, num_classes=10)\n",
        "model.summary()\n",
        "\n",
        "\n",
        "epochs = 100\n",
        "batch_size = 8\n",
        "opt = Adam(lr=0.0001)\n",
        "lr_change = ReduceLROnPlateau(monitor=\"loss\", factor=0.5, patience=2, min_lr=0.000)\n",
        "train_data_iter = data_iter(X_train, y_train, batch_size)\n",
        "test_data_iter = data_iter(X_test, y_test, batch_size)\n",
        "model.compile(\n",
        "            loss='categorical_crossentropy',\n",
        "            metrics=['accuracy'],\n",
        "            optimizer=opt)\n",
        "\n",
        "for e in range(epochs):\n",
        "    batchs = 0\n",
        "    for X_batch, y_batch in train_data_iter:\n",
        "        model.train_on_batch(X_batch, y_batch)\n",
        "        batchs += 1\n",
        "        if batchs >= len(X_train) / 32:\n",
        "            break\n",
        "    train_evaluation = model.evaluate(X_train, y_train, verbose=0)\n",
        "    val_evaluation = model.evaluate(X_val, y_val, verbose=0)\n",
        "    test_evaluation = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "    print('Epoch %d train_loss: %.4f train_acc: %.4f, val_loss: %.4f val_acc: %.4f, test_loss: %.4f, test_acc: %.4f' %\n",
        "          (e+1, train_evaluation[0], train_evaluation[1], val_evaluation[0], val_evaluation[1], test_evaluation[0], test_evaluation[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "collapsed": true,
        "id": "UgkATsSyCM_C"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "L4"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}